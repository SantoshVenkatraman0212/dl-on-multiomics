{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Handling on Multi-omics Gleason Score Dataset (Mergefile_top20(4).csv)**",
   "id": "4204fb2ab06d9d68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Importing necessary libraries and checking the availability of the GPU**",
   "id": "aa1e0b5e11da185c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:10.739101Z",
     "start_time": "2024-10-01T22:46:09.781573Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 18:46:09.889592: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-01 18:46:09.910533: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 18:46:09.910552: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 18:46:09.910568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 18:46:09.915229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 18:46:10.368776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "import tensorflow as tf",
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:10.751117Z",
     "start_time": "2024-10-01T22:46:10.748725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ],
   "id": "1c6ab480a6547fed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:10.858552Z",
     "start_time": "2024-10-01T22:46:10.814980Z"
    }
   },
   "cell_type": "code",
   "source": "print(tf.config.list_physical_devices())",
   "id": "28d538a9d219b812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 18:46:10.840340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:10.856324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:10.856419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:10.870072Z",
     "start_time": "2024-10-01T22:46:10.867389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ],
   "id": "5c5a47bfdab1dc48",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Loading in the CSV file using Pandas**",
   "id": "915c7016bec0ccfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:10.942054Z",
     "start_time": "2024-10-01T22:46:10.926129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Multi_omics dataset\n",
    "df1 = pd.read_csv(r'/home/sanven0212/PycharmProjects/MultiOmicsFYP/Mergefile_top20 (4).csv')\n",
    "df1.head(10)"
   ],
   "id": "d6c0e8269e63da3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  GE_PITPNM2  \\\n",
       "0  0.489097  0.749514   0.703490    0.187461   0.297315  0.744052    0.677605   \n",
       "1  0.373498  0.790415   0.648564    0.367439   0.000000  0.638505    0.156215   \n",
       "2  0.316560  0.714836   0.794977    0.363893   0.523422  0.659055    0.609083   \n",
       "3  0.433556  0.637116   0.777338    0.559615   0.476760  0.623626    0.415425   \n",
       "4  0.363634  0.624428   0.892508    0.501788   0.472260  0.465397    0.619784   \n",
       "5  0.540453  0.630588   0.671140    0.275536   0.662487  0.476065    0.577890   \n",
       "6  0.506989  0.590670   0.727922    0.255442   0.733651  0.691024    0.398213   \n",
       "7  0.602648  0.780246   0.681841    0.362920   0.356234  0.631882    0.189198   \n",
       "8  0.476160  0.319457   0.926185    0.172214   0.711358  0.589873    0.692148   \n",
       "9  0.418700  0.702095   0.745375    0.430890   0.487548  0.669876    0.441037   \n",
       "\n",
       "     GE_ATM   GE_EMG1   GE_ETV3  ...  DM_SLC27A4  DM_PITPNM2   DM_PTEN  \\\n",
       "0  0.736915  0.165418  0.927902  ...    0.309464    0.717767  0.075158   \n",
       "1  0.673907  0.384063  0.807230  ...    0.000000    0.791218  0.045264   \n",
       "2  0.759267  0.107208  0.827580  ...    0.493194    0.782551  0.086650   \n",
       "3  0.539748  0.466929  0.599673  ...    0.085465    0.569465  0.047191   \n",
       "4  0.562305  0.450817  0.729161  ...    0.119817    0.907230  0.082071   \n",
       "5  0.824079  0.297729  0.899286  ...    0.230212    0.831449  0.040868   \n",
       "6  0.644989  0.383090  0.713497  ...    0.502728    0.836422  0.036482   \n",
       "7  0.589256  0.521341  0.691657  ...    0.267739    0.839557  0.314925   \n",
       "8  0.797628  0.445715  0.904204  ...    0.711538    0.562896  0.024710   \n",
       "9  0.506928  0.140151  0.867808  ...    0.299419    0.605663  0.045999   \n",
       "\n",
       "    DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1    PATIENT_ID  \\\n",
       "0  0.395376  0.900153  0.228063   0.592166  0.815008  TCGA-2A-A8VT   \n",
       "1  0.090997  0.942279  0.257308   0.093755  0.762329  TCGA-2A-A8W1   \n",
       "2  0.638757  0.923586  0.210818   0.472227  0.883677  TCGA-2A-A8W3   \n",
       "3  0.101831  0.930270  0.221800   0.257343  1.000000  TCGA-2A-AAYF   \n",
       "4  0.155321  0.901625  0.171257   0.201274  0.695265  TCGA-CH-5737   \n",
       "5  0.266358  0.943859  0.178990   0.505571  0.831493  TCGA-CH-5739   \n",
       "6  0.328812  0.954992  0.185582   0.391752  0.892649  TCGA-CH-5740   \n",
       "7  0.271794  0.858322  0.119971   0.345410  0.831250  TCGA-CH-5741   \n",
       "8  0.636648  0.786474  0.227947   0.863551  0.622275  TCGA-CH-5743   \n",
       "9  0.422079  0.885389  0.238896   0.548180  0.828444  TCGA-CH-5744   \n",
       "\n",
       "   TUMOR_STAGE  \n",
       "0           45  \n",
       "1           43  \n",
       "2           45  \n",
       "3           34  \n",
       "4           43  \n",
       "5           34  \n",
       "6           34  \n",
       "7           45  \n",
       "8           34  \n",
       "9           43  \n",
       "\n",
       "[10 rows x 48 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489097</td>\n",
       "      <td>0.749514</td>\n",
       "      <td>0.703490</td>\n",
       "      <td>0.187461</td>\n",
       "      <td>0.297315</td>\n",
       "      <td>0.744052</td>\n",
       "      <td>0.677605</td>\n",
       "      <td>0.736915</td>\n",
       "      <td>0.165418</td>\n",
       "      <td>0.927902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309464</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.075158</td>\n",
       "      <td>0.395376</td>\n",
       "      <td>0.900153</td>\n",
       "      <td>0.228063</td>\n",
       "      <td>0.592166</td>\n",
       "      <td>0.815008</td>\n",
       "      <td>TCGA-2A-A8VT</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>TCGA-2A-A8W1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316560</td>\n",
       "      <td>0.714836</td>\n",
       "      <td>0.794977</td>\n",
       "      <td>0.363893</td>\n",
       "      <td>0.523422</td>\n",
       "      <td>0.659055</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.759267</td>\n",
       "      <td>0.107208</td>\n",
       "      <td>0.827580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493194</td>\n",
       "      <td>0.782551</td>\n",
       "      <td>0.086650</td>\n",
       "      <td>0.638757</td>\n",
       "      <td>0.923586</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>0.472227</td>\n",
       "      <td>0.883677</td>\n",
       "      <td>TCGA-2A-A8W3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.476760</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>TCGA-2A-AAYF</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>TCGA-CH-5737</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.662487</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "      <td>TCGA-CH-5739</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "      <td>TCGA-CH-5740</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.602648</td>\n",
       "      <td>0.780246</td>\n",
       "      <td>0.681841</td>\n",
       "      <td>0.362920</td>\n",
       "      <td>0.356234</td>\n",
       "      <td>0.631882</td>\n",
       "      <td>0.189198</td>\n",
       "      <td>0.589256</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.691657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267739</td>\n",
       "      <td>0.839557</td>\n",
       "      <td>0.314925</td>\n",
       "      <td>0.271794</td>\n",
       "      <td>0.858322</td>\n",
       "      <td>0.119971</td>\n",
       "      <td>0.345410</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>TCGA-CH-5741</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.476160</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>0.172214</td>\n",
       "      <td>0.711358</td>\n",
       "      <td>0.589873</td>\n",
       "      <td>0.692148</td>\n",
       "      <td>0.797628</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.904204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.562896</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.786474</td>\n",
       "      <td>0.227947</td>\n",
       "      <td>0.863551</td>\n",
       "      <td>0.622275</td>\n",
       "      <td>TCGA-CH-5743</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "      <td>TCGA-CH-5744</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.010490Z",
     "start_time": "2024-10-01T22:46:11.004378Z"
    }
   },
   "cell_type": "code",
   "source": "df1.info()",
   "id": "3edc5277908b11f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 341 entries, 0 to 340\n",
      "Data columns (total 48 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      341 non-null    float64\n",
      " 1   GE_FOXA1     341 non-null    float64\n",
      " 2   GE_CTNNB1    341 non-null    float64\n",
      " 3   GE_CLPTM1L   341 non-null    float64\n",
      " 4   GE_DPYSL2    341 non-null    float64\n",
      " 5   GE_NEIL1     341 non-null    float64\n",
      " 6   GE_PITPNM2   341 non-null    float64\n",
      " 7   GE_ATM       341 non-null    float64\n",
      " 8   GE_EMG1      341 non-null    float64\n",
      " 9   GE_ETV3      341 non-null    float64\n",
      " 10  GE_BRAF      341 non-null    float64\n",
      " 11  GE_NKX3-1    341 non-null    float64\n",
      " 12  GE_ZMYM3     341 non-null    float64\n",
      " 13  GE_SALL1     341 non-null    float64\n",
      " 14  CNA_SPOP     341 non-null    float64\n",
      " 15  CNA_TP53     341 non-null    float64\n",
      " 16  CNA_FOXA1    341 non-null    float64\n",
      " 17  CNA_CTNNB1   341 non-null    float64\n",
      " 18  CNA_MED12    341 non-null    float64\n",
      " 19  CNA_CLPTM1L  341 non-null    float64\n",
      " 20  CNA_DPYSL2   341 non-null    float64\n",
      " 21  CNA_NEIL1    341 non-null    float64\n",
      " 22  CNA_SLC27A4  341 non-null    float64\n",
      " 23  CNA_PITPNM2  341 non-null    float64\n",
      " 24  CNA_PTEN     341 non-null    float64\n",
      " 25  CNA_ATM      341 non-null    float64\n",
      " 26  CNA_EMG1     341 non-null    float64\n",
      " 27  CNA_ETV3     341 non-null    float64\n",
      " 28  CNA_BRAF     341 non-null    float64\n",
      " 29  CNA_ZMYM3    341 non-null    float64\n",
      " 30  CNA_OR4P4    341 non-null    float64\n",
      " 31  CNA_SALL1    341 non-null    float64\n",
      " 32  DM_SPOP      341 non-null    float64\n",
      " 33  DM_FOXA1     341 non-null    float64\n",
      " 34  DM_CTNNB1    341 non-null    float64\n",
      " 35  DM_CLPTM1L   341 non-null    float64\n",
      " 36  DM_DPYSL2    341 non-null    float64\n",
      " 37  DM_NEIL1     341 non-null    float64\n",
      " 38  DM_SLC27A4   341 non-null    float64\n",
      " 39  DM_PITPNM2   341 non-null    float64\n",
      " 40  DM_PTEN      341 non-null    float64\n",
      " 41  DM_EMG1      341 non-null    float64\n",
      " 42  DM_ETV3      341 non-null    float64\n",
      " 43  DM_BRAF      341 non-null    float64\n",
      " 44  DM_NKX3-1    341 non-null    float64\n",
      " 45  DM_SALL1     341 non-null    float64\n",
      " 46  PATIENT_ID   341 non-null    object \n",
      " 47  TUMOR_STAGE  341 non-null    int64  \n",
      "dtypes: float64(46), int64(1), object(1)\n",
      "memory usage: 128.0+ KB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Checking the distribution of samples that are only present in 3 + 4 and 4 + 3 Gleason score classes**",
   "id": "9f913af1bb28d058"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.066133Z",
     "start_time": "2024-10-01T22:46:11.062037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Checking the no of samples beloning to tumor stage (Gleason Score): 3 + 4 and 4 + 3\n",
    "print(f\"No of samples having a Gleason score of 3 + 4: {(df1['TUMOR_STAGE'] == 34).value_counts()}\")\n",
    "print(f\"No of samples having a Gleason score of 4 + 3: {(df1['TUMOR_STAGE'] == 43).value_counts()}\")"
   ],
   "id": "7868649478e7acc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of samples having a Gleason score of 3 + 4: TUMOR_STAGE\n",
      "False    196\n",
      "True     145\n",
      "Name: count, dtype: int64\n",
      "No of samples having a Gleason score of 4 + 3: TUMOR_STAGE\n",
      "False    242\n",
      "True      99\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Note: From the above output it is evident that there is a class imbalance between 3 + 4 (145) and 4 + 3 classes (99.)**",
   "id": "a1e329b9af06ecb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.117022Z",
     "start_time": "2024-10-01T22:46:11.113622Z"
    }
   },
   "cell_type": "code",
   "source": "df1.shape # Out of 341 total samples 3 + 4 and 4 + 3 Gleason score samples account for 244 samples",
   "id": "602a999495dd0c98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**PatientID feature is dropped since it doesn't have any impact on the gleason score or any of the other Omics**",
   "id": "7d1a5905e4bb4558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.166549Z",
     "start_time": "2024-10-01T22:46:11.162696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Dropping the patient ID column\n",
    "df1.drop(['PATIENT_ID'], axis = 1, inplace = True)"
   ],
   "id": "8d4cffe8c3c01cf8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Reducing the gleason dataset to only have samples belonging to 3 + 4 and 4 + 3 classes**",
   "id": "be1446b6c67499d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.218563Z",
     "start_time": "2024-10-01T22:46:11.213943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1_gleason = df1[(df1['TUMOR_STAGE'] == 34) | (df1['TUMOR_STAGE'] == 43)]\n",
    "df1_gleason.shape"
   ],
   "id": "823a707800ec9a25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 47)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Converting the gleason scores from an int to an object to avoid bias**",
   "id": "3bf3a35aa3fa291a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.331759Z",
     "start_time": "2024-10-01T22:46:11.329172Z"
    }
   },
   "cell_type": "code",
   "source": "df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].astype(object)",
   "id": "9884bab9bfbf017",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2081367/1460778627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].astype(object)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.575249Z",
     "start_time": "2024-10-01T22:46:11.571024Z"
    }
   },
   "cell_type": "code",
   "source": "df1_gleason.info()",
   "id": "b96fb11eb53809e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 244 entries, 1 to 320\n",
      "Data columns (total 47 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      244 non-null    float64\n",
      " 1   GE_FOXA1     244 non-null    float64\n",
      " 2   GE_CTNNB1    244 non-null    float64\n",
      " 3   GE_CLPTM1L   244 non-null    float64\n",
      " 4   GE_DPYSL2    244 non-null    float64\n",
      " 5   GE_NEIL1     244 non-null    float64\n",
      " 6   GE_PITPNM2   244 non-null    float64\n",
      " 7   GE_ATM       244 non-null    float64\n",
      " 8   GE_EMG1      244 non-null    float64\n",
      " 9   GE_ETV3      244 non-null    float64\n",
      " 10  GE_BRAF      244 non-null    float64\n",
      " 11  GE_NKX3-1    244 non-null    float64\n",
      " 12  GE_ZMYM3     244 non-null    float64\n",
      " 13  GE_SALL1     244 non-null    float64\n",
      " 14  CNA_SPOP     244 non-null    float64\n",
      " 15  CNA_TP53     244 non-null    float64\n",
      " 16  CNA_FOXA1    244 non-null    float64\n",
      " 17  CNA_CTNNB1   244 non-null    float64\n",
      " 18  CNA_MED12    244 non-null    float64\n",
      " 19  CNA_CLPTM1L  244 non-null    float64\n",
      " 20  CNA_DPYSL2   244 non-null    float64\n",
      " 21  CNA_NEIL1    244 non-null    float64\n",
      " 22  CNA_SLC27A4  244 non-null    float64\n",
      " 23  CNA_PITPNM2  244 non-null    float64\n",
      " 24  CNA_PTEN     244 non-null    float64\n",
      " 25  CNA_ATM      244 non-null    float64\n",
      " 26  CNA_EMG1     244 non-null    float64\n",
      " 27  CNA_ETV3     244 non-null    float64\n",
      " 28  CNA_BRAF     244 non-null    float64\n",
      " 29  CNA_ZMYM3    244 non-null    float64\n",
      " 30  CNA_OR4P4    244 non-null    float64\n",
      " 31  CNA_SALL1    244 non-null    float64\n",
      " 32  DM_SPOP      244 non-null    float64\n",
      " 33  DM_FOXA1     244 non-null    float64\n",
      " 34  DM_CTNNB1    244 non-null    float64\n",
      " 35  DM_CLPTM1L   244 non-null    float64\n",
      " 36  DM_DPYSL2    244 non-null    float64\n",
      " 37  DM_NEIL1     244 non-null    float64\n",
      " 38  DM_SLC27A4   244 non-null    float64\n",
      " 39  DM_PITPNM2   244 non-null    float64\n",
      " 40  DM_PTEN      244 non-null    float64\n",
      " 41  DM_EMG1      244 non-null    float64\n",
      " 42  DM_ETV3      244 non-null    float64\n",
      " 43  DM_BRAF      244 non-null    float64\n",
      " 44  DM_NKX3-1    244 non-null    float64\n",
      " 45  DM_SALL1     244 non-null    float64\n",
      " 46  TUMOR_STAGE  244 non-null    object \n",
      "dtypes: float64(46), object(1)\n",
      "memory usage: 91.5+ KB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Using a custom function to encode 3 + 4 (Majority) class to 0 and 4 + 3 (Minority) to 1**",
   "id": "b42efb390d053ce3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.699833Z",
     "start_time": "2024-10-01T22:46:11.696589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Encoding 34 (majority class) as 0 and 43 (minority class) as 1\n",
    "def label_encode(target):\n",
    "    if target == 34:\n",
    "        target = 0\n",
    "    else:\n",
    "        target = 1\n",
    "    return target\n",
    "        \n",
    "df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].apply(label_encode)"
   ],
   "id": "783e5c6e95965685",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2081367/2632578046.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].apply(label_encode)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.775672Z",
     "start_time": "2024-10-01T22:46:11.772743Z"
    }
   },
   "cell_type": "code",
   "source": "df1_gleason['TUMOR_STAGE'].value_counts()",
   "id": "f41574feeab0ce46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "0    145\n",
       "1     99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Handling Class Imbalance using GANs**\n",
    "**Working of GANs**\n",
    "1. 2 main components: Generator & Discriminator\n",
    "2. Generator generates synthetic samples; Discriminator evaluates and checks the validity of the samples to check if they are fake or do they actually belong to the original dataset\n",
    "3. Both of these networks are trained in tandem in a process called `adversarial training`\n",
    "\n",
    "**Training steps**\n",
    "1. Divide the entire dataset into majority and minority class samples\n",
    "2. The majority class samples data is fed to the discriminator network as real data\n",
    "3. Generator network takes in random normal noise as input which has the input and output shape of the minority class dataset\n",
    "4. The Discriminator network takes in either minority class samples or synthetic samples generated by the generator to classify as real or fake\n",
    "5. Discriminator is trained on real minority samples and generated samples\n",
    "\n",
    "**Note: While Generator is being trained, the discriminator has to be frozen and vice-versa. In addition the input and output_dim for discriminator should match the no of columns in the dataframe and the target feature must be excluded**"
   ],
   "id": "1a2de896a369db5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Splitting the gleason dataset into majority and minority class samples**",
   "id": "f179fcbf458fb51b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.821166Z",
     "start_time": "2024-10-01T22:46:11.817780Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "#Getting the majority and minority datasets\n",
    "majority_dataset = df1_gleason[df1_gleason['TUMOR_STAGE'] == 0].drop(['TUMOR_STAGE'], axis = 1)\n",
    "minority_dataset = df1_gleason[df1_gleason['TUMOR_STAGE'] == 1].drop(['TUMOR_STAGE'], axis = 1)"
   ],
   "id": "f78d51e52d619e9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.875168Z",
     "start_time": "2024-10-01T22:46:11.865708Z"
    }
   },
   "cell_type": "code",
   "source": "majority_dataset",
   "id": "f8f9a6d92f9acdd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "3    0.433556  0.637116   0.777338    0.559615   0.476760  0.623626   \n",
       "5    0.540453  0.630588   0.671140    0.275536   0.662487  0.476065   \n",
       "6    0.506989  0.590670   0.727922    0.255442   0.733651  0.691024   \n",
       "8    0.476160  0.319457   0.926185    0.172214   0.711358  0.589873   \n",
       "10   0.503070  0.628456   0.872354    0.317441   0.428327  0.641957   \n",
       "..        ...       ...        ...         ...        ...       ...   \n",
       "291  0.227620  0.672938   0.530553    0.381428   0.175972  0.824114   \n",
       "297  0.258006  0.687300   0.757727    0.470806   0.394668  0.575239   \n",
       "299  0.485115  0.338921   0.309483    0.128955   0.510424  0.891032   \n",
       "301  0.624653  0.623661   0.434110    0.445462   0.492410  0.912847   \n",
       "303  0.356728  0.592104   0.851501    0.424060   0.337656  0.613548   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_DPYSL2  DM_NEIL1  \\\n",
       "3      0.415425  0.539748  0.466929  0.599673  ...   0.799996  0.111825   \n",
       "5      0.577890  0.824079  0.297729  0.899286  ...   0.785736  0.228759   \n",
       "6      0.398213  0.644989  0.383090  0.713497  ...   0.887808  0.129223   \n",
       "8      0.692148  0.797628  0.445715  0.904204  ...   0.824405  0.256412   \n",
       "10     0.624830  0.775454  0.426589  0.880784  ...   0.868720  0.153859   \n",
       "..          ...       ...       ...       ...  ...        ...       ...   \n",
       "291    0.690394  0.603873  0.581930  0.815111  ...   0.961014  0.007223   \n",
       "297    0.302726  0.603989  0.098050  0.789135  ...   0.921992  0.118404   \n",
       "299    0.612340  0.569947  0.697590  0.351627  ...   0.838034  0.188277   \n",
       "301    0.351620  0.441806  0.503556  0.462003  ...   0.848265  0.112395   \n",
       "303    0.443205  0.632808  0.187181  0.847817  ...   0.776066  0.194689   \n",
       "\n",
       "     DM_SLC27A4  DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  \\\n",
       "3      0.085465    0.569465  0.047191  0.101831  0.930270  0.221800   \n",
       "5      0.230212    0.831449  0.040868  0.266358  0.943859  0.178990   \n",
       "6      0.502728    0.836422  0.036482  0.328812  0.954992  0.185582   \n",
       "8      0.711538    0.562896  0.024710  0.636648  0.786474  0.227947   \n",
       "10     0.391933    0.866063  0.027305  0.472842  0.917519  0.216519   \n",
       "..          ...         ...       ...       ...       ...       ...   \n",
       "291    0.288589    0.872192  0.069334  0.125432  0.929922  0.232844   \n",
       "297    0.060399    0.655711  0.097734  0.271404  0.947257  0.292598   \n",
       "299    0.416333    0.627777  0.085632  0.461510  0.930563  0.284727   \n",
       "301    0.178172    0.669154  0.097933  0.662839  0.963417  0.212591   \n",
       "303    0.349741    0.732153  0.122011  0.586201  0.889234  0.208477   \n",
       "\n",
       "     DM_NKX3-1  DM_SALL1  \n",
       "3     0.257343  1.000000  \n",
       "5     0.505571  0.831493  \n",
       "6     0.391752  0.892649  \n",
       "8     0.863551  0.622275  \n",
       "10    0.427699  0.835883  \n",
       "..         ...       ...  \n",
       "291   0.000000  0.942248  \n",
       "297   0.266765  0.856432  \n",
       "299   0.549605  0.696472  \n",
       "301   0.444958  0.955913  \n",
       "303   0.560052  0.751077  \n",
       "\n",
       "[145 rows x 46 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_DPYSL2</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.476760</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799996</td>\n",
       "      <td>0.111825</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.662487</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785736</td>\n",
       "      <td>0.228759</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887808</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.476160</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>0.172214</td>\n",
       "      <td>0.711358</td>\n",
       "      <td>0.589873</td>\n",
       "      <td>0.692148</td>\n",
       "      <td>0.797628</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.904204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824405</td>\n",
       "      <td>0.256412</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.562896</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.786474</td>\n",
       "      <td>0.227947</td>\n",
       "      <td>0.863551</td>\n",
       "      <td>0.622275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.503070</td>\n",
       "      <td>0.628456</td>\n",
       "      <td>0.872354</td>\n",
       "      <td>0.317441</td>\n",
       "      <td>0.428327</td>\n",
       "      <td>0.641957</td>\n",
       "      <td>0.624830</td>\n",
       "      <td>0.775454</td>\n",
       "      <td>0.426589</td>\n",
       "      <td>0.880784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868720</td>\n",
       "      <td>0.153859</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>0.866063</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.472842</td>\n",
       "      <td>0.917519</td>\n",
       "      <td>0.216519</td>\n",
       "      <td>0.427699</td>\n",
       "      <td>0.835883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.672938</td>\n",
       "      <td>0.530553</td>\n",
       "      <td>0.381428</td>\n",
       "      <td>0.175972</td>\n",
       "      <td>0.824114</td>\n",
       "      <td>0.690394</td>\n",
       "      <td>0.603873</td>\n",
       "      <td>0.581930</td>\n",
       "      <td>0.815111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961014</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.288589</td>\n",
       "      <td>0.872192</td>\n",
       "      <td>0.069334</td>\n",
       "      <td>0.125432</td>\n",
       "      <td>0.929922</td>\n",
       "      <td>0.232844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.258006</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.757727</td>\n",
       "      <td>0.470806</td>\n",
       "      <td>0.394668</td>\n",
       "      <td>0.575239</td>\n",
       "      <td>0.302726</td>\n",
       "      <td>0.603989</td>\n",
       "      <td>0.098050</td>\n",
       "      <td>0.789135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921992</td>\n",
       "      <td>0.118404</td>\n",
       "      <td>0.060399</td>\n",
       "      <td>0.655711</td>\n",
       "      <td>0.097734</td>\n",
       "      <td>0.271404</td>\n",
       "      <td>0.947257</td>\n",
       "      <td>0.292598</td>\n",
       "      <td>0.266765</td>\n",
       "      <td>0.856432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.485115</td>\n",
       "      <td>0.338921</td>\n",
       "      <td>0.309483</td>\n",
       "      <td>0.128955</td>\n",
       "      <td>0.510424</td>\n",
       "      <td>0.891032</td>\n",
       "      <td>0.612340</td>\n",
       "      <td>0.569947</td>\n",
       "      <td>0.697590</td>\n",
       "      <td>0.351627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838034</td>\n",
       "      <td>0.188277</td>\n",
       "      <td>0.416333</td>\n",
       "      <td>0.627777</td>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.461510</td>\n",
       "      <td>0.930563</td>\n",
       "      <td>0.284727</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.696472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.624653</td>\n",
       "      <td>0.623661</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.445462</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>0.912847</td>\n",
       "      <td>0.351620</td>\n",
       "      <td>0.441806</td>\n",
       "      <td>0.503556</td>\n",
       "      <td>0.462003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848265</td>\n",
       "      <td>0.112395</td>\n",
       "      <td>0.178172</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.662839</td>\n",
       "      <td>0.963417</td>\n",
       "      <td>0.212591</td>\n",
       "      <td>0.444958</td>\n",
       "      <td>0.955913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.356728</td>\n",
       "      <td>0.592104</td>\n",
       "      <td>0.851501</td>\n",
       "      <td>0.424060</td>\n",
       "      <td>0.337656</td>\n",
       "      <td>0.613548</td>\n",
       "      <td>0.443205</td>\n",
       "      <td>0.632808</td>\n",
       "      <td>0.187181</td>\n",
       "      <td>0.847817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776066</td>\n",
       "      <td>0.194689</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.732153</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.586201</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.208477</td>\n",
       "      <td>0.560052</td>\n",
       "      <td>0.751077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 46 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:11.961452Z",
     "start_time": "2024-10-01T22:46:11.953144Z"
    }
   },
   "cell_type": "code",
   "source": "minority_dataset",
   "id": "93080f56f384deb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "1    0.373498  0.790415   0.648564    0.367439   0.000000  0.638505   \n",
       "4    0.363634  0.624428   0.892508    0.501788   0.472260  0.465397   \n",
       "9    0.418700  0.702095   0.745375    0.430890   0.487548  0.669876   \n",
       "11   0.470982  0.654804   0.799236    0.414669   0.421094  0.578125   \n",
       "15   0.535495  0.574125   0.990674    0.227795   0.686890  0.486809   \n",
       "..        ...       ...        ...         ...        ...       ...   \n",
       "307  0.678217  0.379478   0.651014    0.313037   0.727314  0.697200   \n",
       "308  0.570659  0.513166   0.756434    0.326435   0.636893  0.703464   \n",
       "309  0.405538  0.608860   0.570281    0.589831   0.408323  0.720283   \n",
       "315  0.372651  0.261140   0.643123    0.389611   0.907982  0.579044   \n",
       "320  0.540018  0.512559   0.602297    0.452478   0.621540  0.749010   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_DPYSL2  DM_NEIL1  \\\n",
       "1      0.156215  0.673907  0.384063  0.807230  ...   0.968367  0.034757   \n",
       "4      0.619784  0.562305  0.450817  0.729161  ...   0.863236  0.182983   \n",
       "9      0.441037  0.506928  0.140151  0.867808  ...   0.784092  0.181650   \n",
       "11     0.466010  0.715588  0.355391  0.813532  ...   0.850998  0.172506   \n",
       "15     0.735071  0.832965  0.387468  0.947819  ...   0.687718  0.330505   \n",
       "..          ...       ...       ...       ...  ...        ...       ...   \n",
       "307    0.609731  0.463844  0.473988  0.731562  ...   0.849849  0.155282   \n",
       "308    0.657117  0.559125  0.441622  0.738266  ...   0.811347  0.149038   \n",
       "309    0.288630  0.456215  0.602279  0.361531  ...   0.861432  0.155776   \n",
       "315    0.608729  0.577335  0.446111  0.763712  ...   0.832840  0.641021   \n",
       "320    0.749921  0.533543  0.472266  0.488479  ...   0.902571  0.226134   \n",
       "\n",
       "     DM_SLC27A4  DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  \\\n",
       "1      0.000000    0.791218  0.045264  0.090997  0.942279  0.257308   \n",
       "4      0.119817    0.907230  0.082071  0.155321  0.901625  0.171257   \n",
       "9      0.299419    0.605663  0.045999  0.422079  0.885389  0.238896   \n",
       "11     0.200640    0.782428  0.035299  0.285695  0.933832  0.175760   \n",
       "15     0.531055    0.546693  0.048915  0.458723  0.868237  0.251513   \n",
       "..          ...         ...       ...       ...       ...       ...   \n",
       "307    0.363361    0.564151  0.044499  0.272052  0.953258  0.111529   \n",
       "308    0.445528    0.621179  0.045135  0.693765  0.926586  0.133146   \n",
       "309    0.039771    0.750137  0.054077  0.262873  0.958069  0.289268   \n",
       "315    0.367391    0.477116  0.079954  0.354398  0.883972  0.342184   \n",
       "320    0.495551    0.639774  0.072104  0.484919  0.949089  0.337387   \n",
       "\n",
       "     DM_NKX3-1  DM_SALL1  \n",
       "1     0.093755  0.762329  \n",
       "4     0.201274  0.695265  \n",
       "9     0.548180  0.828444  \n",
       "11    0.263804  0.776006  \n",
       "15    0.654329  0.667877  \n",
       "..         ...       ...  \n",
       "307   0.416038  0.811600  \n",
       "308   0.672396  0.737615  \n",
       "309   0.356099  0.469004  \n",
       "315   0.509671  0.810917  \n",
       "320   0.444336  0.630323  \n",
       "\n",
       "[99 rows x 46 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_DPYSL2</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968367</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863236</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784092</td>\n",
       "      <td>0.181650</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.654804</td>\n",
       "      <td>0.799236</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.421094</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.466010</td>\n",
       "      <td>0.715588</td>\n",
       "      <td>0.355391</td>\n",
       "      <td>0.813532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850998</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>0.782428</td>\n",
       "      <td>0.035299</td>\n",
       "      <td>0.285695</td>\n",
       "      <td>0.933832</td>\n",
       "      <td>0.175760</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.776006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.535495</td>\n",
       "      <td>0.574125</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.227795</td>\n",
       "      <td>0.686890</td>\n",
       "      <td>0.486809</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.387468</td>\n",
       "      <td>0.947819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687718</td>\n",
       "      <td>0.330505</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.458723</td>\n",
       "      <td>0.868237</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.654329</td>\n",
       "      <td>0.667877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.678217</td>\n",
       "      <td>0.379478</td>\n",
       "      <td>0.651014</td>\n",
       "      <td>0.313037</td>\n",
       "      <td>0.727314</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.609731</td>\n",
       "      <td>0.463844</td>\n",
       "      <td>0.473988</td>\n",
       "      <td>0.731562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849849</td>\n",
       "      <td>0.155282</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>0.564151</td>\n",
       "      <td>0.044499</td>\n",
       "      <td>0.272052</td>\n",
       "      <td>0.953258</td>\n",
       "      <td>0.111529</td>\n",
       "      <td>0.416038</td>\n",
       "      <td>0.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.570659</td>\n",
       "      <td>0.513166</td>\n",
       "      <td>0.756434</td>\n",
       "      <td>0.326435</td>\n",
       "      <td>0.636893</td>\n",
       "      <td>0.703464</td>\n",
       "      <td>0.657117</td>\n",
       "      <td>0.559125</td>\n",
       "      <td>0.441622</td>\n",
       "      <td>0.738266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811347</td>\n",
       "      <td>0.149038</td>\n",
       "      <td>0.445528</td>\n",
       "      <td>0.621179</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.693765</td>\n",
       "      <td>0.926586</td>\n",
       "      <td>0.133146</td>\n",
       "      <td>0.672396</td>\n",
       "      <td>0.737615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.405538</td>\n",
       "      <td>0.608860</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>0.408323</td>\n",
       "      <td>0.720283</td>\n",
       "      <td>0.288630</td>\n",
       "      <td>0.456215</td>\n",
       "      <td>0.602279</td>\n",
       "      <td>0.361531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861432</td>\n",
       "      <td>0.155776</td>\n",
       "      <td>0.039771</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>0.054077</td>\n",
       "      <td>0.262873</td>\n",
       "      <td>0.958069</td>\n",
       "      <td>0.289268</td>\n",
       "      <td>0.356099</td>\n",
       "      <td>0.469004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.372651</td>\n",
       "      <td>0.261140</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.389611</td>\n",
       "      <td>0.907982</td>\n",
       "      <td>0.579044</td>\n",
       "      <td>0.608729</td>\n",
       "      <td>0.577335</td>\n",
       "      <td>0.446111</td>\n",
       "      <td>0.763712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832840</td>\n",
       "      <td>0.641021</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.477116</td>\n",
       "      <td>0.079954</td>\n",
       "      <td>0.354398</td>\n",
       "      <td>0.883972</td>\n",
       "      <td>0.342184</td>\n",
       "      <td>0.509671</td>\n",
       "      <td>0.810917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.540018</td>\n",
       "      <td>0.512559</td>\n",
       "      <td>0.602297</td>\n",
       "      <td>0.452478</td>\n",
       "      <td>0.621540</td>\n",
       "      <td>0.749010</td>\n",
       "      <td>0.749921</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>0.472266</td>\n",
       "      <td>0.488479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902571</td>\n",
       "      <td>0.226134</td>\n",
       "      <td>0.495551</td>\n",
       "      <td>0.639774</td>\n",
       "      <td>0.072104</td>\n",
       "      <td>0.484919</td>\n",
       "      <td>0.949089</td>\n",
       "      <td>0.337387</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.630323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 46 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Generator Network**",
   "id": "418d1f422a289665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:12.212193Z",
     "start_time": "2024-10-01T22:46:12.025280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Generator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "input_dim = 100\n",
    "output_dim = minority_dataset.shape[1]\n",
    "gen_input = Input(shape = (input_dim, ))\n",
    "fc_1 = Dense(128)(gen_input)\n",
    "lr_1 = LeakyReLU(alpha = 0.2)(fc_1)\n",
    "bn_1 = BatchNormalization(momentum = 0.8)(lr_1)\n",
    "fc_2 = Dense(256)(bn_1)\n",
    "lr_2 = LeakyReLU(alpha = 0.2)(fc_2)\n",
    "bn_2 = BatchNormalization(momentum = 0.8)(lr_2)\n",
    "fc_3 = Dense(512)(bn_2)\n",
    "lr_3 = LeakyReLU(alpha = 0.2)(fc_3)\n",
    "bn_3 = BatchNormalization(momentum = 0.8)(lr_3)\n",
    "gen_output = Dense(output_dim, activation = 'sigmoid')(bn_3)\n",
    "gen_model = Model(gen_input, gen_output)\n",
    "gen_model.summary()"
   ],
   "id": "77901552c023530b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               12928     \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204718 (799.68 KB)\n",
      "Trainable params: 202926 (792.68 KB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 18:46:12.039580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.039748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.039807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.090447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.090567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.090639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-01 18:46:12.090694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6257 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Discriminator Network**",
   "id": "a80834d50ee00d12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:12.273626Z",
     "start_time": "2024-10-01T22:46:12.237888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Discriminator\n",
    "dis_input = Input(shape = (output_dim, ))\n",
    "dis_fc_1 = Dense(512)(dis_input)\n",
    "dis_lr_1 = LeakyReLU(alpha = 0.2)(dis_fc_1)\n",
    "dis_dp_1 = Dropout(0.4)(dis_lr_1)\n",
    "dis_fc_2 = Dense(512)(dis_dp_1)\n",
    "dis_lr_2 = LeakyReLU(alpha = 0.2)(dis_fc_2)\n",
    "dis_dp_2 = Dropout(0.4)(dis_lr_2)\n",
    "dis_fc_3 = Dense(256)(dis_dp_2)\n",
    "dis_lr_3 = LeakyReLU(alpha = 0.2)(dis_fc_3)\n",
    "dis_dp_3 = Dropout(0.4)(dis_lr_3)\n",
    "dis_output = Dense(1, activation = 'sigmoid')(dis_dp_3)\n",
    "dis_model = Model(dis_input, dis_output)\n",
    "\n",
    "dis_model.summary()"
   ],
   "id": "27f18890f949c9d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 46)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               24064     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418305 (1.60 MB)\n",
      "Trainable params: 418305 (1.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:12.329536Z",
     "start_time": "2024-10-01T22:46:12.318499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Compiling the discriminator\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "dis_model.compile(loss = BinaryCrossentropy(), optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])"
   ],
   "id": "4077cfb21be0a9f5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**GAN Model**",
   "id": "a5bef18e0f38f9cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:12.402680Z",
     "start_time": "2024-10-01T22:46:12.372649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#GAN Model\n",
    "dis_model.trainable = False\n",
    "#Creating a latent vector layer\n",
    "latent_layer = Input(shape = (input_dim,))\n",
    "#Generated sample\n",
    "gen_sample = gen_model(latent_layer)\n",
    "#Forward Pass into discriminant\n",
    "dis_fp = dis_model(gen_sample)\n",
    "GAN_model = Model(latent_layer, dis_fp)\n",
    "GAN_model.compile(loss = BinaryCrossentropy(), optimizer = Adam(learning_rate = 0.001))"
   ],
   "id": "76e9d2df40db46c1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:12.431445Z",
     "start_time": "2024-10-01T22:46:12.423017Z"
    }
   },
   "cell_type": "code",
   "source": "GAN_model.summary()",
   "id": "471f71446cde9036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 46)                204718    \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 418305    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 623023 (2.38 MB)\n",
      "Trainable params: 202926 (792.68 KB)\n",
      "Non-trainable params: 420097 (1.60 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Training Loop**",
   "id": "387615a5f9a22668"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.439263Z",
     "start_time": "2024-10-01T22:46:12.454820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "from tqdm.auto import tqdm\n",
    "#Training phase for GAN\n",
    "for i in tqdm(range(epochs)):\n",
    "    #Getting a batch of samples from minority class\n",
    "    real_minority_samples = tf.convert_to_tensor(minority_dataset.sample(batch_size).values)\n",
    "    #Fake samples by forward pass of generator\n",
    "    random_latent_vector = tf.random.normal((batch_size, input_dim))\n",
    "    fake_minority_samples = gen_model(random_latent_vector)\n",
    "    #Creating tensors of 1s and 0s for True and Fake samples respectively\n",
    "    real_sample_labels = tf.ones(batch_size, 1)\n",
    "    fake_sample_labels = tf.zeros(batch_size, 1)\n",
    "    #Training Discriminator\n",
    "    dis_real_loss = dis_model.train_on_batch(real_minority_samples, real_sample_labels)\n",
    "    dis_fake_loss = dis_model.train_on_batch(fake_minority_samples, fake_sample_labels)\n",
    "    \n",
    "    #Training Generator\n",
    "    gen_latent_vector = tf.random.normal((batch_size, input_dim))\n",
    "    gen_label = tf.ones((batch_size, 1))\n",
    "    \n",
    "    GAN_loss = GAN_model.train_on_batch(gen_latent_vector, gen_label)\n",
    "    print(f'Epoch: {i + 1} | Discriminator Real Loss: {dis_real_loss} | Discriminator Fake Loss: {dis_fake_loss} | GAN_Loss: {GAN_loss}')"
   ],
   "id": "90fc2ccb674a388a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9557b3cf541d4c9ea356702d5780fc81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 18:46:12.502106: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-01 18:46:13.463827: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7d78ac44fe90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-01 18:46:13.463844: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-10-01 18:46:13.466437: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-01 18:46:13.473866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2024-10-01 18:46:13.526132: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Discriminator Real Loss: [0.929472804069519, 0.09375] | Discriminator Fake Loss: [1.1452064514160156, 0.0] | GAN_Loss: 0.46662989258766174\n",
      "Epoch: 2 | Discriminator Real Loss: [0.3952910304069519, 1.0] | Discriminator Fake Loss: [1.0459357500076294, 0.0] | GAN_Loss: 0.5179858803749084\n",
      "Epoch: 3 | Discriminator Real Loss: [0.3981388807296753, 1.0] | Discriminator Fake Loss: [0.8336026072502136, 0.03125] | GAN_Loss: 0.6533783674240112\n",
      "Epoch: 4 | Discriminator Real Loss: [0.44573119282722473, 1.0] | Discriminator Fake Loss: [0.7134455442428589, 0.375] | GAN_Loss: 0.7652132511138916\n",
      "Epoch: 5 | Discriminator Real Loss: [0.4555297791957855, 1.0] | Discriminator Fake Loss: [0.6134452819824219, 0.84375] | GAN_Loss: 0.8920823931694031\n",
      "Epoch: 6 | Discriminator Real Loss: [0.4237830638885498, 0.96875] | Discriminator Fake Loss: [0.5263268947601318, 0.9375] | GAN_Loss: 0.9927768111228943\n",
      "Epoch: 7 | Discriminator Real Loss: [0.4303426444530487, 1.0] | Discriminator Fake Loss: [0.45385462045669556, 0.96875] | GAN_Loss: 1.2412176132202148\n",
      "Epoch: 8 | Discriminator Real Loss: [0.4065781831741333, 0.96875] | Discriminator Fake Loss: [0.3299102187156677, 0.96875] | GAN_Loss: 1.5071378946304321\n",
      "Epoch: 9 | Discriminator Real Loss: [0.3401537239551544, 1.0] | Discriminator Fake Loss: [0.2915841042995453, 0.96875] | GAN_Loss: 1.7607234716415405\n",
      "Epoch: 10 | Discriminator Real Loss: [0.22806401550769806, 1.0] | Discriminator Fake Loss: [0.3195487856864929, 0.96875] | GAN_Loss: 1.8809261322021484\n",
      "Epoch: 11 | Discriminator Real Loss: [0.15339155495166779, 1.0] | Discriminator Fake Loss: [0.17437544465065002, 1.0] | GAN_Loss: 2.505012035369873\n",
      "Epoch: 12 | Discriminator Real Loss: [0.1043611466884613, 1.0] | Discriminator Fake Loss: [0.15172693133354187, 0.9375] | GAN_Loss: 3.2826972007751465\n",
      "Epoch: 13 | Discriminator Real Loss: [0.10390915721654892, 1.0] | Discriminator Fake Loss: [0.1128118634223938, 1.0] | GAN_Loss: 4.235940933227539\n",
      "Epoch: 14 | Discriminator Real Loss: [0.10100287199020386, 0.96875] | Discriminator Fake Loss: [0.039244554936885834, 1.0] | GAN_Loss: 4.67219352722168\n",
      "Epoch: 15 | Discriminator Real Loss: [0.09450069069862366, 1.0] | Discriminator Fake Loss: [0.08412133157253265, 0.96875] | GAN_Loss: 5.081169605255127\n",
      "Epoch: 16 | Discriminator Real Loss: [0.028014058247208595, 1.0] | Discriminator Fake Loss: [0.11021110415458679, 0.96875] | GAN_Loss: 6.41453742980957\n",
      "Epoch: 17 | Discriminator Real Loss: [0.08694297075271606, 0.96875] | Discriminator Fake Loss: [0.22239899635314941, 0.90625] | GAN_Loss: 6.4259185791015625\n",
      "Epoch: 18 | Discriminator Real Loss: [0.08249329030513763, 0.96875] | Discriminator Fake Loss: [0.13293451070785522, 0.96875] | GAN_Loss: 8.005306243896484\n",
      "Epoch: 19 | Discriminator Real Loss: [0.08508124202489853, 0.96875] | Discriminator Fake Loss: [0.39159131050109863, 0.90625] | GAN_Loss: 8.70419692993164\n",
      "Epoch: 20 | Discriminator Real Loss: [0.08492401242256165, 1.0] | Discriminator Fake Loss: [0.12760210037231445, 0.9375] | GAN_Loss: 8.8077974319458\n",
      "Epoch: 21 | Discriminator Real Loss: [0.27801671624183655, 0.90625] | Discriminator Fake Loss: [0.6485167145729065, 0.84375] | GAN_Loss: 9.081534385681152\n",
      "Epoch: 22 | Discriminator Real Loss: [0.0316036082804203, 1.0] | Discriminator Fake Loss: [0.3312452435493469, 0.875] | GAN_Loss: 9.37926959991455\n",
      "Epoch: 23 | Discriminator Real Loss: [0.18290388584136963, 0.9375] | Discriminator Fake Loss: [0.039241522550582886, 0.96875] | GAN_Loss: 9.594057083129883\n",
      "Epoch: 24 | Discriminator Real Loss: [0.08913333714008331, 1.0] | Discriminator Fake Loss: [0.3302456736564636, 0.90625] | GAN_Loss: 8.56347942352295\n",
      "Epoch: 25 | Discriminator Real Loss: [0.0868673324584961, 1.0] | Discriminator Fake Loss: [0.3280726671218872, 0.875] | GAN_Loss: 8.639164924621582\n",
      "Epoch: 26 | Discriminator Real Loss: [0.18941199779510498, 0.96875] | Discriminator Fake Loss: [0.649956464767456, 0.875] | GAN_Loss: 8.14896011352539\n",
      "Epoch: 27 | Discriminator Real Loss: [0.14223647117614746, 0.9375] | Discriminator Fake Loss: [0.1959054172039032, 0.9375] | GAN_Loss: 8.589394569396973\n",
      "Epoch: 28 | Discriminator Real Loss: [0.09920983016490936, 0.96875] | Discriminator Fake Loss: [0.3624723255634308, 0.90625] | GAN_Loss: 8.147024154663086\n",
      "Epoch: 29 | Discriminator Real Loss: [0.0715148001909256, 1.0] | Discriminator Fake Loss: [0.6452200412750244, 0.84375] | GAN_Loss: 8.615415573120117\n",
      "Epoch: 30 | Discriminator Real Loss: [0.15757112205028534, 0.96875] | Discriminator Fake Loss: [0.44504252076148987, 0.78125] | GAN_Loss: 9.155217170715332\n",
      "Epoch: 31 | Discriminator Real Loss: [0.531075119972229, 0.78125] | Discriminator Fake Loss: [0.34867793321609497, 0.84375] | GAN_Loss: 7.828468322753906\n",
      "Epoch: 32 | Discriminator Real Loss: [0.3260185718536377, 0.84375] | Discriminator Fake Loss: [0.4075963795185089, 0.84375] | GAN_Loss: 6.9513444900512695\n",
      "Epoch: 33 | Discriminator Real Loss: [0.14608019590377808, 0.96875] | Discriminator Fake Loss: [0.6867642998695374, 0.6875] | GAN_Loss: 6.814352989196777\n",
      "Epoch: 34 | Discriminator Real Loss: [0.15117144584655762, 0.96875] | Discriminator Fake Loss: [0.27232879400253296, 0.84375] | GAN_Loss: 6.526398181915283\n",
      "Epoch: 35 | Discriminator Real Loss: [0.14880678057670593, 1.0] | Discriminator Fake Loss: [0.1572118103504181, 0.96875] | GAN_Loss: 6.893058776855469\n",
      "Epoch: 36 | Discriminator Real Loss: [0.1790042221546173, 0.96875] | Discriminator Fake Loss: [0.2277286946773529, 0.875] | GAN_Loss: 6.983658313751221\n",
      "Epoch: 37 | Discriminator Real Loss: [0.24124065041542053, 0.90625] | Discriminator Fake Loss: [0.20831730961799622, 0.9375] | GAN_Loss: 6.574154853820801\n",
      "Epoch: 38 | Discriminator Real Loss: [0.19088783860206604, 0.96875] | Discriminator Fake Loss: [0.18872570991516113, 0.8125] | GAN_Loss: 6.011640548706055\n",
      "Epoch: 39 | Discriminator Real Loss: [0.22116446495056152, 0.9375] | Discriminator Fake Loss: [0.305553674697876, 0.84375] | GAN_Loss: 5.674468040466309\n",
      "Epoch: 40 | Discriminator Real Loss: [0.09343979507684708, 1.0] | Discriminator Fake Loss: [0.11778438091278076, 0.9375] | GAN_Loss: 5.562750339508057\n",
      "Epoch: 41 | Discriminator Real Loss: [0.08945968747138977, 1.0] | Discriminator Fake Loss: [0.2006075531244278, 0.90625] | GAN_Loss: 6.198490619659424\n",
      "Epoch: 42 | Discriminator Real Loss: [0.12572148442268372, 0.96875] | Discriminator Fake Loss: [0.09853655099868774, 0.90625] | GAN_Loss: 6.151031494140625\n",
      "Epoch: 43 | Discriminator Real Loss: [0.12372373044490814, 1.0] | Discriminator Fake Loss: [0.13453972339630127, 0.96875] | GAN_Loss: 6.346671104431152\n",
      "Epoch: 44 | Discriminator Real Loss: [0.13667994737625122, 1.0] | Discriminator Fake Loss: [0.23721083998680115, 0.90625] | GAN_Loss: 5.883870601654053\n",
      "Epoch: 45 | Discriminator Real Loss: [0.13249555230140686, 0.96875] | Discriminator Fake Loss: [0.15824082493782043, 0.875] | GAN_Loss: 6.8366193771362305\n",
      "Epoch: 46 | Discriminator Real Loss: [0.1330622136592865, 0.96875] | Discriminator Fake Loss: [0.12751555442810059, 0.9375] | GAN_Loss: 6.148792743682861\n",
      "Epoch: 47 | Discriminator Real Loss: [0.12102890014648438, 0.96875] | Discriminator Fake Loss: [0.24633312225341797, 0.84375] | GAN_Loss: 7.5912299156188965\n",
      "Epoch: 48 | Discriminator Real Loss: [0.13193517923355103, 1.0] | Discriminator Fake Loss: [0.07218141108751297, 0.96875] | GAN_Loss: 7.479372024536133\n",
      "Epoch: 49 | Discriminator Real Loss: [0.15336784720420837, 0.96875] | Discriminator Fake Loss: [0.051704466342926025, 1.0] | GAN_Loss: 6.074549674987793\n",
      "Epoch: 50 | Discriminator Real Loss: [0.07399614155292511, 1.0] | Discriminator Fake Loss: [0.2067238986492157, 0.90625] | GAN_Loss: 6.719478130340576\n",
      "Epoch: 51 | Discriminator Real Loss: [0.0708438977599144, 1.0] | Discriminator Fake Loss: [0.17150673270225525, 0.90625] | GAN_Loss: 6.403195381164551\n",
      "Epoch: 52 | Discriminator Real Loss: [0.07519126683473587, 0.96875] | Discriminator Fake Loss: [0.16770011186599731, 0.9375] | GAN_Loss: 7.033626079559326\n",
      "Epoch: 53 | Discriminator Real Loss: [0.2265293151140213, 0.9375] | Discriminator Fake Loss: [0.28867796063423157, 0.84375] | GAN_Loss: 8.317245483398438\n",
      "Epoch: 54 | Discriminator Real Loss: [0.0808873325586319, 0.96875] | Discriminator Fake Loss: [0.4081498086452484, 0.84375] | GAN_Loss: 8.565652847290039\n",
      "Epoch: 55 | Discriminator Real Loss: [0.3138701915740967, 0.875] | Discriminator Fake Loss: [0.36002013087272644, 0.8125] | GAN_Loss: 8.496997833251953\n",
      "Epoch: 56 | Discriminator Real Loss: [0.2673472762107849, 0.84375] | Discriminator Fake Loss: [0.34491702914237976, 0.84375] | GAN_Loss: 6.817315101623535\n",
      "Epoch: 57 | Discriminator Real Loss: [0.18147552013397217, 0.96875] | Discriminator Fake Loss: [0.4093233644962311, 0.84375] | GAN_Loss: 7.175529956817627\n",
      "Epoch: 58 | Discriminator Real Loss: [0.20375338196754456, 0.9375] | Discriminator Fake Loss: [0.26435601711273193, 0.875] | GAN_Loss: 7.095722198486328\n",
      "Epoch: 59 | Discriminator Real Loss: [0.16076378524303436, 1.0] | Discriminator Fake Loss: [0.3253258764743805, 0.84375] | GAN_Loss: 7.160253524780273\n",
      "Epoch: 60 | Discriminator Real Loss: [0.31565189361572266, 0.9375] | Discriminator Fake Loss: [0.36891770362854004, 0.78125] | GAN_Loss: 6.472830295562744\n",
      "Epoch: 61 | Discriminator Real Loss: [0.2807021141052246, 0.90625] | Discriminator Fake Loss: [0.33696579933166504, 0.78125] | GAN_Loss: 6.34945011138916\n",
      "Epoch: 62 | Discriminator Real Loss: [0.3535629212856293, 0.90625] | Discriminator Fake Loss: [0.46206510066986084, 0.84375] | GAN_Loss: 5.661857604980469\n",
      "Epoch: 63 | Discriminator Real Loss: [0.21305367350578308, 0.90625] | Discriminator Fake Loss: [0.5958505272865295, 0.75] | GAN_Loss: 4.859868049621582\n",
      "Epoch: 64 | Discriminator Real Loss: [0.1395827829837799, 0.96875] | Discriminator Fake Loss: [0.5121838450431824, 0.78125] | GAN_Loss: 6.284305095672607\n",
      "Epoch: 65 | Discriminator Real Loss: [0.34781613945961, 0.875] | Discriminator Fake Loss: [0.43454208970069885, 0.84375] | GAN_Loss: 4.964546203613281\n",
      "Epoch: 66 | Discriminator Real Loss: [0.34868770837783813, 0.875] | Discriminator Fake Loss: [0.20029190182685852, 0.90625] | GAN_Loss: 5.082786560058594\n",
      "Epoch: 67 | Discriminator Real Loss: [0.34078216552734375, 0.9375] | Discriminator Fake Loss: [0.5497256517410278, 0.6875] | GAN_Loss: 4.449173927307129\n",
      "Epoch: 68 | Discriminator Real Loss: [0.21242138743400574, 0.96875] | Discriminator Fake Loss: [0.49423205852508545, 0.71875] | GAN_Loss: 4.149366855621338\n",
      "Epoch: 69 | Discriminator Real Loss: [0.37795937061309814, 0.875] | Discriminator Fake Loss: [0.19806501269340515, 0.90625] | GAN_Loss: 4.535672187805176\n",
      "Epoch: 70 | Discriminator Real Loss: [0.3981107771396637, 0.875] | Discriminator Fake Loss: [0.513714075088501, 0.6875] | GAN_Loss: 4.028804779052734\n",
      "Epoch: 71 | Discriminator Real Loss: [0.23840954899787903, 0.9375] | Discriminator Fake Loss: [0.3694688081741333, 0.8125] | GAN_Loss: 3.6289961338043213\n",
      "Epoch: 72 | Discriminator Real Loss: [0.2244175374507904, 0.9375] | Discriminator Fake Loss: [0.3105485141277313, 0.84375] | GAN_Loss: 3.9817516803741455\n",
      "Epoch: 73 | Discriminator Real Loss: [0.3014765679836273, 0.9375] | Discriminator Fake Loss: [0.25049513578414917, 0.875] | GAN_Loss: 4.084535598754883\n",
      "Epoch: 74 | Discriminator Real Loss: [0.3032458424568176, 0.90625] | Discriminator Fake Loss: [0.5028666257858276, 0.75] | GAN_Loss: 4.043664455413818\n",
      "Epoch: 75 | Discriminator Real Loss: [0.25164711475372314, 0.96875] | Discriminator Fake Loss: [0.3501231074333191, 0.84375] | GAN_Loss: 3.4899344444274902\n",
      "Epoch: 76 | Discriminator Real Loss: [0.3628009855747223, 0.875] | Discriminator Fake Loss: [0.29242372512817383, 0.875] | GAN_Loss: 3.5471200942993164\n",
      "Epoch: 77 | Discriminator Real Loss: [0.25248563289642334, 0.90625] | Discriminator Fake Loss: [0.2944340705871582, 0.875] | GAN_Loss: 2.8523895740509033\n",
      "Epoch: 78 | Discriminator Real Loss: [0.27908825874328613, 0.9375] | Discriminator Fake Loss: [0.5280792117118835, 0.625] | GAN_Loss: 3.028456211090088\n",
      "Epoch: 79 | Discriminator Real Loss: [0.2492896020412445, 0.96875] | Discriminator Fake Loss: [0.3608977198600769, 0.8125] | GAN_Loss: 3.843453884124756\n",
      "Epoch: 80 | Discriminator Real Loss: [0.408530592918396, 0.8125] | Discriminator Fake Loss: [0.23588323593139648, 0.875] | GAN_Loss: 3.880976676940918\n",
      "Epoch: 81 | Discriminator Real Loss: [0.4890650510787964, 0.78125] | Discriminator Fake Loss: [0.38227003812789917, 0.8125] | GAN_Loss: 3.3685615062713623\n",
      "Epoch: 82 | Discriminator Real Loss: [0.310171902179718, 0.875] | Discriminator Fake Loss: [0.5738610625267029, 0.6875] | GAN_Loss: 2.7155795097351074\n",
      "Epoch: 83 | Discriminator Real Loss: [0.19936653971672058, 1.0] | Discriminator Fake Loss: [0.5179554224014282, 0.71875] | GAN_Loss: 3.30623197555542\n",
      "Epoch: 84 | Discriminator Real Loss: [0.3645857572555542, 0.875] | Discriminator Fake Loss: [0.13229835033416748, 0.90625] | GAN_Loss: 3.7511589527130127\n",
      "Epoch: 85 | Discriminator Real Loss: [0.3358727991580963, 0.875] | Discriminator Fake Loss: [0.20806381106376648, 0.875] | GAN_Loss: 3.0140528678894043\n",
      "Epoch: 86 | Discriminator Real Loss: [0.438542902469635, 0.78125] | Discriminator Fake Loss: [0.43558529019355774, 0.71875] | GAN_Loss: 3.0568346977233887\n",
      "Epoch: 87 | Discriminator Real Loss: [0.2755832374095917, 0.90625] | Discriminator Fake Loss: [0.18308162689208984, 0.90625] | GAN_Loss: 2.7794570922851562\n",
      "Epoch: 88 | Discriminator Real Loss: [0.2508572041988373, 0.9375] | Discriminator Fake Loss: [0.29226982593536377, 0.90625] | GAN_Loss: 3.0058364868164062\n",
      "Epoch: 89 | Discriminator Real Loss: [0.3209663927555084, 0.90625] | Discriminator Fake Loss: [0.3333497643470764, 0.84375] | GAN_Loss: 3.413022994995117\n",
      "Epoch: 90 | Discriminator Real Loss: [0.24339070916175842, 0.9375] | Discriminator Fake Loss: [0.3930584490299225, 0.84375] | GAN_Loss: 3.2116408348083496\n",
      "Epoch: 91 | Discriminator Real Loss: [0.2478182315826416, 0.9375] | Discriminator Fake Loss: [0.20202594995498657, 0.90625] | GAN_Loss: 3.710474967956543\n",
      "Epoch: 92 | Discriminator Real Loss: [0.5022379755973816, 0.78125] | Discriminator Fake Loss: [0.2610929608345032, 0.84375] | GAN_Loss: 3.459468126296997\n",
      "Epoch: 93 | Discriminator Real Loss: [0.2593179941177368, 0.9375] | Discriminator Fake Loss: [0.34075021743774414, 0.90625] | GAN_Loss: 3.2768125534057617\n",
      "Epoch: 94 | Discriminator Real Loss: [0.29192277789115906, 0.90625] | Discriminator Fake Loss: [0.327780544757843, 0.8125] | GAN_Loss: 3.3328473567962646\n",
      "Epoch: 95 | Discriminator Real Loss: [0.3272625207901001, 0.875] | Discriminator Fake Loss: [0.38865333795547485, 0.8125] | GAN_Loss: 4.258780479431152\n",
      "Epoch: 96 | Discriminator Real Loss: [0.43105149269104004, 0.84375] | Discriminator Fake Loss: [0.5351327657699585, 0.75] | GAN_Loss: 3.548100709915161\n",
      "Epoch: 97 | Discriminator Real Loss: [0.6029241681098938, 0.71875] | Discriminator Fake Loss: [0.31232401728630066, 0.84375] | GAN_Loss: 2.970085382461548\n",
      "Epoch: 98 | Discriminator Real Loss: [0.3926350474357605, 0.84375] | Discriminator Fake Loss: [0.42329609394073486, 0.75] | GAN_Loss: 2.5680840015411377\n",
      "Epoch: 99 | Discriminator Real Loss: [0.3221290707588196, 0.8125] | Discriminator Fake Loss: [0.4488682746887207, 0.8125] | GAN_Loss: 2.867281913757324\n",
      "Epoch: 100 | Discriminator Real Loss: [0.3506738543510437, 0.875] | Discriminator Fake Loss: [0.3665658235549927, 0.8125] | GAN_Loss: 3.6945414543151855\n",
      "Epoch: 101 | Discriminator Real Loss: [0.8135015964508057, 0.65625] | Discriminator Fake Loss: [0.28808721899986267, 0.8125] | GAN_Loss: 2.6107542514801025\n",
      "Epoch: 102 | Discriminator Real Loss: [0.3140365779399872, 0.90625] | Discriminator Fake Loss: [0.5664405226707458, 0.75] | GAN_Loss: 1.6075366735458374\n",
      "Epoch: 103 | Discriminator Real Loss: [0.4742063283920288, 0.75] | Discriminator Fake Loss: [1.0366542339324951, 0.4375] | GAN_Loss: 1.8762743473052979\n",
      "Epoch: 104 | Discriminator Real Loss: [0.40647298097610474, 0.84375] | Discriminator Fake Loss: [0.3260468542575836, 0.84375] | GAN_Loss: 2.711190938949585\n",
      "Epoch: 105 | Discriminator Real Loss: [0.6894879937171936, 0.6875] | Discriminator Fake Loss: [0.29305940866470337, 0.78125] | GAN_Loss: 2.494363307952881\n",
      "Epoch: 106 | Discriminator Real Loss: [0.4595906138420105, 0.8125] | Discriminator Fake Loss: [0.3918222188949585, 0.75] | GAN_Loss: 1.676716923713684\n",
      "Epoch: 107 | Discriminator Real Loss: [0.3437131941318512, 0.90625] | Discriminator Fake Loss: [0.571390688419342, 0.71875] | GAN_Loss: 1.9675800800323486\n",
      "Epoch: 108 | Discriminator Real Loss: [0.20730990171432495, 1.0] | Discriminator Fake Loss: [0.34322839975357056, 0.84375] | GAN_Loss: 3.155076026916504\n",
      "Epoch: 109 | Discriminator Real Loss: [0.41925328969955444, 0.84375] | Discriminator Fake Loss: [0.1532132923603058, 0.96875] | GAN_Loss: 3.2574827671051025\n",
      "Epoch: 110 | Discriminator Real Loss: [0.44970595836639404, 0.75] | Discriminator Fake Loss: [0.32296159863471985, 0.84375] | GAN_Loss: 2.4999160766601562\n",
      "Epoch: 111 | Discriminator Real Loss: [0.34765392541885376, 0.90625] | Discriminator Fake Loss: [0.4328271150588989, 0.75] | GAN_Loss: 2.2924318313598633\n",
      "Epoch: 112 | Discriminator Real Loss: [0.23203179240226746, 0.96875] | Discriminator Fake Loss: [0.4232507348060608, 0.84375] | GAN_Loss: 2.511047840118408\n",
      "Epoch: 113 | Discriminator Real Loss: [0.27693960070610046, 0.96875] | Discriminator Fake Loss: [0.33256417512893677, 0.8125] | GAN_Loss: 2.9498939514160156\n",
      "Epoch: 114 | Discriminator Real Loss: [0.3904106616973877, 0.9375] | Discriminator Fake Loss: [0.33363473415374756, 0.84375] | GAN_Loss: 3.260498523712158\n",
      "Epoch: 115 | Discriminator Real Loss: [0.2342318296432495, 0.96875] | Discriminator Fake Loss: [0.2765922546386719, 0.875] | GAN_Loss: 2.8809704780578613\n",
      "Epoch: 116 | Discriminator Real Loss: [0.4357811212539673, 0.8125] | Discriminator Fake Loss: [0.34589430689811707, 0.8125] | GAN_Loss: 2.142983913421631\n",
      "Epoch: 117 | Discriminator Real Loss: [0.28751224279403687, 0.9375] | Discriminator Fake Loss: [0.39516621828079224, 0.78125] | GAN_Loss: 2.760420799255371\n",
      "Epoch: 118 | Discriminator Real Loss: [0.2413131296634674, 0.9375] | Discriminator Fake Loss: [0.3075208067893982, 0.875] | GAN_Loss: 3.7253642082214355\n",
      "Epoch: 119 | Discriminator Real Loss: [0.5657687187194824, 0.78125] | Discriminator Fake Loss: [0.28744691610336304, 0.875] | GAN_Loss: 2.580662488937378\n",
      "Epoch: 120 | Discriminator Real Loss: [0.39829033613204956, 0.875] | Discriminator Fake Loss: [0.30450165271759033, 0.90625] | GAN_Loss: 2.017909526824951\n",
      "Epoch: 121 | Discriminator Real Loss: [0.17525771260261536, 0.96875] | Discriminator Fake Loss: [0.6181787252426147, 0.53125] | GAN_Loss: 2.749034881591797\n",
      "Epoch: 122 | Discriminator Real Loss: [0.3399531841278076, 0.84375] | Discriminator Fake Loss: [0.24470388889312744, 0.84375] | GAN_Loss: 2.5345633029937744\n",
      "Epoch: 123 | Discriminator Real Loss: [0.4589921832084656, 0.84375] | Discriminator Fake Loss: [0.18722333014011383, 0.90625] | GAN_Loss: 3.1357779502868652\n",
      "Epoch: 124 | Discriminator Real Loss: [0.4749319851398468, 0.90625] | Discriminator Fake Loss: [0.29625949263572693, 0.90625] | GAN_Loss: 2.2462105751037598\n",
      "Epoch: 125 | Discriminator Real Loss: [0.17712906002998352, 0.96875] | Discriminator Fake Loss: [0.3285506069660187, 0.84375] | GAN_Loss: 1.964744210243225\n",
      "Epoch: 126 | Discriminator Real Loss: [0.12414885312318802, 1.0] | Discriminator Fake Loss: [0.4183303117752075, 0.8125] | GAN_Loss: 3.3302106857299805\n",
      "Epoch: 127 | Discriminator Real Loss: [0.3387596011161804, 0.90625] | Discriminator Fake Loss: [0.44718828797340393, 0.78125] | GAN_Loss: 5.013946533203125\n",
      "Epoch: 128 | Discriminator Real Loss: [0.9868537187576294, 0.46875] | Discriminator Fake Loss: [0.3231741786003113, 0.84375] | GAN_Loss: 3.964733362197876\n",
      "Epoch: 129 | Discriminator Real Loss: [0.3169798254966736, 0.9375] | Discriminator Fake Loss: [0.517458975315094, 0.6875] | GAN_Loss: 2.5083508491516113\n",
      "Epoch: 130 | Discriminator Real Loss: [0.17445142567157745, 0.96875] | Discriminator Fake Loss: [0.4574280083179474, 0.8125] | GAN_Loss: 3.683300018310547\n",
      "Epoch: 131 | Discriminator Real Loss: [0.23105785250663757, 0.9375] | Discriminator Fake Loss: [0.2563167214393616, 0.90625] | GAN_Loss: 4.271892547607422\n",
      "Epoch: 132 | Discriminator Real Loss: [0.6351751089096069, 0.78125] | Discriminator Fake Loss: [0.14302876591682434, 0.96875] | GAN_Loss: 3.4982686042785645\n",
      "Epoch: 133 | Discriminator Real Loss: [0.26326578855514526, 0.90625] | Discriminator Fake Loss: [0.4584733247756958, 0.71875] | GAN_Loss: 4.082498073577881\n",
      "Epoch: 134 | Discriminator Real Loss: [0.36704879999160767, 0.90625] | Discriminator Fake Loss: [0.28114256262779236, 0.875] | GAN_Loss: 3.962496519088745\n",
      "Epoch: 135 | Discriminator Real Loss: [0.34885603189468384, 0.96875] | Discriminator Fake Loss: [0.20137418806552887, 0.90625] | GAN_Loss: 3.389878273010254\n",
      "Epoch: 136 | Discriminator Real Loss: [0.44502976536750793, 0.8125] | Discriminator Fake Loss: [0.6011953353881836, 0.75] | GAN_Loss: 2.144836902618408\n",
      "Epoch: 137 | Discriminator Real Loss: [0.19824427366256714, 0.96875] | Discriminator Fake Loss: [0.5771410465240479, 0.75] | GAN_Loss: 3.575963020324707\n",
      "Epoch: 138 | Discriminator Real Loss: [0.36568617820739746, 0.8125] | Discriminator Fake Loss: [0.3143797516822815, 0.8125] | GAN_Loss: 3.8713674545288086\n",
      "Epoch: 139 | Discriminator Real Loss: [0.6571201086044312, 0.65625] | Discriminator Fake Loss: [0.14984747767448425, 0.9375] | GAN_Loss: 3.2085227966308594\n",
      "Epoch: 140 | Discriminator Real Loss: [0.29822680354118347, 0.90625] | Discriminator Fake Loss: [0.7199051380157471, 0.59375] | GAN_Loss: 2.6503400802612305\n",
      "Epoch: 141 | Discriminator Real Loss: [0.2941020131111145, 0.90625] | Discriminator Fake Loss: [0.33034273982048035, 0.84375] | GAN_Loss: 3.805347442626953\n",
      "Epoch: 142 | Discriminator Real Loss: [0.42843061685562134, 0.75] | Discriminator Fake Loss: [0.1833614706993103, 0.9375] | GAN_Loss: 3.4649274349212646\n",
      "Epoch: 143 | Discriminator Real Loss: [0.5840505361557007, 0.75] | Discriminator Fake Loss: [0.1727183312177658, 0.9375] | GAN_Loss: 2.963592052459717\n",
      "Epoch: 144 | Discriminator Real Loss: [0.2712215185165405, 0.875] | Discriminator Fake Loss: [0.4148043394088745, 0.75] | GAN_Loss: 2.98335337638855\n",
      "Epoch: 145 | Discriminator Real Loss: [0.16292178630828857, 0.96875] | Discriminator Fake Loss: [0.4589712917804718, 0.78125] | GAN_Loss: 3.375697135925293\n",
      "Epoch: 146 | Discriminator Real Loss: [0.364175021648407, 0.90625] | Discriminator Fake Loss: [0.26628679037094116, 0.8125] | GAN_Loss: 3.5781965255737305\n",
      "Epoch: 147 | Discriminator Real Loss: [0.36725783348083496, 0.875] | Discriminator Fake Loss: [0.2046562284231186, 0.9375] | GAN_Loss: 3.3230106830596924\n",
      "Epoch: 148 | Discriminator Real Loss: [0.24874266982078552, 0.9375] | Discriminator Fake Loss: [0.20952001214027405, 0.90625] | GAN_Loss: 3.2555994987487793\n",
      "Epoch: 149 | Discriminator Real Loss: [0.12901738286018372, 1.0] | Discriminator Fake Loss: [0.32021600008010864, 0.84375] | GAN_Loss: 3.260094404220581\n",
      "Epoch: 150 | Discriminator Real Loss: [0.2681037187576294, 0.9375] | Discriminator Fake Loss: [0.2386903166770935, 0.90625] | GAN_Loss: 3.7727420330047607\n",
      "Epoch: 151 | Discriminator Real Loss: [0.4390726387500763, 0.875] | Discriminator Fake Loss: [0.14852803945541382, 0.9375] | GAN_Loss: 3.877063751220703\n",
      "Epoch: 152 | Discriminator Real Loss: [0.25755131244659424, 0.96875] | Discriminator Fake Loss: [0.3827096223831177, 0.8125] | GAN_Loss: 3.9105725288391113\n",
      "Epoch: 153 | Discriminator Real Loss: [0.1767657995223999, 1.0] | Discriminator Fake Loss: [0.30360203981399536, 0.84375] | GAN_Loss: 4.355648517608643\n",
      "Epoch: 154 | Discriminator Real Loss: [0.25777435302734375, 0.9375] | Discriminator Fake Loss: [0.3326030969619751, 0.90625] | GAN_Loss: 4.946442604064941\n",
      "Epoch: 155 | Discriminator Real Loss: [0.4245506525039673, 0.8125] | Discriminator Fake Loss: [0.19429384171962738, 0.90625] | GAN_Loss: 4.492975234985352\n",
      "Epoch: 156 | Discriminator Real Loss: [0.3055223524570465, 0.875] | Discriminator Fake Loss: [0.7899774312973022, 0.6875] | GAN_Loss: 3.9172840118408203\n",
      "Epoch: 157 | Discriminator Real Loss: [0.21707385778427124, 1.0] | Discriminator Fake Loss: [0.16133712232112885, 0.90625] | GAN_Loss: 5.101944446563721\n",
      "Epoch: 158 | Discriminator Real Loss: [0.31512200832366943, 0.90625] | Discriminator Fake Loss: [0.19369708001613617, 0.9375] | GAN_Loss: 4.646426200866699\n",
      "Epoch: 159 | Discriminator Real Loss: [0.5027434229850769, 0.84375] | Discriminator Fake Loss: [0.37201231718063354, 0.8125] | GAN_Loss: 2.7555742263793945\n",
      "Epoch: 160 | Discriminator Real Loss: [0.2650523781776428, 0.96875] | Discriminator Fake Loss: [0.9226030111312866, 0.5625] | GAN_Loss: 3.9763426780700684\n",
      "Epoch: 161 | Discriminator Real Loss: [0.36368152499198914, 0.9375] | Discriminator Fake Loss: [0.26202043890953064, 0.90625] | GAN_Loss: 4.6213178634643555\n",
      "Epoch: 162 | Discriminator Real Loss: [0.76993328332901, 0.625] | Discriminator Fake Loss: [0.15359210968017578, 0.9375] | GAN_Loss: 3.7120375633239746\n",
      "Epoch: 163 | Discriminator Real Loss: [0.41751599311828613, 0.875] | Discriminator Fake Loss: [0.546217143535614, 0.75] | GAN_Loss: 2.722994804382324\n",
      "Epoch: 164 | Discriminator Real Loss: [0.12320820242166519, 1.0] | Discriminator Fake Loss: [0.3882397413253784, 0.875] | GAN_Loss: 3.3533573150634766\n",
      "Epoch: 165 | Discriminator Real Loss: [0.22441594302654266, 0.96875] | Discriminator Fake Loss: [0.21362854540348053, 0.875] | GAN_Loss: 4.645204544067383\n",
      "Epoch: 166 | Discriminator Real Loss: [0.31852033734321594, 0.875] | Discriminator Fake Loss: [0.22247374057769775, 0.90625] | GAN_Loss: 4.2301025390625\n",
      "Epoch: 167 | Discriminator Real Loss: [0.502109706401825, 0.8125] | Discriminator Fake Loss: [0.15217521786689758, 0.90625] | GAN_Loss: 4.161687850952148\n",
      "Epoch: 168 | Discriminator Real Loss: [0.1653708815574646, 0.96875] | Discriminator Fake Loss: [0.2280975580215454, 0.90625] | GAN_Loss: 4.082927703857422\n",
      "Epoch: 169 | Discriminator Real Loss: [0.138784259557724, 0.96875] | Discriminator Fake Loss: [0.14124682545661926, 0.96875] | GAN_Loss: 4.333405494689941\n",
      "Epoch: 170 | Discriminator Real Loss: [0.14898675680160522, 1.0] | Discriminator Fake Loss: [0.2669910192489624, 0.84375] | GAN_Loss: 4.241835594177246\n",
      "Epoch: 171 | Discriminator Real Loss: [0.16615858674049377, 0.96875] | Discriminator Fake Loss: [0.10880814492702484, 0.9375] | GAN_Loss: 4.517453193664551\n",
      "Epoch: 172 | Discriminator Real Loss: [0.17886747419834137, 0.96875] | Discriminator Fake Loss: [0.33061206340789795, 0.84375] | GAN_Loss: 4.8140740394592285\n",
      "Epoch: 173 | Discriminator Real Loss: [0.12262222915887833, 0.96875] | Discriminator Fake Loss: [0.058528658002614975, 1.0] | GAN_Loss: 5.047085762023926\n",
      "Epoch: 174 | Discriminator Real Loss: [0.1308787614107132, 1.0] | Discriminator Fake Loss: [0.15056759119033813, 0.9375] | GAN_Loss: 5.857500076293945\n",
      "Epoch: 175 | Discriminator Real Loss: [0.26152491569519043, 0.9375] | Discriminator Fake Loss: [0.11455772072076797, 0.9375] | GAN_Loss: 5.251250743865967\n",
      "Epoch: 176 | Discriminator Real Loss: [0.2414250522851944, 0.9375] | Discriminator Fake Loss: [0.17413628101348877, 0.96875] | GAN_Loss: 4.393097877502441\n",
      "Epoch: 177 | Discriminator Real Loss: [0.0763789713382721, 1.0] | Discriminator Fake Loss: [0.09657716006040573, 0.96875] | GAN_Loss: 3.8800406455993652\n",
      "Epoch: 178 | Discriminator Real Loss: [0.08145416527986526, 0.96875] | Discriminator Fake Loss: [0.37890735268592834, 0.8125] | GAN_Loss: 4.533046245574951\n",
      "Epoch: 179 | Discriminator Real Loss: [0.11554169654846191, 0.96875] | Discriminator Fake Loss: [0.2520715594291687, 0.875] | GAN_Loss: 6.777414798736572\n",
      "Epoch: 180 | Discriminator Real Loss: [0.22507426142692566, 0.90625] | Discriminator Fake Loss: [0.25208330154418945, 0.875] | GAN_Loss: 5.889119625091553\n",
      "Epoch: 181 | Discriminator Real Loss: [0.35389649868011475, 0.8125] | Discriminator Fake Loss: [0.07357975095510483, 0.96875] | GAN_Loss: 5.063741683959961\n",
      "Epoch: 182 | Discriminator Real Loss: [0.20664845407009125, 0.90625] | Discriminator Fake Loss: [0.18533459305763245, 0.9375] | GAN_Loss: 4.555769920349121\n",
      "Epoch: 183 | Discriminator Real Loss: [0.04048897698521614, 1.0] | Discriminator Fake Loss: [0.5015639066696167, 0.75] | GAN_Loss: 4.013258457183838\n",
      "Epoch: 184 | Discriminator Real Loss: [0.10612249374389648, 1.0] | Discriminator Fake Loss: [0.30878186225891113, 0.875] | GAN_Loss: 5.541203498840332\n",
      "Epoch: 185 | Discriminator Real Loss: [0.372692346572876, 0.875] | Discriminator Fake Loss: [0.15019161999225616, 0.90625] | GAN_Loss: 6.650246620178223\n",
      "Epoch: 186 | Discriminator Real Loss: [0.431072473526001, 0.8125] | Discriminator Fake Loss: [0.16990457475185394, 0.96875] | GAN_Loss: 5.258278846740723\n",
      "Epoch: 187 | Discriminator Real Loss: [0.17537543177604675, 0.9375] | Discriminator Fake Loss: [0.29023876786231995, 0.84375] | GAN_Loss: 4.778594017028809\n",
      "Epoch: 188 | Discriminator Real Loss: [0.09786964952945709, 1.0] | Discriminator Fake Loss: [0.1766839623451233, 0.90625] | GAN_Loss: 4.172703742980957\n",
      "Epoch: 189 | Discriminator Real Loss: [0.10812725871801376, 1.0] | Discriminator Fake Loss: [0.12846720218658447, 0.9375] | GAN_Loss: 5.0441999435424805\n",
      "Epoch: 190 | Discriminator Real Loss: [0.2307112067937851, 0.90625] | Discriminator Fake Loss: [0.28585416078567505, 0.90625] | GAN_Loss: 5.45008659362793\n",
      "Epoch: 191 | Discriminator Real Loss: [0.19389957189559937, 0.9375] | Discriminator Fake Loss: [0.3137659728527069, 0.84375] | GAN_Loss: 4.391907691955566\n",
      "Epoch: 192 | Discriminator Real Loss: [0.09582719206809998, 1.0] | Discriminator Fake Loss: [0.18153780698776245, 0.90625] | GAN_Loss: 4.97182559967041\n",
      "Epoch: 193 | Discriminator Real Loss: [0.14788466691970825, 0.96875] | Discriminator Fake Loss: [0.10504984855651855, 0.96875] | GAN_Loss: 5.589136123657227\n",
      "Epoch: 194 | Discriminator Real Loss: [0.25750279426574707, 0.90625] | Discriminator Fake Loss: [0.18211111426353455, 0.9375] | GAN_Loss: 4.93438720703125\n",
      "Epoch: 195 | Discriminator Real Loss: [0.12706135213375092, 1.0] | Discriminator Fake Loss: [0.3106023967266083, 0.84375] | GAN_Loss: 5.593194484710693\n",
      "Epoch: 196 | Discriminator Real Loss: [0.20348906517028809, 0.96875] | Discriminator Fake Loss: [0.2943536043167114, 0.875] | GAN_Loss: 5.599287986755371\n",
      "Epoch: 197 | Discriminator Real Loss: [0.1353282332420349, 0.96875] | Discriminator Fake Loss: [0.049829453229904175, 1.0] | GAN_Loss: 6.9280314445495605\n",
      "Epoch: 198 | Discriminator Real Loss: [0.45521998405456543, 0.875] | Discriminator Fake Loss: [0.25799310207366943, 0.875] | GAN_Loss: 4.684169769287109\n",
      "Epoch: 199 | Discriminator Real Loss: [0.11846554279327393, 0.96875] | Discriminator Fake Loss: [0.36721891164779663, 0.8125] | GAN_Loss: 4.487318992614746\n",
      "Epoch: 200 | Discriminator Real Loss: [0.1044432520866394, 0.96875] | Discriminator Fake Loss: [0.44158756732940674, 0.8125] | GAN_Loss: 5.42727518081665\n",
      "Epoch: 201 | Discriminator Real Loss: [0.39054131507873535, 0.875] | Discriminator Fake Loss: [0.07851336151361465, 0.96875] | GAN_Loss: 5.1654462814331055\n",
      "Epoch: 202 | Discriminator Real Loss: [0.8382141590118408, 0.75] | Discriminator Fake Loss: [0.4279901087284088, 0.8125] | GAN_Loss: 3.684640407562256\n",
      "Epoch: 203 | Discriminator Real Loss: [0.15798422694206238, 0.9375] | Discriminator Fake Loss: [0.7536606788635254, 0.75] | GAN_Loss: 3.872955799102783\n",
      "Epoch: 204 | Discriminator Real Loss: [0.09230335801839828, 1.0] | Discriminator Fake Loss: [0.5165295600891113, 0.6875] | GAN_Loss: 5.4078803062438965\n",
      "Epoch: 205 | Discriminator Real Loss: [0.32132965326309204, 0.8125] | Discriminator Fake Loss: [0.11733722686767578, 0.96875] | GAN_Loss: 5.978759765625\n",
      "Epoch: 206 | Discriminator Real Loss: [0.7717452049255371, 0.65625] | Discriminator Fake Loss: [0.19227726757526398, 0.90625] | GAN_Loss: 4.250085830688477\n",
      "Epoch: 207 | Discriminator Real Loss: [0.15816405415534973, 1.0] | Discriminator Fake Loss: [0.4715438485145569, 0.78125] | GAN_Loss: 2.935673236846924\n",
      "Epoch: 208 | Discriminator Real Loss: [0.12557342648506165, 0.9375] | Discriminator Fake Loss: [0.5775837898254395, 0.65625] | GAN_Loss: 3.370199680328369\n",
      "Epoch: 209 | Discriminator Real Loss: [0.1619132161140442, 0.96875] | Discriminator Fake Loss: [0.17688339948654175, 0.9375] | GAN_Loss: 6.1832275390625\n",
      "Epoch: 210 | Discriminator Real Loss: [0.36285650730133057, 0.875] | Discriminator Fake Loss: [0.1326676458120346, 0.9375] | GAN_Loss: 5.0320024490356445\n",
      "Epoch: 211 | Discriminator Real Loss: [0.6608582735061646, 0.71875] | Discriminator Fake Loss: [0.0763254463672638, 0.96875] | GAN_Loss: 3.86694073677063\n",
      "Epoch: 212 | Discriminator Real Loss: [0.1544644832611084, 0.96875] | Discriminator Fake Loss: [0.4235990345478058, 0.78125] | GAN_Loss: 2.784510612487793\n",
      "Epoch: 213 | Discriminator Real Loss: [0.10469375550746918, 1.0] | Discriminator Fake Loss: [0.740010678768158, 0.71875] | GAN_Loss: 3.9472928047180176\n",
      "Epoch: 214 | Discriminator Real Loss: [0.11316686123609543, 1.0] | Discriminator Fake Loss: [0.12963518500328064, 0.9375] | GAN_Loss: 4.974567890167236\n",
      "Epoch: 215 | Discriminator Real Loss: [0.22149839997291565, 0.96875] | Discriminator Fake Loss: [0.1526264101266861, 0.9375] | GAN_Loss: 4.630461692810059\n",
      "Epoch: 216 | Discriminator Real Loss: [0.25921088457107544, 0.9375] | Discriminator Fake Loss: [0.18215937912464142, 0.96875] | GAN_Loss: 4.93810510635376\n",
      "Epoch: 217 | Discriminator Real Loss: [0.236703023314476, 0.96875] | Discriminator Fake Loss: [0.09751605987548828, 1.0] | GAN_Loss: 5.326014995574951\n",
      "Epoch: 218 | Discriminator Real Loss: [0.33218708634376526, 0.84375] | Discriminator Fake Loss: [0.34979838132858276, 0.84375] | GAN_Loss: 3.632307291030884\n",
      "Epoch: 219 | Discriminator Real Loss: [0.19477683305740356, 0.9375] | Discriminator Fake Loss: [0.1481555849313736, 0.9375] | GAN_Loss: 3.5411741733551025\n",
      "Epoch: 220 | Discriminator Real Loss: [0.14626969397068024, 0.9375] | Discriminator Fake Loss: [0.47488290071487427, 0.75] | GAN_Loss: 4.001398086547852\n",
      "Epoch: 221 | Discriminator Real Loss: [0.20980849862098694, 0.96875] | Discriminator Fake Loss: [0.17496593296527863, 0.90625] | GAN_Loss: 4.816379547119141\n",
      "Epoch: 222 | Discriminator Real Loss: [0.3137480914592743, 0.8125] | Discriminator Fake Loss: [0.1256330907344818, 0.9375] | GAN_Loss: 4.685705184936523\n",
      "Epoch: 223 | Discriminator Real Loss: [0.2099452018737793, 0.9375] | Discriminator Fake Loss: [0.12312860786914825, 0.96875] | GAN_Loss: 4.38734245300293\n",
      "Epoch: 224 | Discriminator Real Loss: [0.29118895530700684, 0.875] | Discriminator Fake Loss: [0.24907009303569794, 0.875] | GAN_Loss: 4.991679668426514\n",
      "Epoch: 225 | Discriminator Real Loss: [0.16333948075771332, 0.96875] | Discriminator Fake Loss: [0.6859152913093567, 0.59375] | GAN_Loss: 5.567281246185303\n",
      "Epoch: 226 | Discriminator Real Loss: [0.29983243346214294, 0.875] | Discriminator Fake Loss: [0.12638992071151733, 0.9375] | GAN_Loss: 7.394298553466797\n",
      "Epoch: 227 | Discriminator Real Loss: [0.5726909637451172, 0.78125] | Discriminator Fake Loss: [0.0778726115822792, 0.96875] | GAN_Loss: 3.63291597366333\n",
      "Epoch: 228 | Discriminator Real Loss: [0.2886938452720642, 0.875] | Discriminator Fake Loss: [0.9151462316513062, 0.53125] | GAN_Loss: 2.825155258178711\n",
      "Epoch: 229 | Discriminator Real Loss: [0.11481787264347076, 0.96875] | Discriminator Fake Loss: [0.5949479341506958, 0.65625] | GAN_Loss: 5.235419750213623\n",
      "Epoch: 230 | Discriminator Real Loss: [0.4518529176712036, 0.84375] | Discriminator Fake Loss: [0.1132013276219368, 0.9375] | GAN_Loss: 7.380970001220703\n",
      "Epoch: 231 | Discriminator Real Loss: [1.0738554000854492, 0.46875] | Discriminator Fake Loss: [0.3843165636062622, 0.84375] | GAN_Loss: 3.106755256652832\n",
      "Epoch: 232 | Discriminator Real Loss: [0.1494332253932953, 0.96875] | Discriminator Fake Loss: [0.8170093297958374, 0.625] | GAN_Loss: 2.1869232654571533\n",
      "Epoch: 233 | Discriminator Real Loss: [0.04953186586499214, 1.0] | Discriminator Fake Loss: [0.3477839529514313, 0.875] | GAN_Loss: 2.6333773136138916\n",
      "Epoch: 234 | Discriminator Real Loss: [0.08892959356307983, 1.0] | Discriminator Fake Loss: [0.44148361682891846, 0.84375] | GAN_Loss: 4.538592338562012\n",
      "Epoch: 235 | Discriminator Real Loss: [0.18177488446235657, 0.96875] | Discriminator Fake Loss: [0.17830996215343475, 0.875] | GAN_Loss: 4.671631336212158\n",
      "Epoch: 236 | Discriminator Real Loss: [0.4182543158531189, 0.8125] | Discriminator Fake Loss: [0.03695914149284363, 1.0] | GAN_Loss: 4.850917339324951\n",
      "Epoch: 237 | Discriminator Real Loss: [0.4636593163013458, 0.8125] | Discriminator Fake Loss: [0.2975088655948639, 0.84375] | GAN_Loss: 3.2858195304870605\n",
      "Epoch: 238 | Discriminator Real Loss: [0.17141704261302948, 0.96875] | Discriminator Fake Loss: [0.38476306200027466, 0.8125] | GAN_Loss: 2.88403058052063\n",
      "Epoch: 239 | Discriminator Real Loss: [0.12854276597499847, 0.96875] | Discriminator Fake Loss: [0.4577382206916809, 0.78125] | GAN_Loss: 3.9913296699523926\n",
      "Epoch: 240 | Discriminator Real Loss: [0.13527193665504456, 1.0] | Discriminator Fake Loss: [0.35654962062835693, 0.78125] | GAN_Loss: 5.1844682693481445\n",
      "Epoch: 241 | Discriminator Real Loss: [0.2990604639053345, 0.9375] | Discriminator Fake Loss: [0.11862805485725403, 0.96875] | GAN_Loss: 5.288834571838379\n",
      "Epoch: 242 | Discriminator Real Loss: [0.5696808099746704, 0.65625] | Discriminator Fake Loss: [0.29529574513435364, 0.8125] | GAN_Loss: 4.129214286804199\n",
      "Epoch: 243 | Discriminator Real Loss: [0.22686544060707092, 0.96875] | Discriminator Fake Loss: [0.32708215713500977, 0.84375] | GAN_Loss: 3.346898078918457\n",
      "Epoch: 244 | Discriminator Real Loss: [0.09019170701503754, 1.0] | Discriminator Fake Loss: [0.29303818941116333, 0.84375] | GAN_Loss: 3.666424512863159\n",
      "Epoch: 245 | Discriminator Real Loss: [0.11139944940805435, 1.0] | Discriminator Fake Loss: [0.5750324726104736, 0.6875] | GAN_Loss: 5.054732322692871\n",
      "Epoch: 246 | Discriminator Real Loss: [0.3070327043533325, 0.9375] | Discriminator Fake Loss: [0.1201346218585968, 0.96875] | GAN_Loss: 5.39317512512207\n",
      "Epoch: 247 | Discriminator Real Loss: [0.567490816116333, 0.71875] | Discriminator Fake Loss: [0.2228233963251114, 0.9375] | GAN_Loss: 3.8434700965881348\n",
      "Epoch: 248 | Discriminator Real Loss: [0.2955327033996582, 0.875] | Discriminator Fake Loss: [0.20313844084739685, 0.84375] | GAN_Loss: 3.1682541370391846\n",
      "Epoch: 249 | Discriminator Real Loss: [0.1869714856147766, 0.9375] | Discriminator Fake Loss: [0.6774634122848511, 0.65625] | GAN_Loss: 3.3413400650024414\n",
      "Epoch: 250 | Discriminator Real Loss: [0.1442224383354187, 0.9375] | Discriminator Fake Loss: [0.4559119939804077, 0.75] | GAN_Loss: 5.6583251953125\n",
      "Epoch: 251 | Discriminator Real Loss: [0.36804693937301636, 0.90625] | Discriminator Fake Loss: [0.14242540299892426, 0.9375] | GAN_Loss: 6.15327787399292\n",
      "Epoch: 252 | Discriminator Real Loss: [0.8197435736656189, 0.46875] | Discriminator Fake Loss: [0.22578340768814087, 0.875] | GAN_Loss: 4.336066246032715\n",
      "Epoch: 253 | Discriminator Real Loss: [0.30581068992614746, 0.875] | Discriminator Fake Loss: [0.5140954256057739, 0.625] | GAN_Loss: 3.1758861541748047\n",
      "Epoch: 254 | Discriminator Real Loss: [0.14466966688632965, 0.96875] | Discriminator Fake Loss: [0.4058370888233185, 0.8125] | GAN_Loss: 3.5964419841766357\n",
      "Epoch: 255 | Discriminator Real Loss: [0.21525001525878906, 0.96875] | Discriminator Fake Loss: [0.22100837528705597, 0.875] | GAN_Loss: 4.191890239715576\n",
      "Epoch: 256 | Discriminator Real Loss: [0.392819344997406, 0.84375] | Discriminator Fake Loss: [0.13862404227256775, 0.90625] | GAN_Loss: 4.611865997314453\n",
      "Epoch: 257 | Discriminator Real Loss: [0.6583870649337769, 0.71875] | Discriminator Fake Loss: [0.30838853120803833, 0.8125] | GAN_Loss: 3.439786195755005\n",
      "Epoch: 258 | Discriminator Real Loss: [0.2327660769224167, 0.9375] | Discriminator Fake Loss: [0.5062193274497986, 0.6875] | GAN_Loss: 3.1378986835479736\n",
      "Epoch: 259 | Discriminator Real Loss: [0.0818316861987114, 1.0] | Discriminator Fake Loss: [0.6761890649795532, 0.75] | GAN_Loss: 3.12784481048584\n",
      "Epoch: 260 | Discriminator Real Loss: [0.10146624594926834, 1.0] | Discriminator Fake Loss: [0.37989839911460876, 0.8125] | GAN_Loss: 4.363368511199951\n",
      "Epoch: 261 | Discriminator Real Loss: [0.388995885848999, 0.84375] | Discriminator Fake Loss: [0.12577888369560242, 0.90625] | GAN_Loss: 4.743704795837402\n",
      "Epoch: 262 | Discriminator Real Loss: [0.8416411876678467, 0.53125] | Discriminator Fake Loss: [0.2826089560985565, 0.90625] | GAN_Loss: 2.9108009338378906\n",
      "Epoch: 263 | Discriminator Real Loss: [0.21978759765625, 0.9375] | Discriminator Fake Loss: [0.6977934241294861, 0.59375] | GAN_Loss: 2.600069046020508\n",
      "Epoch: 264 | Discriminator Real Loss: [0.23291215300559998, 0.9375] | Discriminator Fake Loss: [0.2684391736984253, 0.875] | GAN_Loss: 3.9429283142089844\n",
      "Epoch: 265 | Discriminator Real Loss: [0.2857840657234192, 0.90625] | Discriminator Fake Loss: [0.35077103972435, 0.8125] | GAN_Loss: 3.8434059619903564\n",
      "Epoch: 266 | Discriminator Real Loss: [0.44564616680145264, 0.84375] | Discriminator Fake Loss: [0.16271725296974182, 0.90625] | GAN_Loss: 4.691274166107178\n",
      "Epoch: 267 | Discriminator Real Loss: [0.34917694330215454, 0.84375] | Discriminator Fake Loss: [0.32683083415031433, 0.84375] | GAN_Loss: 3.915041923522949\n",
      "Epoch: 268 | Discriminator Real Loss: [0.3906276226043701, 0.875] | Discriminator Fake Loss: [0.3194809556007385, 0.84375] | GAN_Loss: 3.5124011039733887\n",
      "Epoch: 269 | Discriminator Real Loss: [0.16507840156555176, 1.0] | Discriminator Fake Loss: [0.3585200905799866, 0.8125] | GAN_Loss: 3.756596088409424\n",
      "Epoch: 270 | Discriminator Real Loss: [0.2326873242855072, 0.90625] | Discriminator Fake Loss: [0.2581593990325928, 0.90625] | GAN_Loss: 4.523555755615234\n",
      "Epoch: 271 | Discriminator Real Loss: [0.4539954960346222, 0.8125] | Discriminator Fake Loss: [0.1338191032409668, 1.0] | GAN_Loss: 3.358736991882324\n",
      "Epoch: 272 | Discriminator Real Loss: [0.2400643527507782, 0.96875] | Discriminator Fake Loss: [0.688964307308197, 0.65625] | GAN_Loss: 3.4405369758605957\n",
      "Epoch: 273 | Discriminator Real Loss: [0.11197329312562943, 1.0] | Discriminator Fake Loss: [0.3921130299568176, 0.78125] | GAN_Loss: 4.938677787780762\n",
      "Epoch: 274 | Discriminator Real Loss: [0.3664690852165222, 0.84375] | Discriminator Fake Loss: [0.267001748085022, 0.90625] | GAN_Loss: 4.864927291870117\n",
      "Epoch: 275 | Discriminator Real Loss: [0.6678156852722168, 0.65625] | Discriminator Fake Loss: [0.3528960943222046, 0.84375] | GAN_Loss: 4.179941177368164\n",
      "Epoch: 276 | Discriminator Real Loss: [0.27940675616264343, 0.90625] | Discriminator Fake Loss: [0.6016312837600708, 0.71875] | GAN_Loss: 2.6579549312591553\n",
      "Epoch: 277 | Discriminator Real Loss: [0.16187390685081482, 0.9375] | Discriminator Fake Loss: [0.32513731718063354, 0.875] | GAN_Loss: 3.9241585731506348\n",
      "Epoch: 278 | Discriminator Real Loss: [0.18324023485183716, 1.0] | Discriminator Fake Loss: [0.30848202109336853, 0.90625] | GAN_Loss: 4.323352813720703\n",
      "Epoch: 279 | Discriminator Real Loss: [0.3424365818500519, 0.875] | Discriminator Fake Loss: [0.30879777669906616, 0.875] | GAN_Loss: 4.729344844818115\n",
      "Epoch: 280 | Discriminator Real Loss: [0.4355138838291168, 0.8125] | Discriminator Fake Loss: [0.25195780396461487, 0.875] | GAN_Loss: 3.6835522651672363\n",
      "Epoch: 281 | Discriminator Real Loss: [0.3203110098838806, 0.9375] | Discriminator Fake Loss: [0.42974722385406494, 0.78125] | GAN_Loss: 3.3958609104156494\n",
      "Epoch: 282 | Discriminator Real Loss: [0.2539139986038208, 0.90625] | Discriminator Fake Loss: [0.17137983441352844, 0.90625] | GAN_Loss: 3.826594114303589\n",
      "Epoch: 283 | Discriminator Real Loss: [0.23612751066684723, 0.9375] | Discriminator Fake Loss: [0.2980012595653534, 0.78125] | GAN_Loss: 4.136969089508057\n",
      "Epoch: 284 | Discriminator Real Loss: [0.3387451767921448, 0.90625] | Discriminator Fake Loss: [0.17784835398197174, 0.90625] | GAN_Loss: 4.350997447967529\n",
      "Epoch: 285 | Discriminator Real Loss: [0.40828776359558105, 0.84375] | Discriminator Fake Loss: [0.29793262481689453, 0.84375] | GAN_Loss: 3.5963125228881836\n",
      "Epoch: 286 | Discriminator Real Loss: [0.3645349442958832, 0.875] | Discriminator Fake Loss: [0.45218968391418457, 0.78125] | GAN_Loss: 2.852780342102051\n",
      "Epoch: 287 | Discriminator Real Loss: [0.19943158328533173, 0.9375] | Discriminator Fake Loss: [0.37109676003456116, 0.78125] | GAN_Loss: 3.749563694000244\n",
      "Epoch: 288 | Discriminator Real Loss: [0.3302004337310791, 0.84375] | Discriminator Fake Loss: [0.16930609941482544, 0.9375] | GAN_Loss: 4.1981201171875\n",
      "Epoch: 289 | Discriminator Real Loss: [0.43597596883773804, 0.84375] | Discriminator Fake Loss: [0.15507444739341736, 0.96875] | GAN_Loss: 3.5719988346099854\n",
      "Epoch: 290 | Discriminator Real Loss: [0.333619624376297, 0.90625] | Discriminator Fake Loss: [0.31372690200805664, 0.84375] | GAN_Loss: 3.2923336029052734\n",
      "Epoch: 291 | Discriminator Real Loss: [0.25070714950561523, 0.90625] | Discriminator Fake Loss: [0.6139097213745117, 0.65625] | GAN_Loss: 3.646287441253662\n",
      "Epoch: 292 | Discriminator Real Loss: [0.21441765129566193, 0.9375] | Discriminator Fake Loss: [0.3742632567882538, 0.875] | GAN_Loss: 4.081789970397949\n",
      "Epoch: 293 | Discriminator Real Loss: [0.538241982460022, 0.75] | Discriminator Fake Loss: [0.16375577449798584, 0.9375] | GAN_Loss: 3.8637099266052246\n",
      "Epoch: 294 | Discriminator Real Loss: [0.4911893606185913, 0.78125] | Discriminator Fake Loss: [0.746644139289856, 0.625] | GAN_Loss: 3.3541488647460938\n",
      "Epoch: 295 | Discriminator Real Loss: [0.4524834156036377, 0.9375] | Discriminator Fake Loss: [0.44401782751083374, 0.75] | GAN_Loss: 3.7745511531829834\n",
      "Epoch: 296 | Discriminator Real Loss: [0.27206873893737793, 0.96875] | Discriminator Fake Loss: [0.37223565578460693, 0.78125] | GAN_Loss: 3.234663724899292\n",
      "Epoch: 297 | Discriminator Real Loss: [0.4300570487976074, 0.78125] | Discriminator Fake Loss: [0.322049081325531, 0.8125] | GAN_Loss: 3.858217716217041\n",
      "Epoch: 298 | Discriminator Real Loss: [0.4247121512889862, 0.8125] | Discriminator Fake Loss: [0.3464159071445465, 0.78125] | GAN_Loss: 3.6319804191589355\n",
      "Epoch: 299 | Discriminator Real Loss: [0.3537184000015259, 0.875] | Discriminator Fake Loss: [0.31403839588165283, 0.8125] | GAN_Loss: 3.828735113143921\n",
      "Epoch: 300 | Discriminator Real Loss: [0.4917912483215332, 0.75] | Discriminator Fake Loss: [0.45166051387786865, 0.75] | GAN_Loss: 3.6686267852783203\n",
      "Epoch: 301 | Discriminator Real Loss: [0.2162473499774933, 0.9375] | Discriminator Fake Loss: [0.1415863335132599, 0.9375] | GAN_Loss: 3.6255664825439453\n",
      "Epoch: 302 | Discriminator Real Loss: [0.4154742360115051, 0.75] | Discriminator Fake Loss: [0.17613279819488525, 0.9375] | GAN_Loss: 2.8739359378814697\n",
      "Epoch: 303 | Discriminator Real Loss: [0.2352602779865265, 0.9375] | Discriminator Fake Loss: [0.4183248281478882, 0.78125] | GAN_Loss: 2.3104519844055176\n",
      "Epoch: 304 | Discriminator Real Loss: [0.08108213543891907, 1.0] | Discriminator Fake Loss: [0.5539882183074951, 0.71875] | GAN_Loss: 4.478196620941162\n",
      "Epoch: 305 | Discriminator Real Loss: [0.32193320989608765, 0.90625] | Discriminator Fake Loss: [0.2074700891971588, 0.875] | GAN_Loss: 4.491421699523926\n",
      "Epoch: 306 | Discriminator Real Loss: [0.38092440366744995, 0.90625] | Discriminator Fake Loss: [0.09385396540164948, 0.96875] | GAN_Loss: 4.907055854797363\n",
      "Epoch: 307 | Discriminator Real Loss: [0.37645137310028076, 0.84375] | Discriminator Fake Loss: [0.4669340252876282, 0.75] | GAN_Loss: 4.4315290451049805\n",
      "Epoch: 308 | Discriminator Real Loss: [0.18170838057994843, 0.96875] | Discriminator Fake Loss: [0.21662676334381104, 0.875] | GAN_Loss: 2.7693676948547363\n",
      "Epoch: 309 | Discriminator Real Loss: [0.20999053120613098, 0.90625] | Discriminator Fake Loss: [0.5373258590698242, 0.71875] | GAN_Loss: 2.385131359100342\n",
      "Epoch: 310 | Discriminator Real Loss: [0.11073358356952667, 1.0] | Discriminator Fake Loss: [0.5053610801696777, 0.71875] | GAN_Loss: 4.707913398742676\n",
      "Epoch: 311 | Discriminator Real Loss: [0.32973235845565796, 0.875] | Discriminator Fake Loss: [0.24815890192985535, 0.84375] | GAN_Loss: 4.184736251831055\n",
      "Epoch: 312 | Discriminator Real Loss: [0.6938692331314087, 0.75] | Discriminator Fake Loss: [0.3173677623271942, 0.84375] | GAN_Loss: 4.001007556915283\n",
      "Epoch: 313 | Discriminator Real Loss: [0.3681368827819824, 0.8125] | Discriminator Fake Loss: [0.3926345407962799, 0.75] | GAN_Loss: 2.4532313346862793\n",
      "Epoch: 314 | Discriminator Real Loss: [0.12228702753782272, 0.96875] | Discriminator Fake Loss: [0.513032853603363, 0.75] | GAN_Loss: 4.155398368835449\n",
      "Epoch: 315 | Discriminator Real Loss: [0.2980685234069824, 0.90625] | Discriminator Fake Loss: [0.4044511020183563, 0.75] | GAN_Loss: 4.407468795776367\n",
      "Epoch: 316 | Discriminator Real Loss: [0.3848017156124115, 0.8125] | Discriminator Fake Loss: [0.11882108449935913, 0.9375] | GAN_Loss: 4.967824459075928\n",
      "Epoch: 317 | Discriminator Real Loss: [0.33020493388175964, 0.90625] | Discriminator Fake Loss: [0.36146605014801025, 0.8125] | GAN_Loss: 4.797922134399414\n",
      "Epoch: 318 | Discriminator Real Loss: [0.29126960039138794, 0.90625] | Discriminator Fake Loss: [0.17456161975860596, 0.96875] | GAN_Loss: 2.8926401138305664\n",
      "Epoch: 319 | Discriminator Real Loss: [0.15457911789417267, 1.0] | Discriminator Fake Loss: [0.3580976128578186, 0.8125] | GAN_Loss: 4.165547847747803\n",
      "Epoch: 320 | Discriminator Real Loss: [0.15559622645378113, 0.96875] | Discriminator Fake Loss: [0.21122221648693085, 0.9375] | GAN_Loss: 4.45457649230957\n",
      "Epoch: 321 | Discriminator Real Loss: [0.3284580111503601, 0.90625] | Discriminator Fake Loss: [0.24675658345222473, 0.875] | GAN_Loss: 4.991225242614746\n",
      "Epoch: 322 | Discriminator Real Loss: [0.23842063546180725, 0.96875] | Discriminator Fake Loss: [0.1707289069890976, 0.9375] | GAN_Loss: 4.1285576820373535\n",
      "Epoch: 323 | Discriminator Real Loss: [0.37296444177627563, 0.90625] | Discriminator Fake Loss: [0.25706449151039124, 0.90625] | GAN_Loss: 2.7251815795898438\n",
      "Epoch: 324 | Discriminator Real Loss: [0.1570630669593811, 1.0] | Discriminator Fake Loss: [0.4571297764778137, 0.78125] | GAN_Loss: 3.2669594287872314\n",
      "Epoch: 325 | Discriminator Real Loss: [0.15229368209838867, 1.0] | Discriminator Fake Loss: [0.45794883370399475, 0.71875] | GAN_Loss: 4.515089988708496\n",
      "Epoch: 326 | Discriminator Real Loss: [0.7116218209266663, 0.65625] | Discriminator Fake Loss: [0.40399596095085144, 0.78125] | GAN_Loss: 4.631280899047852\n",
      "Epoch: 327 | Discriminator Real Loss: [0.43058961629867554, 0.78125] | Discriminator Fake Loss: [0.5542662739753723, 0.65625] | GAN_Loss: 4.27911901473999\n",
      "Epoch: 328 | Discriminator Real Loss: [0.2648312747478485, 0.90625] | Discriminator Fake Loss: [0.22995242476463318, 0.90625] | GAN_Loss: 4.166201591491699\n",
      "Epoch: 329 | Discriminator Real Loss: [0.17769953608512878, 0.96875] | Discriminator Fake Loss: [0.15060225129127502, 0.90625] | GAN_Loss: 5.879666328430176\n",
      "Epoch: 330 | Discriminator Real Loss: [0.3747901916503906, 0.90625] | Discriminator Fake Loss: [0.24378758668899536, 0.875] | GAN_Loss: 5.424355506896973\n",
      "Epoch: 331 | Discriminator Real Loss: [0.14597323536872864, 1.0] | Discriminator Fake Loss: [0.2554923892021179, 0.875] | GAN_Loss: 6.013238906860352\n",
      "Epoch: 332 | Discriminator Real Loss: [0.2772410809993744, 0.90625] | Discriminator Fake Loss: [0.2876074016094208, 0.84375] | GAN_Loss: 5.963616847991943\n",
      "Epoch: 333 | Discriminator Real Loss: [0.281172513961792, 0.90625] | Discriminator Fake Loss: [0.4071209728717804, 0.78125] | GAN_Loss: 5.247560977935791\n",
      "Epoch: 334 | Discriminator Real Loss: [0.28679853677749634, 0.875] | Discriminator Fake Loss: [0.3199283480644226, 0.84375] | GAN_Loss: 4.309389114379883\n",
      "Epoch: 335 | Discriminator Real Loss: [0.18919163942337036, 0.96875] | Discriminator Fake Loss: [0.23637381196022034, 0.90625] | GAN_Loss: 5.254485130310059\n",
      "Epoch: 336 | Discriminator Real Loss: [0.11605237424373627, 1.0] | Discriminator Fake Loss: [0.20454496145248413, 0.875] | GAN_Loss: 4.858456611633301\n",
      "Epoch: 337 | Discriminator Real Loss: [0.25807809829711914, 0.9375] | Discriminator Fake Loss: [0.29865747690200806, 0.84375] | GAN_Loss: 6.91280460357666\n",
      "Epoch: 338 | Discriminator Real Loss: [0.6467322707176208, 0.75] | Discriminator Fake Loss: [0.33598390221595764, 0.8125] | GAN_Loss: 4.082927227020264\n",
      "Epoch: 339 | Discriminator Real Loss: [0.1098816767334938, 1.0] | Discriminator Fake Loss: [1.0681042671203613, 0.53125] | GAN_Loss: 3.865177869796753\n",
      "Epoch: 340 | Discriminator Real Loss: [0.3031693398952484, 0.875] | Discriminator Fake Loss: [0.2808643579483032, 0.8125] | GAN_Loss: 6.144902229309082\n",
      "Epoch: 341 | Discriminator Real Loss: [0.7710448503494263, 0.6875] | Discriminator Fake Loss: [0.3296278417110443, 0.78125] | GAN_Loss: 4.1840901374816895\n",
      "Epoch: 342 | Discriminator Real Loss: [0.21534527838230133, 0.96875] | Discriminator Fake Loss: [0.8996708393096924, 0.5625] | GAN_Loss: 4.29490327835083\n",
      "Epoch: 343 | Discriminator Real Loss: [0.34096020460128784, 0.875] | Discriminator Fake Loss: [0.834447979927063, 0.625] | GAN_Loss: 4.570809841156006\n",
      "Epoch: 344 | Discriminator Real Loss: [1.1252191066741943, 0.46875] | Discriminator Fake Loss: [0.4145955443382263, 0.71875] | GAN_Loss: 4.292669296264648\n",
      "Epoch: 345 | Discriminator Real Loss: [0.3212395906448364, 0.90625] | Discriminator Fake Loss: [0.6906917691230774, 0.6875] | GAN_Loss: 3.0572562217712402\n",
      "Epoch: 346 | Discriminator Real Loss: [0.37799185514450073, 0.84375] | Discriminator Fake Loss: [0.5863315463066101, 0.6875] | GAN_Loss: 3.5725769996643066\n",
      "Epoch: 347 | Discriminator Real Loss: [0.5849716663360596, 0.65625] | Discriminator Fake Loss: [0.30715876817703247, 0.84375] | GAN_Loss: 3.9128003120422363\n",
      "Epoch: 348 | Discriminator Real Loss: [0.39725184440612793, 0.78125] | Discriminator Fake Loss: [0.26123046875, 0.84375] | GAN_Loss: 3.121016263961792\n",
      "Epoch: 349 | Discriminator Real Loss: [0.4569444954395294, 0.84375] | Discriminator Fake Loss: [0.3578331172466278, 0.78125] | GAN_Loss: 2.7714226245880127\n",
      "Epoch: 350 | Discriminator Real Loss: [0.3395971655845642, 0.875] | Discriminator Fake Loss: [0.4538014829158783, 0.71875] | GAN_Loss: 3.125657081604004\n",
      "Epoch: 351 | Discriminator Real Loss: [0.21530860662460327, 0.96875] | Discriminator Fake Loss: [0.6072695255279541, 0.625] | GAN_Loss: 3.5532755851745605\n",
      "Epoch: 352 | Discriminator Real Loss: [0.270694375038147, 0.9375] | Discriminator Fake Loss: [0.27081868052482605, 0.875] | GAN_Loss: 4.162230491638184\n",
      "Epoch: 353 | Discriminator Real Loss: [0.6293073296546936, 0.625] | Discriminator Fake Loss: [0.16909100115299225, 0.96875] | GAN_Loss: 4.267002582550049\n",
      "Epoch: 354 | Discriminator Real Loss: [0.5054903626441956, 0.75] | Discriminator Fake Loss: [0.48223283886909485, 0.8125] | GAN_Loss: 2.510369300842285\n",
      "Epoch: 355 | Discriminator Real Loss: [0.2526569366455078, 0.96875] | Discriminator Fake Loss: [0.38411468267440796, 0.75] | GAN_Loss: 2.7566535472869873\n",
      "Epoch: 356 | Discriminator Real Loss: [0.12869083881378174, 1.0] | Discriminator Fake Loss: [0.7045952081680298, 0.59375] | GAN_Loss: 3.1212658882141113\n",
      "Epoch: 357 | Discriminator Real Loss: [0.31967902183532715, 0.875] | Discriminator Fake Loss: [0.2594146430492401, 0.84375] | GAN_Loss: 4.3638505935668945\n",
      "Epoch: 358 | Discriminator Real Loss: [0.4806055426597595, 0.84375] | Discriminator Fake Loss: [0.2770059108734131, 0.875] | GAN_Loss: 5.572299003601074\n",
      "Epoch: 359 | Discriminator Real Loss: [0.444946825504303, 0.84375] | Discriminator Fake Loss: [0.11620494723320007, 0.9375] | GAN_Loss: 3.2494165897369385\n",
      "Epoch: 360 | Discriminator Real Loss: [0.21479907631874084, 1.0] | Discriminator Fake Loss: [0.4648738205432892, 0.71875] | GAN_Loss: 2.266960620880127\n",
      "Epoch: 361 | Discriminator Real Loss: [0.2576020359992981, 0.9375] | Discriminator Fake Loss: [0.46617257595062256, 0.78125] | GAN_Loss: 3.8386588096618652\n",
      "Epoch: 362 | Discriminator Real Loss: [0.3194434344768524, 0.9375] | Discriminator Fake Loss: [0.32006898522377014, 0.78125] | GAN_Loss: 3.549213409423828\n",
      "Epoch: 363 | Discriminator Real Loss: [0.475555419921875, 0.78125] | Discriminator Fake Loss: [0.2569161057472229, 0.84375] | GAN_Loss: 3.5671329498291016\n",
      "Epoch: 364 | Discriminator Real Loss: [0.39468249678611755, 0.875] | Discriminator Fake Loss: [0.48791393637657166, 0.71875] | GAN_Loss: 3.4997856616973877\n",
      "Epoch: 365 | Discriminator Real Loss: [0.32900571823120117, 0.875] | Discriminator Fake Loss: [0.27426162362098694, 0.84375] | GAN_Loss: 3.0959653854370117\n",
      "Epoch: 366 | Discriminator Real Loss: [0.38909927010536194, 0.8125] | Discriminator Fake Loss: [0.3932098150253296, 0.84375] | GAN_Loss: 2.910250186920166\n",
      "Epoch: 367 | Discriminator Real Loss: [0.2147592008113861, 0.90625] | Discriminator Fake Loss: [0.482050359249115, 0.71875] | GAN_Loss: 3.1953749656677246\n",
      "Epoch: 368 | Discriminator Real Loss: [0.3641543388366699, 0.8125] | Discriminator Fake Loss: [0.2074269950389862, 0.875] | GAN_Loss: 3.4439079761505127\n",
      "Epoch: 369 | Discriminator Real Loss: [0.3337257206439972, 0.90625] | Discriminator Fake Loss: [0.5651999711990356, 0.65625] | GAN_Loss: 2.7090394496917725\n",
      "Epoch: 370 | Discriminator Real Loss: [0.2677614688873291, 0.90625] | Discriminator Fake Loss: [0.409249484539032, 0.78125] | GAN_Loss: 3.6076323986053467\n",
      "Epoch: 371 | Discriminator Real Loss: [0.39096200466156006, 0.84375] | Discriminator Fake Loss: [0.2050032913684845, 0.90625] | GAN_Loss: 3.5821034908294678\n",
      "Epoch: 372 | Discriminator Real Loss: [0.32286107540130615, 0.90625] | Discriminator Fake Loss: [0.39465928077697754, 0.75] | GAN_Loss: 4.785948753356934\n",
      "Epoch: 373 | Discriminator Real Loss: [0.3787280023097992, 0.9375] | Discriminator Fake Loss: [0.40745678544044495, 0.8125] | GAN_Loss: 3.5966053009033203\n",
      "Epoch: 374 | Discriminator Real Loss: [0.413491815328598, 0.875] | Discriminator Fake Loss: [0.2785305678844452, 0.875] | GAN_Loss: 3.269171714782715\n",
      "Epoch: 375 | Discriminator Real Loss: [0.42829930782318115, 0.8125] | Discriminator Fake Loss: [0.15360140800476074, 0.96875] | GAN_Loss: 2.3071703910827637\n",
      "Epoch: 376 | Discriminator Real Loss: [0.4061164855957031, 0.84375] | Discriminator Fake Loss: [0.9940170645713806, 0.53125] | GAN_Loss: 2.4258317947387695\n",
      "Epoch: 377 | Discriminator Real Loss: [0.20075905323028564, 0.96875] | Discriminator Fake Loss: [0.4804622530937195, 0.65625] | GAN_Loss: 3.9511096477508545\n",
      "Epoch: 378 | Discriminator Real Loss: [0.5178422927856445, 0.84375] | Discriminator Fake Loss: [0.1678442507982254, 0.9375] | GAN_Loss: 3.7588095664978027\n",
      "Epoch: 379 | Discriminator Real Loss: [0.357698917388916, 0.875] | Discriminator Fake Loss: [0.33937740325927734, 0.8125] | GAN_Loss: 3.4388771057128906\n",
      "Epoch: 380 | Discriminator Real Loss: [0.39593005180358887, 0.8125] | Discriminator Fake Loss: [0.30147311091423035, 0.875] | GAN_Loss: 2.160518169403076\n",
      "Epoch: 381 | Discriminator Real Loss: [0.3155292272567749, 0.84375] | Discriminator Fake Loss: [0.669827401638031, 0.625] | GAN_Loss: 2.2452268600463867\n",
      "Epoch: 382 | Discriminator Real Loss: [0.2175397127866745, 0.96875] | Discriminator Fake Loss: [0.16577419638633728, 0.9375] | GAN_Loss: 4.847514629364014\n",
      "Epoch: 383 | Discriminator Real Loss: [0.580804705619812, 0.71875] | Discriminator Fake Loss: [0.34214264154434204, 0.8125] | GAN_Loss: 3.087775230407715\n",
      "Epoch: 384 | Discriminator Real Loss: [0.37968525290489197, 0.84375] | Discriminator Fake Loss: [0.5711938142776489, 0.6875] | GAN_Loss: 2.3822388648986816\n",
      "Epoch: 385 | Discriminator Real Loss: [0.3155887722969055, 0.9375] | Discriminator Fake Loss: [0.5944586992263794, 0.6875] | GAN_Loss: 3.551100730895996\n",
      "Epoch: 386 | Discriminator Real Loss: [0.6083683967590332, 0.71875] | Discriminator Fake Loss: [0.46909868717193604, 0.78125] | GAN_Loss: 2.416790008544922\n",
      "Epoch: 387 | Discriminator Real Loss: [0.3810943365097046, 0.84375] | Discriminator Fake Loss: [0.5390849113464355, 0.75] | GAN_Loss: 2.3943934440612793\n",
      "Epoch: 388 | Discriminator Real Loss: [0.396979421377182, 0.84375] | Discriminator Fake Loss: [0.4211925268173218, 0.75] | GAN_Loss: 2.9653968811035156\n",
      "Epoch: 389 | Discriminator Real Loss: [0.3711799383163452, 0.84375] | Discriminator Fake Loss: [0.3068835735321045, 0.84375] | GAN_Loss: 3.124840259552002\n",
      "Epoch: 390 | Discriminator Real Loss: [0.5564457178115845, 0.78125] | Discriminator Fake Loss: [0.5183566808700562, 0.65625] | GAN_Loss: 3.194176197052002\n",
      "Epoch: 391 | Discriminator Real Loss: [0.3794308602809906, 0.8125] | Discriminator Fake Loss: [0.3502529561519623, 0.84375] | GAN_Loss: 2.259474039077759\n",
      "Epoch: 392 | Discriminator Real Loss: [0.34284359216690063, 0.875] | Discriminator Fake Loss: [0.5961881279945374, 0.6875] | GAN_Loss: 2.5431857109069824\n",
      "Epoch: 393 | Discriminator Real Loss: [0.2386579066514969, 0.96875] | Discriminator Fake Loss: [0.6431282758712769, 0.65625] | GAN_Loss: 3.1013715267181396\n",
      "Epoch: 394 | Discriminator Real Loss: [0.42763522267341614, 0.75] | Discriminator Fake Loss: [0.4462657570838928, 0.75] | GAN_Loss: 3.4535202980041504\n",
      "Epoch: 395 | Discriminator Real Loss: [0.6790405511856079, 0.71875] | Discriminator Fake Loss: [0.5775246024131775, 0.6875] | GAN_Loss: 2.4669528007507324\n",
      "Epoch: 396 | Discriminator Real Loss: [0.3655776083469391, 0.8125] | Discriminator Fake Loss: [0.6787421703338623, 0.59375] | GAN_Loss: 3.2033896446228027\n",
      "Epoch: 397 | Discriminator Real Loss: [0.4845585823059082, 0.8125] | Discriminator Fake Loss: [0.25089552998542786, 0.90625] | GAN_Loss: 2.29369854927063\n",
      "Epoch: 398 | Discriminator Real Loss: [0.40852123498916626, 0.8125] | Discriminator Fake Loss: [0.4608769714832306, 0.75] | GAN_Loss: 2.546889305114746\n",
      "Epoch: 399 | Discriminator Real Loss: [0.2779540717601776, 0.9375] | Discriminator Fake Loss: [0.39965394139289856, 0.8125] | GAN_Loss: 2.482311248779297\n",
      "Epoch: 400 | Discriminator Real Loss: [0.5476108193397522, 0.75] | Discriminator Fake Loss: [0.27421438694000244, 0.875] | GAN_Loss: 3.7064433097839355\n",
      "Epoch: 401 | Discriminator Real Loss: [0.3492375612258911, 0.90625] | Discriminator Fake Loss: [0.4347915053367615, 0.71875] | GAN_Loss: 2.6182243824005127\n",
      "Epoch: 402 | Discriminator Real Loss: [0.3945283591747284, 0.84375] | Discriminator Fake Loss: [0.45976707339286804, 0.625] | GAN_Loss: 2.9231185913085938\n",
      "Epoch: 403 | Discriminator Real Loss: [0.38354361057281494, 0.90625] | Discriminator Fake Loss: [0.5568199157714844, 0.625] | GAN_Loss: 2.2989208698272705\n",
      "Epoch: 404 | Discriminator Real Loss: [0.42410728335380554, 0.84375] | Discriminator Fake Loss: [0.39027008414268494, 0.71875] | GAN_Loss: 3.04506516456604\n",
      "Epoch: 405 | Discriminator Real Loss: [0.3297633230686188, 0.84375] | Discriminator Fake Loss: [0.4156522750854492, 0.78125] | GAN_Loss: 2.4287638664245605\n",
      "Epoch: 406 | Discriminator Real Loss: [0.31464266777038574, 0.90625] | Discriminator Fake Loss: [0.4439035654067993, 0.75] | GAN_Loss: 2.3054866790771484\n",
      "Epoch: 407 | Discriminator Real Loss: [0.39553844928741455, 0.8125] | Discriminator Fake Loss: [0.3633034825325012, 0.875] | GAN_Loss: 2.1969199180603027\n",
      "Epoch: 408 | Discriminator Real Loss: [0.27133285999298096, 0.9375] | Discriminator Fake Loss: [0.36819955706596375, 0.78125] | GAN_Loss: 2.8372879028320312\n",
      "Epoch: 409 | Discriminator Real Loss: [0.3732876777648926, 0.90625] | Discriminator Fake Loss: [0.36055558919906616, 0.8125] | GAN_Loss: 3.8643198013305664\n",
      "Epoch: 410 | Discriminator Real Loss: [0.3656788468360901, 0.875] | Discriminator Fake Loss: [0.366800457239151, 0.8125] | GAN_Loss: 3.024643898010254\n",
      "Epoch: 411 | Discriminator Real Loss: [0.5007312297821045, 0.84375] | Discriminator Fake Loss: [0.33048924803733826, 0.8125] | GAN_Loss: 2.262140989303589\n",
      "Epoch: 412 | Discriminator Real Loss: [0.42067068815231323, 0.875] | Discriminator Fake Loss: [0.6322665214538574, 0.5625] | GAN_Loss: 2.412024736404419\n",
      "Epoch: 413 | Discriminator Real Loss: [0.22805973887443542, 0.9375] | Discriminator Fake Loss: [0.43880611658096313, 0.75] | GAN_Loss: 2.6898279190063477\n",
      "Epoch: 414 | Discriminator Real Loss: [0.4060376286506653, 0.78125] | Discriminator Fake Loss: [0.2939068078994751, 0.84375] | GAN_Loss: 3.8887457847595215\n",
      "Epoch: 415 | Discriminator Real Loss: [0.7352035641670227, 0.6875] | Discriminator Fake Loss: [0.272552490234375, 0.875] | GAN_Loss: 2.5851798057556152\n",
      "Epoch: 416 | Discriminator Real Loss: [0.3390781879425049, 0.90625] | Discriminator Fake Loss: [0.7838793992996216, 0.625] | GAN_Loss: 2.05088472366333\n",
      "Epoch: 417 | Discriminator Real Loss: [0.23217108845710754, 0.875] | Discriminator Fake Loss: [0.771611213684082, 0.53125] | GAN_Loss: 3.6501009464263916\n",
      "Epoch: 418 | Discriminator Real Loss: [0.6346303820610046, 0.71875] | Discriminator Fake Loss: [0.234124094247818, 0.9375] | GAN_Loss: 4.21265983581543\n",
      "Epoch: 419 | Discriminator Real Loss: [1.307582974433899, 0.34375] | Discriminator Fake Loss: [0.3179985284805298, 0.84375] | GAN_Loss: 1.9151489734649658\n",
      "Epoch: 420 | Discriminator Real Loss: [0.3189449608325958, 0.875] | Discriminator Fake Loss: [0.999840259552002, 0.40625] | GAN_Loss: 1.8886785507202148\n",
      "Epoch: 421 | Discriminator Real Loss: [0.25934499502182007, 0.9375] | Discriminator Fake Loss: [0.8762223720550537, 0.40625] | GAN_Loss: 1.8704062700271606\n",
      "Epoch: 422 | Discriminator Real Loss: [0.39711445569992065, 0.84375] | Discriminator Fake Loss: [0.40965285897254944, 0.78125] | GAN_Loss: 2.756761312484741\n",
      "Epoch: 423 | Discriminator Real Loss: [0.8939502835273743, 0.46875] | Discriminator Fake Loss: [0.3464559018611908, 0.84375] | GAN_Loss: 2.2164745330810547\n",
      "Epoch: 424 | Discriminator Real Loss: [0.5360649228096008, 0.78125] | Discriminator Fake Loss: [0.6658832430839539, 0.59375] | GAN_Loss: 1.3793830871582031\n",
      "Epoch: 425 | Discriminator Real Loss: [0.4041641354560852, 0.84375] | Discriminator Fake Loss: [0.5876062512397766, 0.65625] | GAN_Loss: 1.5585302114486694\n",
      "Epoch: 426 | Discriminator Real Loss: [0.3178645372390747, 0.96875] | Discriminator Fake Loss: [0.7862270474433899, 0.40625] | GAN_Loss: 1.6597472429275513\n",
      "Epoch: 427 | Discriminator Real Loss: [0.43655431270599365, 0.8125] | Discriminator Fake Loss: [0.606399655342102, 0.59375] | GAN_Loss: 2.230924129486084\n",
      "Epoch: 428 | Discriminator Real Loss: [0.7015293836593628, 0.53125] | Discriminator Fake Loss: [0.4390403628349304, 0.78125] | GAN_Loss: 2.228170394897461\n",
      "Epoch: 429 | Discriminator Real Loss: [0.8022810220718384, 0.5] | Discriminator Fake Loss: [0.3951156735420227, 0.875] | GAN_Loss: 1.5082957744598389\n",
      "Epoch: 430 | Discriminator Real Loss: [0.48565781116485596, 0.75] | Discriminator Fake Loss: [0.7838971614837646, 0.5] | GAN_Loss: 1.3217490911483765\n",
      "Epoch: 431 | Discriminator Real Loss: [0.40224993228912354, 0.9375] | Discriminator Fake Loss: [0.8036056756973267, 0.5] | GAN_Loss: 1.577786922454834\n",
      "Epoch: 432 | Discriminator Real Loss: [0.36051636934280396, 0.90625] | Discriminator Fake Loss: [0.5798755884170532, 0.6875] | GAN_Loss: 1.663804531097412\n",
      "Epoch: 433 | Discriminator Real Loss: [0.5181374549865723, 0.75] | Discriminator Fake Loss: [0.4639010429382324, 0.71875] | GAN_Loss: 1.9692208766937256\n",
      "Epoch: 434 | Discriminator Real Loss: [0.5942842960357666, 0.65625] | Discriminator Fake Loss: [0.301455557346344, 0.875] | GAN_Loss: 2.0753211975097656\n",
      "Epoch: 435 | Discriminator Real Loss: [0.6597764492034912, 0.625] | Discriminator Fake Loss: [0.39303600788116455, 0.875] | GAN_Loss: 1.9410135746002197\n",
      "Epoch: 436 | Discriminator Real Loss: [0.4922495186328888, 0.78125] | Discriminator Fake Loss: [0.7121102809906006, 0.5] | GAN_Loss: 1.574965476989746\n",
      "Epoch: 437 | Discriminator Real Loss: [0.30141952633857727, 1.0] | Discriminator Fake Loss: [0.6192430257797241, 0.59375] | GAN_Loss: 1.7116596698760986\n",
      "Epoch: 438 | Discriminator Real Loss: [0.42399656772613525, 0.84375] | Discriminator Fake Loss: [0.45255088806152344, 0.78125] | GAN_Loss: 2.1903367042541504\n",
      "Epoch: 439 | Discriminator Real Loss: [0.4947059452533722, 0.8125] | Discriminator Fake Loss: [0.3307414650917053, 0.78125] | GAN_Loss: 1.84519362449646\n",
      "Epoch: 440 | Discriminator Real Loss: [0.6786564588546753, 0.5625] | Discriminator Fake Loss: [0.4365925192832947, 0.75] | GAN_Loss: 2.285820484161377\n",
      "Epoch: 441 | Discriminator Real Loss: [0.4644261598587036, 0.78125] | Discriminator Fake Loss: [0.39132237434387207, 0.75] | GAN_Loss: 1.946608066558838\n",
      "Epoch: 442 | Discriminator Real Loss: [0.46375393867492676, 0.84375] | Discriminator Fake Loss: [0.4720185399055481, 0.75] | GAN_Loss: 2.048990249633789\n",
      "Epoch: 443 | Discriminator Real Loss: [0.4177444875240326, 0.8125] | Discriminator Fake Loss: [0.40313926339149475, 0.75] | GAN_Loss: 2.2127175331115723\n",
      "Epoch: 444 | Discriminator Real Loss: [0.3460998833179474, 0.875] | Discriminator Fake Loss: [0.5081482529640198, 0.625] | GAN_Loss: 2.850193500518799\n",
      "Epoch: 445 | Discriminator Real Loss: [0.41201770305633545, 0.875] | Discriminator Fake Loss: [0.3462303876876831, 0.8125] | GAN_Loss: 2.787675380706787\n",
      "Epoch: 446 | Discriminator Real Loss: [0.5920961499214172, 0.6875] | Discriminator Fake Loss: [0.6634892821311951, 0.5625] | GAN_Loss: 2.820230722427368\n",
      "Epoch: 447 | Discriminator Real Loss: [0.5971159934997559, 0.71875] | Discriminator Fake Loss: [0.6458800435066223, 0.59375] | GAN_Loss: 2.4135947227478027\n",
      "Epoch: 448 | Discriminator Real Loss: [0.6068358421325684, 0.6875] | Discriminator Fake Loss: [0.5530550479888916, 0.75] | GAN_Loss: 3.119225025177002\n",
      "Epoch: 449 | Discriminator Real Loss: [0.529700517654419, 0.78125] | Discriminator Fake Loss: [0.5573474764823914, 0.6875] | GAN_Loss: 2.9567384719848633\n",
      "Epoch: 450 | Discriminator Real Loss: [1.0016530752182007, 0.5625] | Discriminator Fake Loss: [0.5907115936279297, 0.625] | GAN_Loss: 1.989270806312561\n",
      "Epoch: 451 | Discriminator Real Loss: [0.46201080083847046, 0.84375] | Discriminator Fake Loss: [0.7102072834968567, 0.5625] | GAN_Loss: 1.987597942352295\n",
      "Epoch: 452 | Discriminator Real Loss: [0.3601827919483185, 0.90625] | Discriminator Fake Loss: [0.5631437301635742, 0.65625] | GAN_Loss: 2.314009189605713\n",
      "Epoch: 453 | Discriminator Real Loss: [0.4754340350627899, 0.8125] | Discriminator Fake Loss: [0.39052367210388184, 0.75] | GAN_Loss: 3.50789213180542\n",
      "Epoch: 454 | Discriminator Real Loss: [0.6060793399810791, 0.75] | Discriminator Fake Loss: [0.3207859396934509, 0.84375] | GAN_Loss: 2.5969393253326416\n",
      "Epoch: 455 | Discriminator Real Loss: [0.420743465423584, 0.875] | Discriminator Fake Loss: [0.6725935935974121, 0.53125] | GAN_Loss: 2.118133306503296\n",
      "Epoch: 456 | Discriminator Real Loss: [0.5705638527870178, 0.8125] | Discriminator Fake Loss: [0.4676452577114105, 0.75] | GAN_Loss: 1.9435029029846191\n",
      "Epoch: 457 | Discriminator Real Loss: [0.3431859612464905, 0.9375] | Discriminator Fake Loss: [0.5197775363922119, 0.71875] | GAN_Loss: 2.293506145477295\n",
      "Epoch: 458 | Discriminator Real Loss: [0.4903876781463623, 0.78125] | Discriminator Fake Loss: [0.4965585470199585, 0.75] | GAN_Loss: 2.0621697902679443\n",
      "Epoch: 459 | Discriminator Real Loss: [0.5851311087608337, 0.71875] | Discriminator Fake Loss: [0.42830565571784973, 0.71875] | GAN_Loss: 1.976460337638855\n",
      "Epoch: 460 | Discriminator Real Loss: [0.5011072158813477, 0.78125] | Discriminator Fake Loss: [0.5461262464523315, 0.625] | GAN_Loss: 1.9819176197052002\n",
      "Epoch: 461 | Discriminator Real Loss: [0.47047069668769836, 0.8125] | Discriminator Fake Loss: [0.45491451025009155, 0.75] | GAN_Loss: 1.9804868698120117\n",
      "Epoch: 462 | Discriminator Real Loss: [0.48632749915122986, 0.875] | Discriminator Fake Loss: [0.37757378816604614, 0.84375] | GAN_Loss: 1.96012544631958\n",
      "Epoch: 463 | Discriminator Real Loss: [0.5544437170028687, 0.75] | Discriminator Fake Loss: [0.43146684765815735, 0.78125] | GAN_Loss: 1.8280640840530396\n",
      "Epoch: 464 | Discriminator Real Loss: [0.45964205265045166, 0.875] | Discriminator Fake Loss: [0.6452675461769104, 0.59375] | GAN_Loss: 2.026707887649536\n",
      "Epoch: 465 | Discriminator Real Loss: [0.37979432940483093, 0.90625] | Discriminator Fake Loss: [0.785307765007019, 0.46875] | GAN_Loss: 2.3624987602233887\n",
      "Epoch: 466 | Discriminator Real Loss: [0.8488737940788269, 0.5625] | Discriminator Fake Loss: [0.39334437251091003, 0.8125] | GAN_Loss: 2.6205267906188965\n",
      "Epoch: 467 | Discriminator Real Loss: [0.530013918876648, 0.78125] | Discriminator Fake Loss: [0.594952404499054, 0.65625] | GAN_Loss: 1.9912227392196655\n",
      "Epoch: 468 | Discriminator Real Loss: [0.32148584723472595, 0.96875] | Discriminator Fake Loss: [0.9318331480026245, 0.4375] | GAN_Loss: 1.8648152351379395\n",
      "Epoch: 469 | Discriminator Real Loss: [0.7298822999000549, 0.6875] | Discriminator Fake Loss: [0.5867701172828674, 0.59375] | GAN_Loss: 2.0034914016723633\n",
      "Epoch: 470 | Discriminator Real Loss: [0.4814453721046448, 0.75] | Discriminator Fake Loss: [0.6929870843887329, 0.53125] | GAN_Loss: 1.7065927982330322\n",
      "Epoch: 471 | Discriminator Real Loss: [0.4277477264404297, 0.875] | Discriminator Fake Loss: [0.5064970254898071, 0.65625] | GAN_Loss: 2.0344080924987793\n",
      "Epoch: 472 | Discriminator Real Loss: [0.5420930981636047, 0.75] | Discriminator Fake Loss: [0.44879117608070374, 0.75] | GAN_Loss: 2.2298197746276855\n",
      "Epoch: 473 | Discriminator Real Loss: [0.706670880317688, 0.53125] | Discriminator Fake Loss: [0.460759699344635, 0.71875] | GAN_Loss: 1.9385803937911987\n",
      "Epoch: 474 | Discriminator Real Loss: [0.7284975647926331, 0.53125] | Discriminator Fake Loss: [0.49096235632896423, 0.71875] | GAN_Loss: 1.3913495540618896\n",
      "Epoch: 475 | Discriminator Real Loss: [0.3810458779335022, 0.96875] | Discriminator Fake Loss: [0.7616613507270813, 0.40625] | GAN_Loss: 1.3346730470657349\n",
      "Epoch: 476 | Discriminator Real Loss: [0.452297568321228, 0.875] | Discriminator Fake Loss: [0.4436515271663666, 0.6875] | GAN_Loss: 1.8116722106933594\n",
      "Epoch: 477 | Discriminator Real Loss: [0.5155853629112244, 0.75] | Discriminator Fake Loss: [0.5791995525360107, 0.75] | GAN_Loss: 1.6072473526000977\n",
      "Epoch: 478 | Discriminator Real Loss: [0.6264396905899048, 0.65625] | Discriminator Fake Loss: [0.5927044153213501, 0.625] | GAN_Loss: 1.3989957571029663\n",
      "Epoch: 479 | Discriminator Real Loss: [0.5834960341453552, 0.78125] | Discriminator Fake Loss: [0.538489580154419, 0.65625] | GAN_Loss: 1.6629924774169922\n",
      "Epoch: 480 | Discriminator Real Loss: [0.5522853136062622, 0.78125] | Discriminator Fake Loss: [0.7036280035972595, 0.53125] | GAN_Loss: 1.717340350151062\n",
      "Epoch: 481 | Discriminator Real Loss: [0.4865180253982544, 0.84375] | Discriminator Fake Loss: [0.6193387508392334, 0.6875] | GAN_Loss: 1.9069162607192993\n",
      "Epoch: 482 | Discriminator Real Loss: [0.5099936723709106, 0.75] | Discriminator Fake Loss: [0.6636124849319458, 0.625] | GAN_Loss: 1.644899606704712\n",
      "Epoch: 483 | Discriminator Real Loss: [0.6074230670928955, 0.625] | Discriminator Fake Loss: [0.5224753618240356, 0.6875] | GAN_Loss: 1.8210135698318481\n",
      "Epoch: 484 | Discriminator Real Loss: [0.6181389689445496, 0.75] | Discriminator Fake Loss: [0.5419896841049194, 0.65625] | GAN_Loss: 1.8849823474884033\n",
      "Epoch: 485 | Discriminator Real Loss: [0.6614718437194824, 0.6875] | Discriminator Fake Loss: [0.6073138117790222, 0.59375] | GAN_Loss: 1.6577937602996826\n",
      "Epoch: 486 | Discriminator Real Loss: [0.43824562430381775, 0.90625] | Discriminator Fake Loss: [0.7902030348777771, 0.46875] | GAN_Loss: 1.2826788425445557\n",
      "Epoch: 487 | Discriminator Real Loss: [0.368702232837677, 0.9375] | Discriminator Fake Loss: [0.8677239418029785, 0.5] | GAN_Loss: 1.7087887525558472\n",
      "Epoch: 488 | Discriminator Real Loss: [0.44880983233451843, 0.84375] | Discriminator Fake Loss: [0.7589519023895264, 0.46875] | GAN_Loss: 1.6899793148040771\n",
      "Epoch: 489 | Discriminator Real Loss: [0.740787923336029, 0.53125] | Discriminator Fake Loss: [0.5764716863632202, 0.59375] | GAN_Loss: 2.4109315872192383\n",
      "Epoch: 490 | Discriminator Real Loss: [0.94761723279953, 0.34375] | Discriminator Fake Loss: [0.5294692516326904, 0.6875] | GAN_Loss: 1.8708559274673462\n",
      "Epoch: 491 | Discriminator Real Loss: [0.6529190540313721, 0.65625] | Discriminator Fake Loss: [0.7071569561958313, 0.46875] | GAN_Loss: 1.732805848121643\n",
      "Epoch: 492 | Discriminator Real Loss: [0.4830235242843628, 0.78125] | Discriminator Fake Loss: [0.8412798643112183, 0.40625] | GAN_Loss: 1.6356735229492188\n",
      "Epoch: 493 | Discriminator Real Loss: [0.4537964165210724, 0.875] | Discriminator Fake Loss: [0.5982789993286133, 0.59375] | GAN_Loss: 2.067105293273926\n",
      "Epoch: 494 | Discriminator Real Loss: [0.8335932493209839, 0.53125] | Discriminator Fake Loss: [0.48926565051078796, 0.71875] | GAN_Loss: 1.5928547382354736\n",
      "Epoch: 495 | Discriminator Real Loss: [0.6100713014602661, 0.65625] | Discriminator Fake Loss: [0.7650607824325562, 0.40625] | GAN_Loss: 1.3694524765014648\n",
      "Epoch: 496 | Discriminator Real Loss: [0.6787124276161194, 0.65625] | Discriminator Fake Loss: [0.6411409378051758, 0.53125] | GAN_Loss: 1.6746121644973755\n",
      "Epoch: 497 | Discriminator Real Loss: [0.5378286838531494, 0.78125] | Discriminator Fake Loss: [0.6733181476593018, 0.40625] | GAN_Loss: 1.4720897674560547\n",
      "Epoch: 498 | Discriminator Real Loss: [0.5633636116981506, 0.78125] | Discriminator Fake Loss: [0.652111291885376, 0.46875] | GAN_Loss: 1.2148077487945557\n",
      "Epoch: 499 | Discriminator Real Loss: [0.4087870717048645, 0.90625] | Discriminator Fake Loss: [0.6902188062667847, 0.375] | GAN_Loss: 1.6899360418319702\n",
      "Epoch: 500 | Discriminator Real Loss: [0.5901739597320557, 0.8125] | Discriminator Fake Loss: [0.5458735227584839, 0.71875] | GAN_Loss: 2.2673532962799072\n",
      "Epoch: 501 | Discriminator Real Loss: [0.6857292652130127, 0.75] | Discriminator Fake Loss: [0.5666546821594238, 0.5625] | GAN_Loss: 1.7127065658569336\n",
      "Epoch: 502 | Discriminator Real Loss: [0.6762412786483765, 0.5] | Discriminator Fake Loss: [0.5217780470848083, 0.75] | GAN_Loss: 1.8500206470489502\n",
      "Epoch: 503 | Discriminator Real Loss: [0.75605309009552, 0.46875] | Discriminator Fake Loss: [0.5655691623687744, 0.625] | GAN_Loss: 1.126662254333496\n",
      "Epoch: 504 | Discriminator Real Loss: [0.4896745979785919, 0.90625] | Discriminator Fake Loss: [0.7822142839431763, 0.34375] | GAN_Loss: 1.2415534257888794\n",
      "Epoch: 505 | Discriminator Real Loss: [0.4887191653251648, 0.84375] | Discriminator Fake Loss: [0.6860847473144531, 0.5625] | GAN_Loss: 1.1359813213348389\n",
      "Epoch: 506 | Discriminator Real Loss: [0.5183326005935669, 0.8125] | Discriminator Fake Loss: [0.6498870849609375, 0.53125] | GAN_Loss: 1.3524452447891235\n",
      "Epoch: 507 | Discriminator Real Loss: [0.5181736946105957, 0.8125] | Discriminator Fake Loss: [0.49776580929756165, 0.71875] | GAN_Loss: 1.6061410903930664\n",
      "Epoch: 508 | Discriminator Real Loss: [0.6271600723266602, 0.78125] | Discriminator Fake Loss: [0.5317603349685669, 0.6875] | GAN_Loss: 1.5690107345581055\n",
      "Epoch: 509 | Discriminator Real Loss: [0.6398355960845947, 0.65625] | Discriminator Fake Loss: [0.4701642394065857, 0.71875] | GAN_Loss: 1.563370704650879\n",
      "Epoch: 510 | Discriminator Real Loss: [0.7135737538337708, 0.59375] | Discriminator Fake Loss: [0.5462172627449036, 0.625] | GAN_Loss: 1.2762928009033203\n",
      "Epoch: 511 | Discriminator Real Loss: [0.5639137029647827, 0.78125] | Discriminator Fake Loss: [0.642806887626648, 0.5625] | GAN_Loss: 1.166941523551941\n",
      "Epoch: 512 | Discriminator Real Loss: [0.42632460594177246, 0.90625] | Discriminator Fake Loss: [0.6521387100219727, 0.5] | GAN_Loss: 1.350417971611023\n",
      "Epoch: 513 | Discriminator Real Loss: [0.4667603671550751, 0.84375] | Discriminator Fake Loss: [0.7411288619041443, 0.4375] | GAN_Loss: 1.396393895149231\n",
      "Epoch: 514 | Discriminator Real Loss: [0.5467168688774109, 0.78125] | Discriminator Fake Loss: [0.4831291437149048, 0.78125] | GAN_Loss: 1.753692865371704\n",
      "Epoch: 515 | Discriminator Real Loss: [0.791025698184967, 0.5] | Discriminator Fake Loss: [0.4234435558319092, 0.84375] | GAN_Loss: 2.111929416656494\n",
      "Epoch: 516 | Discriminator Real Loss: [0.7649478912353516, 0.53125] | Discriminator Fake Loss: [0.4592054784297943, 0.78125] | GAN_Loss: 1.207800030708313\n",
      "Epoch: 517 | Discriminator Real Loss: [0.4979761242866516, 0.875] | Discriminator Fake Loss: [0.4918200373649597, 0.6875] | GAN_Loss: 1.5023019313812256\n",
      "Epoch: 518 | Discriminator Real Loss: [0.45011013746261597, 0.84375] | Discriminator Fake Loss: [0.6520222425460815, 0.53125] | GAN_Loss: 1.7186743021011353\n",
      "Epoch: 519 | Discriminator Real Loss: [0.4072834253311157, 0.875] | Discriminator Fake Loss: [0.7045063972473145, 0.46875] | GAN_Loss: 1.5433683395385742\n",
      "Epoch: 520 | Discriminator Real Loss: [0.4662066400051117, 0.875] | Discriminator Fake Loss: [0.32623186707496643, 0.875] | GAN_Loss: 2.226712226867676\n",
      "Epoch: 521 | Discriminator Real Loss: [0.5252884030342102, 0.78125] | Discriminator Fake Loss: [0.24661335349082947, 0.875] | GAN_Loss: 2.7577672004699707\n",
      "Epoch: 522 | Discriminator Real Loss: [0.6644896864891052, 0.6875] | Discriminator Fake Loss: [0.5108750462532043, 0.6875] | GAN_Loss: 1.9311522245407104\n",
      "Epoch: 523 | Discriminator Real Loss: [0.5807497501373291, 0.75] | Discriminator Fake Loss: [0.4867764115333557, 0.8125] | GAN_Loss: 1.511106252670288\n",
      "Epoch: 524 | Discriminator Real Loss: [0.38605302572250366, 0.84375] | Discriminator Fake Loss: [0.7638899683952332, 0.5] | GAN_Loss: 1.555275559425354\n",
      "Epoch: 525 | Discriminator Real Loss: [0.29452842473983765, 0.9375] | Discriminator Fake Loss: [0.6577463746070862, 0.5625] | GAN_Loss: 2.623422861099243\n",
      "Epoch: 526 | Discriminator Real Loss: [0.8105278611183167, 0.59375] | Discriminator Fake Loss: [0.3601287007331848, 0.84375] | GAN_Loss: 2.5131850242614746\n",
      "Epoch: 527 | Discriminator Real Loss: [0.7187413573265076, 0.5625] | Discriminator Fake Loss: [0.5389977693557739, 0.6875] | GAN_Loss: 1.6874926090240479\n",
      "Epoch: 528 | Discriminator Real Loss: [0.4031638205051422, 0.90625] | Discriminator Fake Loss: [0.7730813026428223, 0.53125] | GAN_Loss: 1.3357062339782715\n",
      "Epoch: 529 | Discriminator Real Loss: [0.3269554376602173, 0.9375] | Discriminator Fake Loss: [0.7714889049530029, 0.46875] | GAN_Loss: 1.653385043144226\n",
      "Epoch: 530 | Discriminator Real Loss: [0.8830083012580872, 0.5625] | Discriminator Fake Loss: [0.5979981422424316, 0.59375] | GAN_Loss: 2.0340652465820312\n",
      "Epoch: 531 | Discriminator Real Loss: [0.6588387489318848, 0.65625] | Discriminator Fake Loss: [0.5585991144180298, 0.71875] | GAN_Loss: 2.199014902114868\n",
      "Epoch: 532 | Discriminator Real Loss: [0.6035798788070679, 0.71875] | Discriminator Fake Loss: [0.6099604368209839, 0.59375] | GAN_Loss: 1.693673849105835\n",
      "Epoch: 533 | Discriminator Real Loss: [0.6998848915100098, 0.78125] | Discriminator Fake Loss: [0.6207867860794067, 0.6875] | GAN_Loss: 1.4318513870239258\n",
      "Epoch: 534 | Discriminator Real Loss: [0.6777029037475586, 0.59375] | Discriminator Fake Loss: [0.49080991744995117, 0.71875] | GAN_Loss: 1.1889052391052246\n",
      "Epoch: 535 | Discriminator Real Loss: [0.41654878854751587, 0.90625] | Discriminator Fake Loss: [0.8312475681304932, 0.5] | GAN_Loss: 1.0736794471740723\n",
      "Epoch: 536 | Discriminator Real Loss: [0.4446726441383362, 0.84375] | Discriminator Fake Loss: [0.9606930017471313, 0.34375] | GAN_Loss: 1.2999054193496704\n",
      "Epoch: 537 | Discriminator Real Loss: [0.5548162460327148, 0.8125] | Discriminator Fake Loss: [0.6170519590377808, 0.53125] | GAN_Loss: 1.9670342206954956\n",
      "Epoch: 538 | Discriminator Real Loss: [0.7808029651641846, 0.5625] | Discriminator Fake Loss: [0.44402363896369934, 0.75] | GAN_Loss: 1.7431130409240723\n",
      "Epoch: 539 | Discriminator Real Loss: [0.7844303846359253, 0.6875] | Discriminator Fake Loss: [0.548123836517334, 0.625] | GAN_Loss: 1.5885186195373535\n",
      "Epoch: 540 | Discriminator Real Loss: [0.6256400346755981, 0.6875] | Discriminator Fake Loss: [0.7607432007789612, 0.3125] | GAN_Loss: 1.2885504961013794\n",
      "Epoch: 541 | Discriminator Real Loss: [0.49825525283813477, 0.78125] | Discriminator Fake Loss: [0.6415562629699707, 0.5] | GAN_Loss: 1.2907168865203857\n",
      "Epoch: 542 | Discriminator Real Loss: [0.5654895305633545, 0.6875] | Discriminator Fake Loss: [0.7233020663261414, 0.46875] | GAN_Loss: 1.3482012748718262\n",
      "Epoch: 543 | Discriminator Real Loss: [0.6083629131317139, 0.6875] | Discriminator Fake Loss: [0.5762130618095398, 0.5625] | GAN_Loss: 1.3377100229263306\n",
      "Epoch: 544 | Discriminator Real Loss: [0.5350871086120605, 0.8125] | Discriminator Fake Loss: [0.6019881367683411, 0.59375] | GAN_Loss: 1.5280799865722656\n",
      "Epoch: 545 | Discriminator Real Loss: [0.6024997234344482, 0.71875] | Discriminator Fake Loss: [0.49542874097824097, 0.6875] | GAN_Loss: 1.9438726902008057\n",
      "Epoch: 546 | Discriminator Real Loss: [0.6546422839164734, 0.71875] | Discriminator Fake Loss: [0.49546337127685547, 0.65625] | GAN_Loss: 1.5005075931549072\n",
      "Epoch: 547 | Discriminator Real Loss: [0.5202218294143677, 0.90625] | Discriminator Fake Loss: [0.5457135438919067, 0.6875] | GAN_Loss: 1.5048651695251465\n",
      "Epoch: 548 | Discriminator Real Loss: [0.48967599868774414, 0.875] | Discriminator Fake Loss: [0.6071814298629761, 0.59375] | GAN_Loss: 1.5634181499481201\n",
      "Epoch: 549 | Discriminator Real Loss: [0.5387293100357056, 0.8125] | Discriminator Fake Loss: [0.6255053877830505, 0.625] | GAN_Loss: 1.524548053741455\n",
      "Epoch: 550 | Discriminator Real Loss: [0.5092527866363525, 0.78125] | Discriminator Fake Loss: [0.6334934830665588, 0.5625] | GAN_Loss: 1.2701129913330078\n",
      "Epoch: 551 | Discriminator Real Loss: [0.515499472618103, 0.84375] | Discriminator Fake Loss: [0.6914787888526917, 0.5] | GAN_Loss: 1.3263964653015137\n",
      "Epoch: 552 | Discriminator Real Loss: [0.7314832806587219, 0.59375] | Discriminator Fake Loss: [0.5277256369590759, 0.625] | GAN_Loss: 1.4037230014801025\n",
      "Epoch: 553 | Discriminator Real Loss: [0.5329976081848145, 0.6875] | Discriminator Fake Loss: [0.5707398056983948, 0.65625] | GAN_Loss: 1.6389007568359375\n",
      "Epoch: 554 | Discriminator Real Loss: [0.5635625123977661, 0.71875] | Discriminator Fake Loss: [0.5564751625061035, 0.59375] | GAN_Loss: 1.4892292022705078\n",
      "Epoch: 555 | Discriminator Real Loss: [0.4460541009902954, 0.875] | Discriminator Fake Loss: [0.6073088049888611, 0.625] | GAN_Loss: 2.0659024715423584\n",
      "Epoch: 556 | Discriminator Real Loss: [0.6313513517379761, 0.71875] | Discriminator Fake Loss: [0.47113579511642456, 0.78125] | GAN_Loss: 1.5197038650512695\n",
      "Epoch: 557 | Discriminator Real Loss: [0.5010708570480347, 0.84375] | Discriminator Fake Loss: [0.5828375816345215, 0.59375] | GAN_Loss: 1.4958949089050293\n",
      "Epoch: 558 | Discriminator Real Loss: [0.5109519362449646, 0.75] | Discriminator Fake Loss: [0.442815363407135, 0.84375] | GAN_Loss: 1.5203897953033447\n",
      "Epoch: 559 | Discriminator Real Loss: [0.5681552290916443, 0.78125] | Discriminator Fake Loss: [0.6413341760635376, 0.625] | GAN_Loss: 1.3435451984405518\n",
      "Epoch: 560 | Discriminator Real Loss: [0.4531010389328003, 0.84375] | Discriminator Fake Loss: [0.6022629141807556, 0.5625] | GAN_Loss: 1.5283595323562622\n",
      "Epoch: 561 | Discriminator Real Loss: [0.49743348360061646, 0.78125] | Discriminator Fake Loss: [0.6142626404762268, 0.59375] | GAN_Loss: 1.5522220134735107\n",
      "Epoch: 562 | Discriminator Real Loss: [0.5698217153549194, 0.75] | Discriminator Fake Loss: [0.4478367567062378, 0.75] | GAN_Loss: 1.8082233667373657\n",
      "Epoch: 563 | Discriminator Real Loss: [0.6976784467697144, 0.625] | Discriminator Fake Loss: [0.520085871219635, 0.65625] | GAN_Loss: 1.408889651298523\n",
      "Epoch: 564 | Discriminator Real Loss: [0.5244894623756409, 0.78125] | Discriminator Fake Loss: [0.6020379066467285, 0.59375] | GAN_Loss: 1.5334643125534058\n",
      "Epoch: 565 | Discriminator Real Loss: [0.6256719827651978, 0.65625] | Discriminator Fake Loss: [0.6752074956893921, 0.59375] | GAN_Loss: 1.401667833328247\n",
      "Epoch: 566 | Discriminator Real Loss: [0.3860190510749817, 0.875] | Discriminator Fake Loss: [0.7076910734176636, 0.5625] | GAN_Loss: 1.4624791145324707\n",
      "Epoch: 567 | Discriminator Real Loss: [0.47266995906829834, 0.8125] | Discriminator Fake Loss: [0.6008105278015137, 0.65625] | GAN_Loss: 1.9565309286117554\n",
      "Epoch: 568 | Discriminator Real Loss: [0.6682791709899902, 0.71875] | Discriminator Fake Loss: [0.45851099491119385, 0.71875] | GAN_Loss: 1.9973909854888916\n",
      "Epoch: 569 | Discriminator Real Loss: [0.8377236127853394, 0.5] | Discriminator Fake Loss: [0.49424970149993896, 0.78125] | GAN_Loss: 1.384864091873169\n",
      "Epoch: 570 | Discriminator Real Loss: [0.7151613235473633, 0.53125] | Discriminator Fake Loss: [0.7562252283096313, 0.4375] | GAN_Loss: 1.0040979385375977\n",
      "Epoch: 571 | Discriminator Real Loss: [0.35949549078941345, 0.9375] | Discriminator Fake Loss: [0.7232476472854614, 0.4375] | GAN_Loss: 0.9628183245658875\n",
      "Epoch: 572 | Discriminator Real Loss: [0.39050936698913574, 0.90625] | Discriminator Fake Loss: [0.6439661383628845, 0.5625] | GAN_Loss: 1.2250679731369019\n",
      "Epoch: 573 | Discriminator Real Loss: [0.6235413551330566, 0.6875] | Discriminator Fake Loss: [0.5673733353614807, 0.625] | GAN_Loss: 1.6992216110229492\n",
      "Epoch: 574 | Discriminator Real Loss: [0.6376219987869263, 0.6875] | Discriminator Fake Loss: [0.42083093523979187, 0.78125] | GAN_Loss: 1.7053124904632568\n",
      "Epoch: 575 | Discriminator Real Loss: [0.6005408763885498, 0.8125] | Discriminator Fake Loss: [0.605349063873291, 0.625] | GAN_Loss: 1.6466360092163086\n",
      "Epoch: 576 | Discriminator Real Loss: [0.6362043619155884, 0.65625] | Discriminator Fake Loss: [0.5988585352897644, 0.59375] | GAN_Loss: 1.5686604976654053\n",
      "Epoch: 577 | Discriminator Real Loss: [0.567618727684021, 0.78125] | Discriminator Fake Loss: [0.5252300500869751, 0.6875] | GAN_Loss: 1.6152572631835938\n",
      "Epoch: 578 | Discriminator Real Loss: [0.45458146929740906, 0.75] | Discriminator Fake Loss: [0.4742860794067383, 0.78125] | GAN_Loss: 1.4297356605529785\n",
      "Epoch: 579 | Discriminator Real Loss: [0.42659851908683777, 0.90625] | Discriminator Fake Loss: [0.4413059949874878, 0.78125] | GAN_Loss: 2.209838390350342\n",
      "Epoch: 580 | Discriminator Real Loss: [0.6080833673477173, 0.71875] | Discriminator Fake Loss: [0.3976649045944214, 0.8125] | GAN_Loss: 2.1218276023864746\n",
      "Epoch: 581 | Discriminator Real Loss: [0.8452683687210083, 0.5625] | Discriminator Fake Loss: [0.682759165763855, 0.625] | GAN_Loss: 1.1004507541656494\n",
      "Epoch: 582 | Discriminator Real Loss: [0.3440116047859192, 0.875] | Discriminator Fake Loss: [0.8576231598854065, 0.5] | GAN_Loss: 1.1586787700653076\n",
      "Epoch: 583 | Discriminator Real Loss: [0.5846017003059387, 0.65625] | Discriminator Fake Loss: [0.6926993131637573, 0.5625] | GAN_Loss: 1.396735668182373\n",
      "Epoch: 584 | Discriminator Real Loss: [0.458179771900177, 0.71875] | Discriminator Fake Loss: [0.5602812170982361, 0.625] | GAN_Loss: 1.6305267810821533\n",
      "Epoch: 585 | Discriminator Real Loss: [0.9017224311828613, 0.46875] | Discriminator Fake Loss: [0.4103897213935852, 0.8125] | GAN_Loss: 1.55440092086792\n",
      "Epoch: 586 | Discriminator Real Loss: [0.683197557926178, 0.6875] | Discriminator Fake Loss: [0.6101415157318115, 0.5625] | GAN_Loss: 1.1638894081115723\n",
      "Epoch: 587 | Discriminator Real Loss: [0.4912092387676239, 0.75] | Discriminator Fake Loss: [0.7675738334655762, 0.53125] | GAN_Loss: 1.0000174045562744\n",
      "Epoch: 588 | Discriminator Real Loss: [0.4726570248603821, 0.75] | Discriminator Fake Loss: [0.6896252632141113, 0.59375] | GAN_Loss: 1.2386893033981323\n",
      "Epoch: 589 | Discriminator Real Loss: [0.5605185031890869, 0.78125] | Discriminator Fake Loss: [0.5999883413314819, 0.625] | GAN_Loss: 1.1074423789978027\n",
      "Epoch: 590 | Discriminator Real Loss: [0.5415921211242676, 0.75] | Discriminator Fake Loss: [0.6339901685714722, 0.59375] | GAN_Loss: 1.324384331703186\n",
      "Epoch: 591 | Discriminator Real Loss: [0.6835904121398926, 0.59375] | Discriminator Fake Loss: [0.5908499956130981, 0.65625] | GAN_Loss: 1.4486796855926514\n",
      "Epoch: 592 | Discriminator Real Loss: [0.7187356948852539, 0.59375] | Discriminator Fake Loss: [0.5407238006591797, 0.71875] | GAN_Loss: 1.3766453266143799\n",
      "Epoch: 593 | Discriminator Real Loss: [0.678464412689209, 0.6875] | Discriminator Fake Loss: [0.5379922986030579, 0.625] | GAN_Loss: 1.205711007118225\n",
      "Epoch: 594 | Discriminator Real Loss: [0.6176398992538452, 0.71875] | Discriminator Fake Loss: [0.6957863569259644, 0.5] | GAN_Loss: 0.9872724413871765\n",
      "Epoch: 595 | Discriminator Real Loss: [0.48619890213012695, 0.84375] | Discriminator Fake Loss: [0.8020687103271484, 0.40625] | GAN_Loss: 0.8693497180938721\n",
      "Epoch: 596 | Discriminator Real Loss: [0.49970388412475586, 0.84375] | Discriminator Fake Loss: [0.7510541677474976, 0.46875] | GAN_Loss: 0.9254985451698303\n",
      "Epoch: 597 | Discriminator Real Loss: [0.5072389841079712, 0.8125] | Discriminator Fake Loss: [0.6230021119117737, 0.625] | GAN_Loss: 1.2270824909210205\n",
      "Epoch: 598 | Discriminator Real Loss: [0.5654401779174805, 0.8125] | Discriminator Fake Loss: [0.6604790687561035, 0.53125] | GAN_Loss: 1.4516204595565796\n",
      "Epoch: 599 | Discriminator Real Loss: [0.7479516863822937, 0.5625] | Discriminator Fake Loss: [0.5851190090179443, 0.65625] | GAN_Loss: 1.248420000076294\n",
      "Epoch: 600 | Discriminator Real Loss: [0.7998979091644287, 0.5] | Discriminator Fake Loss: [0.4875671863555908, 0.75] | GAN_Loss: 1.3480558395385742\n",
      "Epoch: 601 | Discriminator Real Loss: [0.8179372549057007, 0.53125] | Discriminator Fake Loss: [0.5384088158607483, 0.6875] | GAN_Loss: 0.7136090397834778\n",
      "Epoch: 602 | Discriminator Real Loss: [0.4252697229385376, 0.90625] | Discriminator Fake Loss: [0.8389146327972412, 0.40625] | GAN_Loss: 0.82378089427948\n",
      "Epoch: 603 | Discriminator Real Loss: [0.42005687952041626, 0.875] | Discriminator Fake Loss: [0.9927705526351929, 0.25] | GAN_Loss: 0.7755674123764038\n",
      "Epoch: 604 | Discriminator Real Loss: [0.45442813634872437, 0.84375] | Discriminator Fake Loss: [0.8244497179985046, 0.4375] | GAN_Loss: 1.2871949672698975\n",
      "Epoch: 605 | Discriminator Real Loss: [0.6184656620025635, 0.71875] | Discriminator Fake Loss: [0.49543336033821106, 0.78125] | GAN_Loss: 1.4743894338607788\n",
      "Epoch: 606 | Discriminator Real Loss: [0.8886157870292664, 0.40625] | Discriminator Fake Loss: [0.5004080533981323, 0.75] | GAN_Loss: 1.2487916946411133\n",
      "Epoch: 607 | Discriminator Real Loss: [0.7678369283676147, 0.65625] | Discriminator Fake Loss: [0.5329394936561584, 0.71875] | GAN_Loss: 1.0481230020523071\n",
      "Epoch: 608 | Discriminator Real Loss: [0.6036993265151978, 0.75] | Discriminator Fake Loss: [0.6932639479637146, 0.5625] | GAN_Loss: 0.9336990118026733\n",
      "Epoch: 609 | Discriminator Real Loss: [0.44332605600357056, 0.9375] | Discriminator Fake Loss: [0.7362015843391418, 0.40625] | GAN_Loss: 0.9876940846443176\n",
      "Epoch: 610 | Discriminator Real Loss: [0.532642126083374, 0.75] | Discriminator Fake Loss: [0.6905092000961304, 0.53125] | GAN_Loss: 1.1549608707427979\n",
      "Epoch: 611 | Discriminator Real Loss: [0.5335329174995422, 0.8125] | Discriminator Fake Loss: [0.5417636632919312, 0.8125] | GAN_Loss: 1.7597417831420898\n",
      "Epoch: 612 | Discriminator Real Loss: [0.7204656600952148, 0.53125] | Discriminator Fake Loss: [0.45593079924583435, 0.8125] | GAN_Loss: 1.5169742107391357\n",
      "Epoch: 613 | Discriminator Real Loss: [0.838787317276001, 0.40625] | Discriminator Fake Loss: [0.45894598960876465, 0.8125] | GAN_Loss: 1.3923406600952148\n",
      "Epoch: 614 | Discriminator Real Loss: [0.6669247150421143, 0.65625] | Discriminator Fake Loss: [0.49152714014053345, 0.75] | GAN_Loss: 0.9545321464538574\n",
      "Epoch: 615 | Discriminator Real Loss: [0.5016044974327087, 0.84375] | Discriminator Fake Loss: [0.7859941720962524, 0.5] | GAN_Loss: 0.9824799299240112\n",
      "Epoch: 616 | Discriminator Real Loss: [0.4214403033256531, 0.875] | Discriminator Fake Loss: [0.6044970750808716, 0.5625] | GAN_Loss: 1.137287974357605\n",
      "Epoch: 617 | Discriminator Real Loss: [0.4867693781852722, 0.8125] | Discriminator Fake Loss: [0.7149856686592102, 0.53125] | GAN_Loss: 1.4525219202041626\n",
      "Epoch: 618 | Discriminator Real Loss: [0.6798439025878906, 0.625] | Discriminator Fake Loss: [0.470828652381897, 0.71875] | GAN_Loss: 1.2918897867202759\n",
      "Epoch: 619 | Discriminator Real Loss: [0.6279991269111633, 0.59375] | Discriminator Fake Loss: [0.6427041292190552, 0.625] | GAN_Loss: 1.5235313177108765\n",
      "Epoch: 620 | Discriminator Real Loss: [0.5236336588859558, 0.84375] | Discriminator Fake Loss: [0.6251611709594727, 0.59375] | GAN_Loss: 1.233187198638916\n",
      "Epoch: 621 | Discriminator Real Loss: [0.5907512903213501, 0.71875] | Discriminator Fake Loss: [0.6261081099510193, 0.53125] | GAN_Loss: 1.2519437074661255\n",
      "Epoch: 622 | Discriminator Real Loss: [0.6824083924293518, 0.53125] | Discriminator Fake Loss: [0.5443287491798401, 0.75] | GAN_Loss: 1.2366989850997925\n",
      "Epoch: 623 | Discriminator Real Loss: [0.6683562994003296, 0.65625] | Discriminator Fake Loss: [0.6476820707321167, 0.53125] | GAN_Loss: 0.9197043180465698\n",
      "Epoch: 624 | Discriminator Real Loss: [0.5412535667419434, 0.71875] | Discriminator Fake Loss: [0.8909286260604858, 0.40625] | GAN_Loss: 0.8952364325523376\n",
      "Epoch: 625 | Discriminator Real Loss: [0.4327995777130127, 0.84375] | Discriminator Fake Loss: [0.6930807828903198, 0.53125] | GAN_Loss: 0.9885220527648926\n",
      "Epoch: 626 | Discriminator Real Loss: [0.49451035261154175, 0.78125] | Discriminator Fake Loss: [0.5693714618682861, 0.65625] | GAN_Loss: 1.3277825117111206\n",
      "Epoch: 627 | Discriminator Real Loss: [0.8297631144523621, 0.5] | Discriminator Fake Loss: [0.41487517952919006, 0.84375] | GAN_Loss: 1.203282356262207\n",
      "Epoch: 628 | Discriminator Real Loss: [0.8113131523132324, 0.53125] | Discriminator Fake Loss: [0.6248152256011963, 0.625] | GAN_Loss: 1.2938828468322754\n",
      "Epoch: 629 | Discriminator Real Loss: [0.6121921539306641, 0.75] | Discriminator Fake Loss: [0.7045486569404602, 0.5] | GAN_Loss: 0.9404823184013367\n",
      "Epoch: 630 | Discriminator Real Loss: [0.695658266544342, 0.625] | Discriminator Fake Loss: [0.6452040672302246, 0.5625] | GAN_Loss: 1.0269577503204346\n",
      "Epoch: 631 | Discriminator Real Loss: [0.48509481549263, 0.78125] | Discriminator Fake Loss: [0.677812397480011, 0.5625] | GAN_Loss: 1.1324985027313232\n",
      "Epoch: 632 | Discriminator Real Loss: [0.6412518620491028, 0.75] | Discriminator Fake Loss: [0.5515233874320984, 0.71875] | GAN_Loss: 1.3356789350509644\n",
      "Epoch: 633 | Discriminator Real Loss: [0.5804392099380493, 0.6875] | Discriminator Fake Loss: [0.4885399043560028, 0.8125] | GAN_Loss: 1.4387186765670776\n",
      "Epoch: 634 | Discriminator Real Loss: [0.666273295879364, 0.625] | Discriminator Fake Loss: [0.6294763088226318, 0.59375] | GAN_Loss: 1.5221939086914062\n",
      "Epoch: 635 | Discriminator Real Loss: [0.657931923866272, 0.65625] | Discriminator Fake Loss: [0.5502291321754456, 0.65625] | GAN_Loss: 1.3816704750061035\n",
      "Epoch: 636 | Discriminator Real Loss: [0.7504697442054749, 0.5] | Discriminator Fake Loss: [0.42700687050819397, 0.875] | GAN_Loss: 1.2507460117340088\n",
      "Epoch: 637 | Discriminator Real Loss: [0.6860436201095581, 0.59375] | Discriminator Fake Loss: [0.5715625882148743, 0.6875] | GAN_Loss: 1.2101521492004395\n",
      "Epoch: 638 | Discriminator Real Loss: [0.5113904476165771, 0.78125] | Discriminator Fake Loss: [0.6648485064506531, 0.59375] | GAN_Loss: 0.9062443971633911\n",
      "Epoch: 639 | Discriminator Real Loss: [0.5400288701057434, 0.8125] | Discriminator Fake Loss: [0.685552716255188, 0.5] | GAN_Loss: 1.2554874420166016\n",
      "Epoch: 640 | Discriminator Real Loss: [0.6974413990974426, 0.5625] | Discriminator Fake Loss: [0.6570452451705933, 0.59375] | GAN_Loss: 1.243106484413147\n",
      "Epoch: 641 | Discriminator Real Loss: [0.6562479734420776, 0.6875] | Discriminator Fake Loss: [0.6297755241394043, 0.59375] | GAN_Loss: 1.1060583591461182\n",
      "Epoch: 642 | Discriminator Real Loss: [0.7602918148040771, 0.59375] | Discriminator Fake Loss: [0.6315567493438721, 0.625] | GAN_Loss: 1.3400458097457886\n",
      "Epoch: 643 | Discriminator Real Loss: [0.46567949652671814, 0.90625] | Discriminator Fake Loss: [0.7085795402526855, 0.5625] | GAN_Loss: 1.0472471714019775\n",
      "Epoch: 644 | Discriminator Real Loss: [0.5369176864624023, 0.8125] | Discriminator Fake Loss: [0.6716966032981873, 0.53125] | GAN_Loss: 1.4453716278076172\n",
      "Epoch: 645 | Discriminator Real Loss: [0.5489712357521057, 0.75] | Discriminator Fake Loss: [0.632413387298584, 0.53125] | GAN_Loss: 1.5432860851287842\n",
      "Epoch: 646 | Discriminator Real Loss: [0.5948394536972046, 0.6875] | Discriminator Fake Loss: [0.6007776260375977, 0.6875] | GAN_Loss: 1.3531274795532227\n",
      "Epoch: 647 | Discriminator Real Loss: [0.7633531093597412, 0.4375] | Discriminator Fake Loss: [0.5586637854576111, 0.65625] | GAN_Loss: 1.4190404415130615\n",
      "Epoch: 648 | Discriminator Real Loss: [0.5787874460220337, 0.6875] | Discriminator Fake Loss: [0.6017378568649292, 0.59375] | GAN_Loss: 1.2181103229522705\n",
      "Epoch: 649 | Discriminator Real Loss: [0.6552503705024719, 0.65625] | Discriminator Fake Loss: [0.609122097492218, 0.59375] | GAN_Loss: 1.361869215965271\n",
      "Epoch: 650 | Discriminator Real Loss: [0.5914521217346191, 0.84375] | Discriminator Fake Loss: [0.600635290145874, 0.59375] | GAN_Loss: 1.33860182762146\n",
      "Epoch: 651 | Discriminator Real Loss: [0.6431789398193359, 0.59375] | Discriminator Fake Loss: [0.5956593155860901, 0.53125] | GAN_Loss: 1.1374242305755615\n",
      "Epoch: 652 | Discriminator Real Loss: [0.5962721109390259, 0.71875] | Discriminator Fake Loss: [0.6320335865020752, 0.5625] | GAN_Loss: 1.1726620197296143\n",
      "Epoch: 653 | Discriminator Real Loss: [0.5342541337013245, 0.875] | Discriminator Fake Loss: [0.6042267084121704, 0.59375] | GAN_Loss: 1.206970453262329\n",
      "Epoch: 654 | Discriminator Real Loss: [0.5758496522903442, 0.71875] | Discriminator Fake Loss: [0.6876389980316162, 0.53125] | GAN_Loss: 1.275829553604126\n",
      "Epoch: 655 | Discriminator Real Loss: [0.5946202278137207, 0.71875] | Discriminator Fake Loss: [0.6528773307800293, 0.53125] | GAN_Loss: 1.2810343503952026\n",
      "Epoch: 656 | Discriminator Real Loss: [0.5630049705505371, 0.75] | Discriminator Fake Loss: [0.6484782099723816, 0.59375] | GAN_Loss: 1.1770906448364258\n",
      "Epoch: 657 | Discriminator Real Loss: [0.680635929107666, 0.78125] | Discriminator Fake Loss: [0.6269938945770264, 0.5625] | GAN_Loss: 1.4929125308990479\n",
      "Epoch: 658 | Discriminator Real Loss: [0.7806179523468018, 0.53125] | Discriminator Fake Loss: [0.6545829176902771, 0.6875] | GAN_Loss: 1.4013124704360962\n",
      "Epoch: 659 | Discriminator Real Loss: [0.6787004470825195, 0.59375] | Discriminator Fake Loss: [0.6768617033958435, 0.65625] | GAN_Loss: 0.9865608215332031\n",
      "Epoch: 660 | Discriminator Real Loss: [0.5726948380470276, 0.71875] | Discriminator Fake Loss: [0.6554690599441528, 0.5625] | GAN_Loss: 0.9136574268341064\n",
      "Epoch: 661 | Discriminator Real Loss: [0.508564293384552, 0.78125] | Discriminator Fake Loss: [0.7268270254135132, 0.375] | GAN_Loss: 1.0613123178482056\n",
      "Epoch: 662 | Discriminator Real Loss: [0.5046892166137695, 0.78125] | Discriminator Fake Loss: [0.7221418023109436, 0.5] | GAN_Loss: 1.0149376392364502\n",
      "Epoch: 663 | Discriminator Real Loss: [0.6066198348999023, 0.65625] | Discriminator Fake Loss: [0.6399062871932983, 0.5] | GAN_Loss: 1.309425711631775\n",
      "Epoch: 664 | Discriminator Real Loss: [0.6483584642410278, 0.59375] | Discriminator Fake Loss: [0.6118495464324951, 0.5625] | GAN_Loss: 1.0652320384979248\n",
      "Epoch: 665 | Discriminator Real Loss: [0.6281256079673767, 0.625] | Discriminator Fake Loss: [0.6682692766189575, 0.5625] | GAN_Loss: 1.0672215223312378\n",
      "Epoch: 666 | Discriminator Real Loss: [0.7369275093078613, 0.53125] | Discriminator Fake Loss: [0.628655731678009, 0.6875] | GAN_Loss: 1.025899887084961\n",
      "Epoch: 667 | Discriminator Real Loss: [0.6388080716133118, 0.625] | Discriminator Fake Loss: [0.7592235803604126, 0.4375] | GAN_Loss: 1.0713999271392822\n",
      "Epoch: 668 | Discriminator Real Loss: [0.6028285622596741, 0.6875] | Discriminator Fake Loss: [0.6913706064224243, 0.5625] | GAN_Loss: 0.9474321603775024\n",
      "Epoch: 669 | Discriminator Real Loss: [0.4962058365345001, 0.8125] | Discriminator Fake Loss: [0.7245537638664246, 0.46875] | GAN_Loss: 0.8931642174720764\n",
      "Epoch: 670 | Discriminator Real Loss: [0.675858199596405, 0.59375] | Discriminator Fake Loss: [0.6077554225921631, 0.59375] | GAN_Loss: 1.0918134450912476\n",
      "Epoch: 671 | Discriminator Real Loss: [0.6234224438667297, 0.65625] | Discriminator Fake Loss: [0.7393289804458618, 0.4375] | GAN_Loss: 1.0008001327514648\n",
      "Epoch: 672 | Discriminator Real Loss: [0.6187283992767334, 0.8125] | Discriminator Fake Loss: [0.6703307032585144, 0.5625] | GAN_Loss: 0.9813469052314758\n",
      "Epoch: 673 | Discriminator Real Loss: [0.5826579332351685, 0.75] | Discriminator Fake Loss: [0.5627204775810242, 0.6875] | GAN_Loss: 1.0160928964614868\n",
      "Epoch: 674 | Discriminator Real Loss: [0.4970256984233856, 0.875] | Discriminator Fake Loss: [0.7076719999313354, 0.625] | GAN_Loss: 0.8471450805664062\n",
      "Epoch: 675 | Discriminator Real Loss: [0.5252542495727539, 0.75] | Discriminator Fake Loss: [0.7372767329216003, 0.5625] | GAN_Loss: 0.9680570363998413\n",
      "Epoch: 676 | Discriminator Real Loss: [0.6029483079910278, 0.65625] | Discriminator Fake Loss: [0.7947677373886108, 0.53125] | GAN_Loss: 1.178710699081421\n",
      "Epoch: 677 | Discriminator Real Loss: [0.6783892512321472, 0.6875] | Discriminator Fake Loss: [0.5935390591621399, 0.59375] | GAN_Loss: 1.2524594068527222\n",
      "Epoch: 678 | Discriminator Real Loss: [0.8351271152496338, 0.5] | Discriminator Fake Loss: [0.47369712591171265, 0.8125] | GAN_Loss: 1.0767483711242676\n",
      "Epoch: 679 | Discriminator Real Loss: [0.7523446679115295, 0.59375] | Discriminator Fake Loss: [0.48986563086509705, 0.78125] | GAN_Loss: 0.951468288898468\n",
      "Epoch: 680 | Discriminator Real Loss: [0.5272496938705444, 0.84375] | Discriminator Fake Loss: [0.8295387625694275, 0.40625] | GAN_Loss: 0.83213210105896\n",
      "Epoch: 681 | Discriminator Real Loss: [0.49891674518585205, 0.84375] | Discriminator Fake Loss: [0.8126706480979919, 0.40625] | GAN_Loss: 0.8466732501983643\n",
      "Epoch: 682 | Discriminator Real Loss: [0.5211505889892578, 0.875] | Discriminator Fake Loss: [0.721733570098877, 0.5] | GAN_Loss: 0.9075148701667786\n",
      "Epoch: 683 | Discriminator Real Loss: [0.6622250080108643, 0.6875] | Discriminator Fake Loss: [0.6287739276885986, 0.59375] | GAN_Loss: 0.9851365089416504\n",
      "Epoch: 684 | Discriminator Real Loss: [0.6411452293395996, 0.65625] | Discriminator Fake Loss: [0.6145128011703491, 0.59375] | GAN_Loss: 0.9988886117935181\n",
      "Epoch: 685 | Discriminator Real Loss: [0.6236141920089722, 0.65625] | Discriminator Fake Loss: [0.6711112260818481, 0.53125] | GAN_Loss: 1.0273168087005615\n",
      "Epoch: 686 | Discriminator Real Loss: [0.5584644079208374, 0.75] | Discriminator Fake Loss: [0.559822678565979, 0.71875] | GAN_Loss: 1.1017966270446777\n",
      "Epoch: 687 | Discriminator Real Loss: [0.7754244804382324, 0.53125] | Discriminator Fake Loss: [0.5823754072189331, 0.6875] | GAN_Loss: 1.0473229885101318\n",
      "Epoch: 688 | Discriminator Real Loss: [0.6647924184799194, 0.625] | Discriminator Fake Loss: [0.6307663917541504, 0.59375] | GAN_Loss: 1.092118740081787\n",
      "Epoch: 689 | Discriminator Real Loss: [0.6103853583335876, 0.6875] | Discriminator Fake Loss: [0.6720982193946838, 0.59375] | GAN_Loss: 1.3992162942886353\n",
      "Epoch: 690 | Discriminator Real Loss: [0.6244354248046875, 0.65625] | Discriminator Fake Loss: [0.7039058208465576, 0.5] | GAN_Loss: 1.0592955350875854\n",
      "Epoch: 691 | Discriminator Real Loss: [0.6096582412719727, 0.625] | Discriminator Fake Loss: [0.7630677819252014, 0.40625] | GAN_Loss: 0.9012799263000488\n",
      "Epoch: 692 | Discriminator Real Loss: [0.6506028175354004, 0.75] | Discriminator Fake Loss: [0.5982643365859985, 0.5625] | GAN_Loss: 0.9810261726379395\n",
      "Epoch: 693 | Discriminator Real Loss: [0.5751914978027344, 0.875] | Discriminator Fake Loss: [0.8360487222671509, 0.40625] | GAN_Loss: 0.7584748268127441\n",
      "Epoch: 694 | Discriminator Real Loss: [0.580431342124939, 0.78125] | Discriminator Fake Loss: [0.8124430179595947, 0.28125] | GAN_Loss: 0.9552462100982666\n",
      "Epoch: 695 | Discriminator Real Loss: [0.464507520198822, 0.90625] | Discriminator Fake Loss: [0.7923649549484253, 0.375] | GAN_Loss: 0.8046796321868896\n",
      "Epoch: 696 | Discriminator Real Loss: [0.4800313711166382, 0.875] | Discriminator Fake Loss: [0.9266698360443115, 0.28125] | GAN_Loss: 0.958667516708374\n",
      "Epoch: 697 | Discriminator Real Loss: [0.6160558462142944, 0.75] | Discriminator Fake Loss: [0.7840126752853394, 0.5] | GAN_Loss: 0.9430906772613525\n",
      "Epoch: 698 | Discriminator Real Loss: [0.6676152944564819, 0.625] | Discriminator Fake Loss: [0.7348372340202332, 0.53125] | GAN_Loss: 0.9307056665420532\n",
      "Epoch: 699 | Discriminator Real Loss: [0.7883756160736084, 0.40625] | Discriminator Fake Loss: [0.5532258749008179, 0.75] | GAN_Loss: 1.078493356704712\n",
      "Epoch: 700 | Discriminator Real Loss: [0.8331621885299683, 0.28125] | Discriminator Fake Loss: [0.6120363473892212, 0.59375] | GAN_Loss: 0.9388756155967712\n",
      "Epoch: 701 | Discriminator Real Loss: [0.6548177003860474, 0.65625] | Discriminator Fake Loss: [0.6597087383270264, 0.53125] | GAN_Loss: 0.8419190645217896\n",
      "Epoch: 702 | Discriminator Real Loss: [0.6494606733322144, 0.59375] | Discriminator Fake Loss: [0.5852488279342651, 0.71875] | GAN_Loss: 0.7921698689460754\n",
      "Epoch: 703 | Discriminator Real Loss: [0.6801612973213196, 0.6875] | Discriminator Fake Loss: [0.6311880946159363, 0.5] | GAN_Loss: 0.9215731620788574\n",
      "Epoch: 704 | Discriminator Real Loss: [0.5873304009437561, 0.8125] | Discriminator Fake Loss: [0.6909485459327698, 0.4375] | GAN_Loss: 0.8660596609115601\n",
      "Epoch: 705 | Discriminator Real Loss: [0.5699023008346558, 0.8125] | Discriminator Fake Loss: [0.5869607925415039, 0.625] | GAN_Loss: 1.1532598733901978\n",
      "Epoch: 706 | Discriminator Real Loss: [0.5982335805892944, 0.8125] | Discriminator Fake Loss: [0.6339837908744812, 0.625] | GAN_Loss: 0.8680037260055542\n",
      "Epoch: 707 | Discriminator Real Loss: [0.6358292698860168, 0.65625] | Discriminator Fake Loss: [0.6139681339263916, 0.625] | GAN_Loss: 1.2464529275894165\n",
      "Epoch: 708 | Discriminator Real Loss: [0.7656630277633667, 0.59375] | Discriminator Fake Loss: [0.5993988513946533, 0.71875] | GAN_Loss: 1.0787663459777832\n",
      "Epoch: 709 | Discriminator Real Loss: [0.6837891936302185, 0.625] | Discriminator Fake Loss: [0.5946162939071655, 0.65625] | GAN_Loss: 0.9971095323562622\n",
      "Epoch: 710 | Discriminator Real Loss: [0.6578383445739746, 0.625] | Discriminator Fake Loss: [0.5739649534225464, 0.65625] | GAN_Loss: 1.1012321710586548\n",
      "Epoch: 711 | Discriminator Real Loss: [0.6319094896316528, 0.71875] | Discriminator Fake Loss: [0.529984712600708, 0.78125] | GAN_Loss: 1.137924313545227\n",
      "Epoch: 712 | Discriminator Real Loss: [0.6355812549591064, 0.75] | Discriminator Fake Loss: [0.646216094493866, 0.59375] | GAN_Loss: 1.015252709388733\n",
      "Epoch: 713 | Discriminator Real Loss: [0.7664041519165039, 0.4375] | Discriminator Fake Loss: [0.5899342894554138, 0.71875] | GAN_Loss: 1.0989105701446533\n",
      "Epoch: 714 | Discriminator Real Loss: [0.683025062084198, 0.59375] | Discriminator Fake Loss: [0.7244254350662231, 0.46875] | GAN_Loss: 0.9822845458984375\n",
      "Epoch: 715 | Discriminator Real Loss: [0.5410457253456116, 0.875] | Discriminator Fake Loss: [0.5880107283592224, 0.59375] | GAN_Loss: 1.0349881649017334\n",
      "Epoch: 716 | Discriminator Real Loss: [0.5780567526817322, 0.78125] | Discriminator Fake Loss: [0.6748365163803101, 0.5625] | GAN_Loss: 0.9914989471435547\n",
      "Epoch: 717 | Discriminator Real Loss: [0.5611525177955627, 0.875] | Discriminator Fake Loss: [0.62835693359375, 0.65625] | GAN_Loss: 1.1131892204284668\n",
      "Epoch: 718 | Discriminator Real Loss: [0.6197835803031921, 0.65625] | Discriminator Fake Loss: [0.7502749562263489, 0.5625] | GAN_Loss: 0.9009618759155273\n",
      "Epoch: 719 | Discriminator Real Loss: [0.527869462966919, 0.78125] | Discriminator Fake Loss: [0.6098084449768066, 0.625] | GAN_Loss: 0.9568178057670593\n",
      "Epoch: 720 | Discriminator Real Loss: [0.6221315264701843, 0.75] | Discriminator Fake Loss: [0.6035366058349609, 0.625] | GAN_Loss: 0.9455860257148743\n",
      "Epoch: 721 | Discriminator Real Loss: [0.654431164264679, 0.71875] | Discriminator Fake Loss: [0.6477759480476379, 0.5625] | GAN_Loss: 1.0362309217453003\n",
      "Epoch: 722 | Discriminator Real Loss: [0.5861247777938843, 0.8125] | Discriminator Fake Loss: [0.7281938791275024, 0.375] | GAN_Loss: 0.7144178152084351\n",
      "Epoch: 723 | Discriminator Real Loss: [0.5932403802871704, 0.65625] | Discriminator Fake Loss: [0.7753089666366577, 0.53125] | GAN_Loss: 1.004981279373169\n",
      "Epoch: 724 | Discriminator Real Loss: [0.5435810685157776, 0.84375] | Discriminator Fake Loss: [0.7212511301040649, 0.5] | GAN_Loss: 0.7540886998176575\n",
      "Epoch: 725 | Discriminator Real Loss: [0.5895287394523621, 0.6875] | Discriminator Fake Loss: [0.7148700952529907, 0.46875] | GAN_Loss: 0.8806459903717041\n",
      "Epoch: 726 | Discriminator Real Loss: [0.5963379144668579, 0.78125] | Discriminator Fake Loss: [0.7898502349853516, 0.34375] | GAN_Loss: 0.9859784841537476\n",
      "Epoch: 727 | Discriminator Real Loss: [0.8291295766830444, 0.5] | Discriminator Fake Loss: [0.6194039583206177, 0.625] | GAN_Loss: 1.0172364711761475\n",
      "Epoch: 728 | Discriminator Real Loss: [0.663519024848938, 0.625] | Discriminator Fake Loss: [0.6384785771369934, 0.6875] | GAN_Loss: 0.938055694103241\n",
      "Epoch: 729 | Discriminator Real Loss: [0.7672913670539856, 0.53125] | Discriminator Fake Loss: [0.6788872480392456, 0.5] | GAN_Loss: 0.855808675289154\n",
      "Epoch: 730 | Discriminator Real Loss: [0.7040796279907227, 0.59375] | Discriminator Fake Loss: [0.6322381496429443, 0.5] | GAN_Loss: 0.8867409825325012\n",
      "Epoch: 731 | Discriminator Real Loss: [0.5727813243865967, 0.75] | Discriminator Fake Loss: [0.7157884836196899, 0.375] | GAN_Loss: 0.9563778638839722\n",
      "Epoch: 732 | Discriminator Real Loss: [0.5117360353469849, 0.84375] | Discriminator Fake Loss: [0.8304214477539062, 0.28125] | GAN_Loss: 0.8120093941688538\n",
      "Epoch: 733 | Discriminator Real Loss: [0.49485066533088684, 0.9375] | Discriminator Fake Loss: [0.7884100079536438, 0.28125] | GAN_Loss: 0.88201504945755\n",
      "Epoch: 734 | Discriminator Real Loss: [0.5608514547348022, 0.84375] | Discriminator Fake Loss: [0.6797521710395813, 0.5] | GAN_Loss: 0.8810024261474609\n",
      "Epoch: 735 | Discriminator Real Loss: [0.744701623916626, 0.53125] | Discriminator Fake Loss: [0.6338707804679871, 0.6875] | GAN_Loss: 0.9041564464569092\n",
      "Epoch: 736 | Discriminator Real Loss: [0.6716179847717285, 0.59375] | Discriminator Fake Loss: [0.6368032693862915, 0.5625] | GAN_Loss: 1.0175786018371582\n",
      "Epoch: 737 | Discriminator Real Loss: [0.6515838503837585, 0.65625] | Discriminator Fake Loss: [0.5913233757019043, 0.59375] | GAN_Loss: 0.9298091530799866\n",
      "Epoch: 738 | Discriminator Real Loss: [0.7482826709747314, 0.59375] | Discriminator Fake Loss: [0.6575629711151123, 0.5] | GAN_Loss: 0.9455245733261108\n",
      "Epoch: 739 | Discriminator Real Loss: [0.6094484329223633, 0.75] | Discriminator Fake Loss: [0.6757371425628662, 0.46875] | GAN_Loss: 0.9673720002174377\n",
      "Epoch: 740 | Discriminator Real Loss: [0.7011706233024597, 0.59375] | Discriminator Fake Loss: [0.7014833688735962, 0.4375] | GAN_Loss: 1.136664628982544\n",
      "Epoch: 741 | Discriminator Real Loss: [0.5739047527313232, 0.6875] | Discriminator Fake Loss: [0.6290105581283569, 0.59375] | GAN_Loss: 0.924507200717926\n",
      "Epoch: 742 | Discriminator Real Loss: [0.5930941700935364, 0.6875] | Discriminator Fake Loss: [0.7000048756599426, 0.46875] | GAN_Loss: 0.8316893577575684\n",
      "Epoch: 743 | Discriminator Real Loss: [0.5566543340682983, 0.78125] | Discriminator Fake Loss: [0.6936612129211426, 0.5] | GAN_Loss: 0.8514600992202759\n",
      "Epoch: 744 | Discriminator Real Loss: [0.5756373405456543, 0.71875] | Discriminator Fake Loss: [0.7809158563613892, 0.4375] | GAN_Loss: 0.8532600402832031\n",
      "Epoch: 745 | Discriminator Real Loss: [0.6445678472518921, 0.65625] | Discriminator Fake Loss: [0.6880318522453308, 0.375] | GAN_Loss: 0.8814295530319214\n",
      "Epoch: 746 | Discriminator Real Loss: [0.6630692481994629, 0.625] | Discriminator Fake Loss: [0.6506074666976929, 0.5625] | GAN_Loss: 1.007550597190857\n",
      "Epoch: 747 | Discriminator Real Loss: [0.6422755718231201, 0.65625] | Discriminator Fake Loss: [0.5999612808227539, 0.75] | GAN_Loss: 0.9361193776130676\n",
      "Epoch: 748 | Discriminator Real Loss: [0.7587379217147827, 0.40625] | Discriminator Fake Loss: [0.6204264760017395, 0.65625] | GAN_Loss: 0.9781110882759094\n",
      "Epoch: 749 | Discriminator Real Loss: [0.7504696846008301, 0.4375] | Discriminator Fake Loss: [0.6075546741485596, 0.71875] | GAN_Loss: 0.9212644100189209\n",
      "Epoch: 750 | Discriminator Real Loss: [0.7113553285598755, 0.5625] | Discriminator Fake Loss: [0.6746017336845398, 0.53125] | GAN_Loss: 0.8203271627426147\n",
      "Epoch: 751 | Discriminator Real Loss: [0.6539473533630371, 0.625] | Discriminator Fake Loss: [0.6561791896820068, 0.5625] | GAN_Loss: 0.7129745483398438\n",
      "Epoch: 752 | Discriminator Real Loss: [0.5652333498001099, 0.75] | Discriminator Fake Loss: [0.656971275806427, 0.65625] | GAN_Loss: 0.735945463180542\n",
      "Epoch: 753 | Discriminator Real Loss: [0.5365302562713623, 0.8125] | Discriminator Fake Loss: [0.8660179972648621, 0.28125] | GAN_Loss: 0.8031768798828125\n",
      "Epoch: 754 | Discriminator Real Loss: [0.5589441061019897, 0.6875] | Discriminator Fake Loss: [0.7791618704795837, 0.40625] | GAN_Loss: 0.8443977236747742\n",
      "Epoch: 755 | Discriminator Real Loss: [0.6510843634605408, 0.625] | Discriminator Fake Loss: [0.554820716381073, 0.71875] | GAN_Loss: 1.0542171001434326\n",
      "Epoch: 756 | Discriminator Real Loss: [0.7460047602653503, 0.5] | Discriminator Fake Loss: [0.6489690542221069, 0.59375] | GAN_Loss: 0.8763724565505981\n",
      "Epoch: 757 | Discriminator Real Loss: [0.6685119271278381, 0.46875] | Discriminator Fake Loss: [0.7178786993026733, 0.40625] | GAN_Loss: 0.7711782455444336\n",
      "Epoch: 758 | Discriminator Real Loss: [0.7336492538452148, 0.53125] | Discriminator Fake Loss: [0.754463255405426, 0.5] | GAN_Loss: 0.7876807451248169\n",
      "Epoch: 759 | Discriminator Real Loss: [0.6534295678138733, 0.65625] | Discriminator Fake Loss: [0.7621620893478394, 0.34375] | GAN_Loss: 0.7258479595184326\n",
      "Epoch: 760 | Discriminator Real Loss: [0.6860944032669067, 0.5625] | Discriminator Fake Loss: [0.7561123371124268, 0.46875] | GAN_Loss: 0.8152334690093994\n",
      "Epoch: 761 | Discriminator Real Loss: [0.7351315021514893, 0.5] | Discriminator Fake Loss: [0.6784814596176147, 0.5] | GAN_Loss: 0.8936569690704346\n",
      "Epoch: 762 | Discriminator Real Loss: [0.6816384792327881, 0.59375] | Discriminator Fake Loss: [0.6837853789329529, 0.5625] | GAN_Loss: 0.8173204064369202\n",
      "Epoch: 763 | Discriminator Real Loss: [0.5934711694717407, 0.71875] | Discriminator Fake Loss: [0.7594404816627502, 0.34375] | GAN_Loss: 0.7043361663818359\n",
      "Epoch: 764 | Discriminator Real Loss: [0.6592463850975037, 0.5625] | Discriminator Fake Loss: [0.6677663326263428, 0.5] | GAN_Loss: 0.7727180123329163\n",
      "Epoch: 765 | Discriminator Real Loss: [0.7213687300682068, 0.4375] | Discriminator Fake Loss: [0.6505210399627686, 0.53125] | GAN_Loss: 0.7509744763374329\n",
      "Epoch: 766 | Discriminator Real Loss: [0.756758987903595, 0.53125] | Discriminator Fake Loss: [0.7110930681228638, 0.5] | GAN_Loss: 0.7068688273429871\n",
      "Epoch: 767 | Discriminator Real Loss: [0.6132321953773499, 0.6875] | Discriminator Fake Loss: [0.8412137627601624, 0.21875] | GAN_Loss: 0.6481812000274658\n",
      "Epoch: 768 | Discriminator Real Loss: [0.5297030210494995, 0.90625] | Discriminator Fake Loss: [0.8171485662460327, 0.25] | GAN_Loss: 0.6725292801856995\n",
      "Epoch: 769 | Discriminator Real Loss: [0.5281936526298523, 0.875] | Discriminator Fake Loss: [0.8248277902603149, 0.25] | GAN_Loss: 0.6681035757064819\n",
      "Epoch: 770 | Discriminator Real Loss: [0.5664138197898865, 0.84375] | Discriminator Fake Loss: [0.7496358156204224, 0.28125] | GAN_Loss: 0.7486714124679565\n",
      "Epoch: 771 | Discriminator Real Loss: [0.5988500118255615, 0.8125] | Discriminator Fake Loss: [0.6901285648345947, 0.40625] | GAN_Loss: 0.7625316381454468\n",
      "Epoch: 772 | Discriminator Real Loss: [0.6536989808082581, 0.6875] | Discriminator Fake Loss: [0.6696252822875977, 0.59375] | GAN_Loss: 0.8082743883132935\n",
      "Epoch: 773 | Discriminator Real Loss: [0.7534916400909424, 0.46875] | Discriminator Fake Loss: [0.548136293888092, 0.8125] | GAN_Loss: 1.0131728649139404\n",
      "Epoch: 774 | Discriminator Real Loss: [0.8144590258598328, 0.21875] | Discriminator Fake Loss: [0.615537166595459, 0.75] | GAN_Loss: 1.092289924621582\n",
      "Epoch: 775 | Discriminator Real Loss: [0.807222306728363, 0.25] | Discriminator Fake Loss: [0.485928475856781, 0.90625] | GAN_Loss: 1.0391902923583984\n",
      "Epoch: 776 | Discriminator Real Loss: [0.7692503929138184, 0.25] | Discriminator Fake Loss: [0.5456811189651489, 0.84375] | GAN_Loss: 1.046104907989502\n",
      "Epoch: 777 | Discriminator Real Loss: [0.7295005321502686, 0.5625] | Discriminator Fake Loss: [0.5248079299926758, 0.8125] | GAN_Loss: 1.013283133506775\n",
      "Epoch: 778 | Discriminator Real Loss: [0.7626140713691711, 0.40625] | Discriminator Fake Loss: [0.516606867313385, 0.8125] | GAN_Loss: 1.054822325706482\n",
      "Epoch: 779 | Discriminator Real Loss: [0.7629145383834839, 0.4375] | Discriminator Fake Loss: [0.5898468494415283, 0.78125] | GAN_Loss: 0.902756929397583\n",
      "Epoch: 780 | Discriminator Real Loss: [0.7251174449920654, 0.65625] | Discriminator Fake Loss: [0.6178791522979736, 0.625] | GAN_Loss: 0.8566305041313171\n",
      "Epoch: 781 | Discriminator Real Loss: [0.7759767174720764, 0.40625] | Discriminator Fake Loss: [0.6962698698043823, 0.5625] | GAN_Loss: 0.9686346650123596\n",
      "Epoch: 782 | Discriminator Real Loss: [0.6534268856048584, 0.65625] | Discriminator Fake Loss: [0.7278262376785278, 0.46875] | GAN_Loss: 0.7035599946975708\n",
      "Epoch: 783 | Discriminator Real Loss: [0.597793698310852, 0.8125] | Discriminator Fake Loss: [0.7056092023849487, 0.5] | GAN_Loss: 0.8603872656822205\n",
      "Epoch: 784 | Discriminator Real Loss: [0.6087157726287842, 0.75] | Discriminator Fake Loss: [0.7880035638809204, 0.28125] | GAN_Loss: 0.6852296590805054\n",
      "Epoch: 785 | Discriminator Real Loss: [0.6324723958969116, 0.71875] | Discriminator Fake Loss: [0.679762065410614, 0.46875] | GAN_Loss: 0.7770751714706421\n",
      "Epoch: 786 | Discriminator Real Loss: [0.6370633840560913, 0.625] | Discriminator Fake Loss: [0.5415183305740356, 0.78125] | GAN_Loss: 0.9699329137802124\n",
      "Epoch: 787 | Discriminator Real Loss: [0.7001297473907471, 0.625] | Discriminator Fake Loss: [0.5890090465545654, 0.6875] | GAN_Loss: 0.930963397026062\n",
      "Epoch: 788 | Discriminator Real Loss: [0.8384572267532349, 0.3125] | Discriminator Fake Loss: [0.45006081461906433, 0.8125] | GAN_Loss: 1.1782503128051758\n",
      "Epoch: 789 | Discriminator Real Loss: [0.8337579965591431, 0.34375] | Discriminator Fake Loss: [0.6571059226989746, 0.5] | GAN_Loss: 0.8369580507278442\n",
      "Epoch: 790 | Discriminator Real Loss: [0.693474531173706, 0.625] | Discriminator Fake Loss: [0.773227870464325, 0.46875] | GAN_Loss: 0.730618953704834\n",
      "Epoch: 791 | Discriminator Real Loss: [0.5817129015922546, 0.8125] | Discriminator Fake Loss: [0.8017865419387817, 0.1875] | GAN_Loss: 0.6122561693191528\n",
      "Epoch: 792 | Discriminator Real Loss: [0.536718487739563, 0.84375] | Discriminator Fake Loss: [0.7710728645324707, 0.28125] | GAN_Loss: 0.6455768942832947\n",
      "Epoch: 793 | Discriminator Real Loss: [0.5796031355857849, 0.84375] | Discriminator Fake Loss: [0.8267211318016052, 0.25] | GAN_Loss: 0.7315777540206909\n",
      "Epoch: 794 | Discriminator Real Loss: [0.646468997001648, 0.6875] | Discriminator Fake Loss: [0.790789008140564, 0.34375] | GAN_Loss: 0.7112785577774048\n",
      "Epoch: 795 | Discriminator Real Loss: [0.647269606590271, 0.75] | Discriminator Fake Loss: [0.7433532476425171, 0.34375] | GAN_Loss: 0.7929818630218506\n",
      "Epoch: 796 | Discriminator Real Loss: [0.6873918771743774, 0.5] | Discriminator Fake Loss: [0.7061611413955688, 0.40625] | GAN_Loss: 0.7449600696563721\n",
      "Epoch: 797 | Discriminator Real Loss: [0.722474217414856, 0.5625] | Discriminator Fake Loss: [0.7319201231002808, 0.34375] | GAN_Loss: 0.7400819063186646\n",
      "Epoch: 798 | Discriminator Real Loss: [0.6376925706863403, 0.65625] | Discriminator Fake Loss: [0.6756423711776733, 0.53125] | GAN_Loss: 0.7680931091308594\n",
      "Epoch: 799 | Discriminator Real Loss: [0.6742130517959595, 0.5625] | Discriminator Fake Loss: [0.6974714994430542, 0.375] | GAN_Loss: 0.7120026350021362\n",
      "Epoch: 800 | Discriminator Real Loss: [0.7324739098548889, 0.4375] | Discriminator Fake Loss: [0.6622545123100281, 0.53125] | GAN_Loss: 0.7555969953536987\n",
      "Epoch: 801 | Discriminator Real Loss: [0.7441275119781494, 0.53125] | Discriminator Fake Loss: [0.7403416037559509, 0.375] | GAN_Loss: 0.7646024823188782\n",
      "Epoch: 802 | Discriminator Real Loss: [0.6728920936584473, 0.59375] | Discriminator Fake Loss: [0.6941655278205872, 0.53125] | GAN_Loss: 0.7308511734008789\n",
      "Epoch: 803 | Discriminator Real Loss: [0.6363236904144287, 0.6875] | Discriminator Fake Loss: [0.7095699906349182, 0.3125] | GAN_Loss: 0.7222685813903809\n",
      "Epoch: 804 | Discriminator Real Loss: [0.646106481552124, 0.71875] | Discriminator Fake Loss: [0.737748384475708, 0.3125] | GAN_Loss: 0.7079970240592957\n",
      "Epoch: 805 | Discriminator Real Loss: [0.6588214039802551, 0.65625] | Discriminator Fake Loss: [0.745491623878479, 0.40625] | GAN_Loss: 0.7238329648971558\n",
      "Epoch: 806 | Discriminator Real Loss: [0.6808708906173706, 0.65625] | Discriminator Fake Loss: [0.6577368974685669, 0.46875] | GAN_Loss: 0.7198337912559509\n",
      "Epoch: 807 | Discriminator Real Loss: [0.6926307082176208, 0.5] | Discriminator Fake Loss: [0.6772330403327942, 0.4375] | GAN_Loss: 0.7299386858940125\n",
      "Epoch: 808 | Discriminator Real Loss: [0.6387283205986023, 0.75] | Discriminator Fake Loss: [0.6952953338623047, 0.4375] | GAN_Loss: 0.7493473887443542\n",
      "Epoch: 809 | Discriminator Real Loss: [0.6498426198959351, 0.71875] | Discriminator Fake Loss: [0.7431867718696594, 0.34375] | GAN_Loss: 0.7740082740783691\n",
      "Epoch: 810 | Discriminator Real Loss: [0.6280137896537781, 0.75] | Discriminator Fake Loss: [0.6598173975944519, 0.625] | GAN_Loss: 0.7115780115127563\n",
      "Epoch: 811 | Discriminator Real Loss: [0.652611255645752, 0.65625] | Discriminator Fake Loss: [0.7058433294296265, 0.375] | GAN_Loss: 0.779299795627594\n",
      "Epoch: 812 | Discriminator Real Loss: [0.7006711363792419, 0.53125] | Discriminator Fake Loss: [0.6909856796264648, 0.5625] | GAN_Loss: 0.7465594410896301\n",
      "Epoch: 813 | Discriminator Real Loss: [0.6772447824478149, 0.65625] | Discriminator Fake Loss: [0.6648725271224976, 0.59375] | GAN_Loss: 0.7801662683486938\n",
      "Epoch: 814 | Discriminator Real Loss: [0.7269734144210815, 0.5] | Discriminator Fake Loss: [0.6556829214096069, 0.65625] | GAN_Loss: 0.7096191644668579\n",
      "Epoch: 815 | Discriminator Real Loss: [0.6421349048614502, 0.6875] | Discriminator Fake Loss: [0.672692060470581, 0.46875] | GAN_Loss: 0.7349321246147156\n",
      "Epoch: 816 | Discriminator Real Loss: [0.6438725590705872, 0.71875] | Discriminator Fake Loss: [0.6800388693809509, 0.53125] | GAN_Loss: 0.9735360145568848\n",
      "Epoch: 817 | Discriminator Real Loss: [0.6885437369346619, 0.6875] | Discriminator Fake Loss: [0.5623644590377808, 0.71875] | GAN_Loss: 1.1000643968582153\n",
      "Epoch: 818 | Discriminator Real Loss: [0.736883819103241, 0.5] | Discriminator Fake Loss: [0.49914905428886414, 0.78125] | GAN_Loss: 1.107706069946289\n",
      "Epoch: 819 | Discriminator Real Loss: [0.7769746780395508, 0.46875] | Discriminator Fake Loss: [0.505994975566864, 0.8125] | GAN_Loss: 1.1146752834320068\n",
      "Epoch: 820 | Discriminator Real Loss: [0.7643294930458069, 0.375] | Discriminator Fake Loss: [0.5396549105644226, 0.78125] | GAN_Loss: 1.0286673307418823\n",
      "Epoch: 821 | Discriminator Real Loss: [0.7031804323196411, 0.4375] | Discriminator Fake Loss: [0.5424944162368774, 0.78125] | GAN_Loss: 0.8965113759040833\n",
      "Epoch: 822 | Discriminator Real Loss: [0.758426308631897, 0.53125] | Discriminator Fake Loss: [0.6591960787773132, 0.625] | GAN_Loss: 0.9412810802459717\n",
      "Epoch: 823 | Discriminator Real Loss: [0.6325595378875732, 0.71875] | Discriminator Fake Loss: [0.6110299229621887, 0.65625] | GAN_Loss: 0.8884012699127197\n",
      "Epoch: 824 | Discriminator Real Loss: [0.5933855175971985, 0.78125] | Discriminator Fake Loss: [0.7105399370193481, 0.46875] | GAN_Loss: 0.8411774635314941\n",
      "Epoch: 825 | Discriminator Real Loss: [0.6087638735771179, 0.6875] | Discriminator Fake Loss: [0.7138029336929321, 0.5625] | GAN_Loss: 0.8633408546447754\n",
      "Epoch: 826 | Discriminator Real Loss: [0.6877341866493225, 0.5625] | Discriminator Fake Loss: [0.6128175258636475, 0.71875] | GAN_Loss: 0.8789504170417786\n",
      "Epoch: 827 | Discriminator Real Loss: [0.7163973450660706, 0.65625] | Discriminator Fake Loss: [0.6465378999710083, 0.5625] | GAN_Loss: 0.835577130317688\n",
      "Epoch: 828 | Discriminator Real Loss: [0.6782968640327454, 0.65625] | Discriminator Fake Loss: [0.6317511200904846, 0.625] | GAN_Loss: 0.8640713691711426\n",
      "Epoch: 829 | Discriminator Real Loss: [0.6195909976959229, 0.78125] | Discriminator Fake Loss: [0.605751097202301, 0.625] | GAN_Loss: 0.8321173191070557\n",
      "Epoch: 830 | Discriminator Real Loss: [0.7725697755813599, 0.375] | Discriminator Fake Loss: [0.6324589848518372, 0.71875] | GAN_Loss: 0.8149723410606384\n",
      "Epoch: 831 | Discriminator Real Loss: [0.6593090295791626, 0.65625] | Discriminator Fake Loss: [0.7193747162818909, 0.5] | GAN_Loss: 0.7083559036254883\n",
      "Epoch: 832 | Discriminator Real Loss: [0.6511980891227722, 0.625] | Discriminator Fake Loss: [0.7941303849220276, 0.3125] | GAN_Loss: 0.7673947811126709\n",
      "Epoch: 833 | Discriminator Real Loss: [0.5325384140014648, 0.9375] | Discriminator Fake Loss: [0.7898473739624023, 0.34375] | GAN_Loss: 0.6377649307250977\n",
      "Epoch: 834 | Discriminator Real Loss: [0.5565561056137085, 0.875] | Discriminator Fake Loss: [0.8125377297401428, 0.1875] | GAN_Loss: 0.7990866899490356\n",
      "Epoch: 835 | Discriminator Real Loss: [0.629413366317749, 0.75] | Discriminator Fake Loss: [0.7335183620452881, 0.3125] | GAN_Loss: 0.8137145042419434\n",
      "Epoch: 836 | Discriminator Real Loss: [0.7042073011398315, 0.59375] | Discriminator Fake Loss: [0.6964989900588989, 0.40625] | GAN_Loss: 0.7995960116386414\n",
      "Epoch: 837 | Discriminator Real Loss: [0.7304988503456116, 0.625] | Discriminator Fake Loss: [0.7115688323974609, 0.46875] | GAN_Loss: 0.728145956993103\n",
      "Epoch: 838 | Discriminator Real Loss: [0.6694169044494629, 0.65625] | Discriminator Fake Loss: [0.70429527759552, 0.5] | GAN_Loss: 0.7224187850952148\n",
      "Epoch: 839 | Discriminator Real Loss: [0.680304229259491, 0.59375] | Discriminator Fake Loss: [0.653739333152771, 0.6875] | GAN_Loss: 0.8769058585166931\n",
      "Epoch: 840 | Discriminator Real Loss: [0.6338720321655273, 0.8125] | Discriminator Fake Loss: [0.5876971483230591, 0.65625] | GAN_Loss: 0.9158600568771362\n",
      "Epoch: 841 | Discriminator Real Loss: [0.6728466153144836, 0.625] | Discriminator Fake Loss: [0.6849866509437561, 0.40625] | GAN_Loss: 0.8020074367523193\n",
      "Epoch: 842 | Discriminator Real Loss: [0.6731374859809875, 0.625] | Discriminator Fake Loss: [0.6776355504989624, 0.5] | GAN_Loss: 0.80130934715271\n",
      "Epoch: 843 | Discriminator Real Loss: [0.7354730367660522, 0.53125] | Discriminator Fake Loss: [0.6364182233810425, 0.65625] | GAN_Loss: 0.7988244295120239\n",
      "Epoch: 844 | Discriminator Real Loss: [0.7106653451919556, 0.5625] | Discriminator Fake Loss: [0.6917374134063721, 0.5] | GAN_Loss: 0.7453564405441284\n",
      "Epoch: 845 | Discriminator Real Loss: [0.6268413066864014, 0.6875] | Discriminator Fake Loss: [0.6870332360267639, 0.53125] | GAN_Loss: 0.7653763294219971\n",
      "Epoch: 846 | Discriminator Real Loss: [0.6510281562805176, 0.65625] | Discriminator Fake Loss: [0.6139668822288513, 0.6875] | GAN_Loss: 0.748470664024353\n",
      "Epoch: 847 | Discriminator Real Loss: [0.6560510993003845, 0.65625] | Discriminator Fake Loss: [0.6212290525436401, 0.6875] | GAN_Loss: 0.8899757862091064\n",
      "Epoch: 848 | Discriminator Real Loss: [0.6230401992797852, 0.78125] | Discriminator Fake Loss: [0.5774071216583252, 0.65625] | GAN_Loss: 0.9547775387763977\n",
      "Epoch: 849 | Discriminator Real Loss: [0.6666117906570435, 0.6875] | Discriminator Fake Loss: [0.647770881652832, 0.59375] | GAN_Loss: 0.9275484085083008\n",
      "Epoch: 850 | Discriminator Real Loss: [0.679911732673645, 0.5625] | Discriminator Fake Loss: [0.6568476557731628, 0.5] | GAN_Loss: 0.7808430194854736\n",
      "Epoch: 851 | Discriminator Real Loss: [0.7277228832244873, 0.5] | Discriminator Fake Loss: [0.6271902918815613, 0.65625] | GAN_Loss: 0.9484275579452515\n",
      "Epoch: 852 | Discriminator Real Loss: [0.717879056930542, 0.4375] | Discriminator Fake Loss: [0.6844987869262695, 0.5625] | GAN_Loss: 0.7585203647613525\n",
      "Epoch: 853 | Discriminator Real Loss: [0.6365374326705933, 0.71875] | Discriminator Fake Loss: [0.7068244218826294, 0.4375] | GAN_Loss: 0.7397891879081726\n",
      "Epoch: 854 | Discriminator Real Loss: [0.6107587814331055, 0.71875] | Discriminator Fake Loss: [0.6919628381729126, 0.4375] | GAN_Loss: 0.7730120420455933\n",
      "Epoch: 855 | Discriminator Real Loss: [0.699470043182373, 0.59375] | Discriminator Fake Loss: [0.6384571194648743, 0.75] | GAN_Loss: 0.9686388373374939\n",
      "Epoch: 856 | Discriminator Real Loss: [0.6355994939804077, 0.75] | Discriminator Fake Loss: [0.5511372685432434, 0.75] | GAN_Loss: 0.8492968082427979\n",
      "Epoch: 857 | Discriminator Real Loss: [0.701342761516571, 0.5] | Discriminator Fake Loss: [0.6209436655044556, 0.75] | GAN_Loss: 0.858579158782959\n",
      "Epoch: 858 | Discriminator Real Loss: [0.7209787368774414, 0.4375] | Discriminator Fake Loss: [0.6154892444610596, 0.625] | GAN_Loss: 0.9090069532394409\n",
      "Epoch: 859 | Discriminator Real Loss: [0.7287627458572388, 0.4375] | Discriminator Fake Loss: [0.5558101534843445, 0.84375] | GAN_Loss: 0.8588594198226929\n",
      "Epoch: 860 | Discriminator Real Loss: [0.7301487922668457, 0.53125] | Discriminator Fake Loss: [0.6470502614974976, 0.65625] | GAN_Loss: 0.823067843914032\n",
      "Epoch: 861 | Discriminator Real Loss: [0.6434738039970398, 0.625] | Discriminator Fake Loss: [0.6544589400291443, 0.65625] | GAN_Loss: 0.8057668209075928\n",
      "Epoch: 862 | Discriminator Real Loss: [0.6288779973983765, 0.75] | Discriminator Fake Loss: [0.6656578779220581, 0.53125] | GAN_Loss: 0.7683629989624023\n",
      "Epoch: 863 | Discriminator Real Loss: [0.6578435897827148, 0.59375] | Discriminator Fake Loss: [0.6265481114387512, 0.75] | GAN_Loss: 0.8161813616752625\n",
      "Epoch: 864 | Discriminator Real Loss: [0.6466681957244873, 0.5625] | Discriminator Fake Loss: [0.6715927124023438, 0.6875] | GAN_Loss: 0.8723121881484985\n",
      "Epoch: 865 | Discriminator Real Loss: [0.7069147825241089, 0.5] | Discriminator Fake Loss: [0.6432012319564819, 0.71875] | GAN_Loss: 0.8569090366363525\n",
      "Epoch: 866 | Discriminator Real Loss: [0.7147700786590576, 0.5625] | Discriminator Fake Loss: [0.6433334946632385, 0.75] | GAN_Loss: 0.8473505973815918\n",
      "Epoch: 867 | Discriminator Real Loss: [0.6499127745628357, 0.71875] | Discriminator Fake Loss: [0.6794012784957886, 0.59375] | GAN_Loss: 0.7937558889389038\n",
      "Epoch: 868 | Discriminator Real Loss: [0.6366770267486572, 0.625] | Discriminator Fake Loss: [0.7037778496742249, 0.46875] | GAN_Loss: 0.8473336696624756\n",
      "Epoch: 869 | Discriminator Real Loss: [0.7453584671020508, 0.40625] | Discriminator Fake Loss: [0.6402527093887329, 0.71875] | GAN_Loss: 0.8645201921463013\n",
      "Epoch: 870 | Discriminator Real Loss: [0.7177532911300659, 0.53125] | Discriminator Fake Loss: [0.638870894908905, 0.75] | GAN_Loss: 0.8502035140991211\n",
      "Epoch: 871 | Discriminator Real Loss: [0.6526476144790649, 0.59375] | Discriminator Fake Loss: [0.7855128049850464, 0.34375] | GAN_Loss: 0.7414512038230896\n",
      "Epoch: 872 | Discriminator Real Loss: [0.6858226656913757, 0.5625] | Discriminator Fake Loss: [0.6906841993331909, 0.5625] | GAN_Loss: 0.7709660530090332\n",
      "Epoch: 873 | Discriminator Real Loss: [0.6638171672821045, 0.625] | Discriminator Fake Loss: [0.7014323472976685, 0.5] | GAN_Loss: 0.8082500696182251\n",
      "Epoch: 874 | Discriminator Real Loss: [0.636667013168335, 0.6875] | Discriminator Fake Loss: [0.6720315217971802, 0.59375] | GAN_Loss: 0.8332154750823975\n",
      "Epoch: 875 | Discriminator Real Loss: [0.7431105375289917, 0.40625] | Discriminator Fake Loss: [0.6880597472190857, 0.46875] | GAN_Loss: 0.9130123853683472\n",
      "Epoch: 876 | Discriminator Real Loss: [0.7456883788108826, 0.40625] | Discriminator Fake Loss: [0.6655298471450806, 0.59375] | GAN_Loss: 0.7489500641822815\n",
      "Epoch: 877 | Discriminator Real Loss: [0.7583411931991577, 0.46875] | Discriminator Fake Loss: [0.7535340785980225, 0.25] | GAN_Loss: 0.7630389332771301\n",
      "Epoch: 878 | Discriminator Real Loss: [0.601542592048645, 0.84375] | Discriminator Fake Loss: [0.7639493346214294, 0.46875] | GAN_Loss: 0.7776833176612854\n",
      "Epoch: 879 | Discriminator Real Loss: [0.6182029247283936, 0.6875] | Discriminator Fake Loss: [0.6999629735946655, 0.4375] | GAN_Loss: 0.7893663644790649\n",
      "Epoch: 880 | Discriminator Real Loss: [0.6611983180046082, 0.65625] | Discriminator Fake Loss: [0.6989370584487915, 0.40625] | GAN_Loss: 0.7993614077568054\n",
      "Epoch: 881 | Discriminator Real Loss: [0.6885595321655273, 0.6875] | Discriminator Fake Loss: [0.6437596082687378, 0.5625] | GAN_Loss: 0.8518676161766052\n",
      "Epoch: 882 | Discriminator Real Loss: [0.7001442313194275, 0.59375] | Discriminator Fake Loss: [0.6163684129714966, 0.625] | GAN_Loss: 0.9103316068649292\n",
      "Epoch: 883 | Discriminator Real Loss: [0.7149690985679626, 0.53125] | Discriminator Fake Loss: [0.6017379760742188, 0.625] | GAN_Loss: 0.8966963887214661\n",
      "Epoch: 884 | Discriminator Real Loss: [0.6770807504653931, 0.71875] | Discriminator Fake Loss: [0.6156175136566162, 0.5625] | GAN_Loss: 0.9356405138969421\n",
      "Epoch: 885 | Discriminator Real Loss: [0.9646838903427124, 0.34375] | Discriminator Fake Loss: [0.625710129737854, 0.625] | GAN_Loss: 0.8158420324325562\n",
      "Epoch: 886 | Discriminator Real Loss: [0.657135009765625, 0.65625] | Discriminator Fake Loss: [0.5936681628227234, 0.625] | GAN_Loss: 0.7839230298995972\n",
      "Epoch: 887 | Discriminator Real Loss: [0.6524057388305664, 0.75] | Discriminator Fake Loss: [0.7174059152603149, 0.4375] | GAN_Loss: 0.7721888422966003\n",
      "Epoch: 888 | Discriminator Real Loss: [0.6093038320541382, 0.8125] | Discriminator Fake Loss: [0.7309279441833496, 0.375] | GAN_Loss: 0.7245476245880127\n",
      "Epoch: 889 | Discriminator Real Loss: [0.6481496691703796, 0.625] | Discriminator Fake Loss: [0.6553693413734436, 0.65625] | GAN_Loss: 0.8891772031784058\n",
      "Epoch: 890 | Discriminator Real Loss: [0.6271636486053467, 0.78125] | Discriminator Fake Loss: [0.67267906665802, 0.5] | GAN_Loss: 1.163459300994873\n",
      "Epoch: 891 | Discriminator Real Loss: [0.7019988894462585, 0.625] | Discriminator Fake Loss: [0.6087119579315186, 0.6875] | GAN_Loss: 0.8758003115653992\n",
      "Epoch: 892 | Discriminator Real Loss: [0.7753341197967529, 0.46875] | Discriminator Fake Loss: [0.6603488922119141, 0.5625] | GAN_Loss: 0.8791590332984924\n",
      "Epoch: 893 | Discriminator Real Loss: [0.7156843543052673, 0.46875] | Discriminator Fake Loss: [0.6655073165893555, 0.46875] | GAN_Loss: 0.8491297364234924\n",
      "Epoch: 894 | Discriminator Real Loss: [0.6785391569137573, 0.53125] | Discriminator Fake Loss: [0.6697869300842285, 0.5] | GAN_Loss: 0.8063191771507263\n",
      "Epoch: 895 | Discriminator Real Loss: [0.6541316509246826, 0.65625] | Discriminator Fake Loss: [0.7020319700241089, 0.375] | GAN_Loss: 0.7156916856765747\n",
      "Epoch: 896 | Discriminator Real Loss: [0.6417514085769653, 0.65625] | Discriminator Fake Loss: [0.7159688472747803, 0.46875] | GAN_Loss: 0.8019163012504578\n",
      "Epoch: 897 | Discriminator Real Loss: [0.6259157657623291, 0.71875] | Discriminator Fake Loss: [0.7114623785018921, 0.3125] | GAN_Loss: 0.9473044276237488\n",
      "Epoch: 898 | Discriminator Real Loss: [0.6485952138900757, 0.71875] | Discriminator Fake Loss: [0.7173781991004944, 0.40625] | GAN_Loss: 0.8049728274345398\n",
      "Epoch: 899 | Discriminator Real Loss: [0.6524733304977417, 0.71875] | Discriminator Fake Loss: [0.770505428314209, 0.40625] | GAN_Loss: 0.6898671984672546\n",
      "Epoch: 900 | Discriminator Real Loss: [0.6168851852416992, 0.78125] | Discriminator Fake Loss: [0.8168877363204956, 0.21875] | GAN_Loss: 0.7648541927337646\n",
      "Epoch: 901 | Discriminator Real Loss: [0.6517378687858582, 0.6875] | Discriminator Fake Loss: [0.6905781030654907, 0.40625] | GAN_Loss: 0.8326279520988464\n",
      "Epoch: 902 | Discriminator Real Loss: [0.6917323470115662, 0.625] | Discriminator Fake Loss: [0.5102808475494385, 0.6875] | GAN_Loss: 0.9197083711624146\n",
      "Epoch: 903 | Discriminator Real Loss: [0.8020747900009155, 0.46875] | Discriminator Fake Loss: [0.6855405569076538, 0.4375] | GAN_Loss: 0.9467033743858337\n",
      "Epoch: 904 | Discriminator Real Loss: [0.7641899585723877, 0.5625] | Discriminator Fake Loss: [0.6954406499862671, 0.4375] | GAN_Loss: 0.748253583908081\n",
      "Epoch: 905 | Discriminator Real Loss: [0.627393364906311, 0.8125] | Discriminator Fake Loss: [0.7767626047134399, 0.21875] | GAN_Loss: 0.717941403388977\n",
      "Epoch: 906 | Discriminator Real Loss: [0.6020172834396362, 0.78125] | Discriminator Fake Loss: [0.7699991464614868, 0.28125] | GAN_Loss: 0.622677206993103\n",
      "Epoch: 907 | Discriminator Real Loss: [0.5758541822433472, 0.90625] | Discriminator Fake Loss: [0.7497242093086243, 0.34375] | GAN_Loss: 0.7185620069503784\n",
      "Epoch: 908 | Discriminator Real Loss: [0.6767851114273071, 0.6875] | Discriminator Fake Loss: [0.7377628684043884, 0.375] | GAN_Loss: 0.7782888412475586\n",
      "Epoch: 909 | Discriminator Real Loss: [0.6950027942657471, 0.5625] | Discriminator Fake Loss: [0.6683299541473389, 0.40625] | GAN_Loss: 0.75993812084198\n",
      "Epoch: 910 | Discriminator Real Loss: [0.6987321972846985, 0.59375] | Discriminator Fake Loss: [0.6338264346122742, 0.4375] | GAN_Loss: 0.8816908597946167\n",
      "Epoch: 911 | Discriminator Real Loss: [0.6584978699684143, 0.75] | Discriminator Fake Loss: [0.5686941146850586, 0.6875] | GAN_Loss: 0.9702698588371277\n",
      "Epoch: 912 | Discriminator Real Loss: [0.6659411191940308, 0.625] | Discriminator Fake Loss: [0.6844285726547241, 0.53125] | GAN_Loss: 0.785495936870575\n",
      "Epoch: 913 | Discriminator Real Loss: [0.6475061178207397, 0.71875] | Discriminator Fake Loss: [0.5676765441894531, 0.8125] | GAN_Loss: 0.8216946125030518\n",
      "Epoch: 914 | Discriminator Real Loss: [0.6864480376243591, 0.5] | Discriminator Fake Loss: [0.5734084248542786, 0.6875] | GAN_Loss: 0.902887225151062\n",
      "Epoch: 915 | Discriminator Real Loss: [0.7292782068252563, 0.46875] | Discriminator Fake Loss: [0.5526890754699707, 0.78125] | GAN_Loss: 0.9572403430938721\n",
      "Epoch: 916 | Discriminator Real Loss: [0.6815216541290283, 0.5] | Discriminator Fake Loss: [0.5906875729560852, 0.6875] | GAN_Loss: 0.800290584564209\n",
      "Epoch: 917 | Discriminator Real Loss: [0.7007962465286255, 0.625] | Discriminator Fake Loss: [0.6176692247390747, 0.6875] | GAN_Loss: 0.967509388923645\n",
      "Epoch: 918 | Discriminator Real Loss: [0.6489554643630981, 0.65625] | Discriminator Fake Loss: [0.5767563581466675, 0.84375] | GAN_Loss: 1.0590530633926392\n",
      "Epoch: 919 | Discriminator Real Loss: [0.6824851632118225, 0.65625] | Discriminator Fake Loss: [0.5457333326339722, 0.75] | GAN_Loss: 0.8956789970397949\n",
      "Epoch: 920 | Discriminator Real Loss: [0.6736003756523132, 0.59375] | Discriminator Fake Loss: [0.6274836659431458, 0.6875] | GAN_Loss: 0.881005048751831\n",
      "Epoch: 921 | Discriminator Real Loss: [0.6498380899429321, 0.6875] | Discriminator Fake Loss: [0.6568645238876343, 0.59375] | GAN_Loss: 0.8888110518455505\n",
      "Epoch: 922 | Discriminator Real Loss: [0.6871727705001831, 0.65625] | Discriminator Fake Loss: [0.6000025868415833, 0.6875] | GAN_Loss: 0.9338176846504211\n",
      "Epoch: 923 | Discriminator Real Loss: [0.6708386540412903, 0.59375] | Discriminator Fake Loss: [0.6491813659667969, 0.53125] | GAN_Loss: 0.8614266514778137\n",
      "Epoch: 924 | Discriminator Real Loss: [0.6858190298080444, 0.59375] | Discriminator Fake Loss: [0.6437837481498718, 0.75] | GAN_Loss: 0.8962388634681702\n",
      "Epoch: 925 | Discriminator Real Loss: [0.6721563935279846, 0.625] | Discriminator Fake Loss: [0.6258372068405151, 0.625] | GAN_Loss: 0.8204419612884521\n",
      "Epoch: 926 | Discriminator Real Loss: [0.6374853849411011, 0.59375] | Discriminator Fake Loss: [0.6873939037322998, 0.5] | GAN_Loss: 0.91419917345047\n",
      "Epoch: 927 | Discriminator Real Loss: [0.6943560838699341, 0.5] | Discriminator Fake Loss: [0.6903821229934692, 0.5] | GAN_Loss: 0.911484956741333\n",
      "Epoch: 928 | Discriminator Real Loss: [0.6486188769340515, 0.6875] | Discriminator Fake Loss: [0.5635500550270081, 0.71875] | GAN_Loss: 1.0831398963928223\n",
      "Epoch: 929 | Discriminator Real Loss: [0.6451858282089233, 0.59375] | Discriminator Fake Loss: [0.6705828309059143, 0.53125] | GAN_Loss: 0.7897617816925049\n",
      "Epoch: 930 | Discriminator Real Loss: [0.6368585824966431, 0.6875] | Discriminator Fake Loss: [0.6158950328826904, 0.625] | GAN_Loss: 0.9202241897583008\n",
      "Epoch: 931 | Discriminator Real Loss: [0.6706477403640747, 0.625] | Discriminator Fake Loss: [0.6556661128997803, 0.5] | GAN_Loss: 0.7975670099258423\n",
      "Epoch: 932 | Discriminator Real Loss: [0.6747703552246094, 0.5625] | Discriminator Fake Loss: [0.7625956535339355, 0.28125] | GAN_Loss: 0.7553032636642456\n",
      "Epoch: 933 | Discriminator Real Loss: [0.636611819267273, 0.6875] | Discriminator Fake Loss: [0.732810378074646, 0.46875] | GAN_Loss: 0.7073783278465271\n",
      "Epoch: 934 | Discriminator Real Loss: [0.5839842557907104, 0.78125] | Discriminator Fake Loss: [0.6472283601760864, 0.40625] | GAN_Loss: 0.8465132713317871\n",
      "Epoch: 935 | Discriminator Real Loss: [0.6276931762695312, 0.75] | Discriminator Fake Loss: [0.6230554580688477, 0.53125] | GAN_Loss: 0.791985034942627\n",
      "Epoch: 936 | Discriminator Real Loss: [0.6560894846916199, 0.6875] | Discriminator Fake Loss: [0.6331219673156738, 0.625] | GAN_Loss: 0.8035322427749634\n",
      "Epoch: 937 | Discriminator Real Loss: [0.6615075469017029, 0.6875] | Discriminator Fake Loss: [0.671695351600647, 0.5625] | GAN_Loss: 0.9431989192962646\n",
      "Epoch: 938 | Discriminator Real Loss: [0.6939176917076111, 0.71875] | Discriminator Fake Loss: [0.5999116897583008, 0.75] | GAN_Loss: 0.928801953792572\n",
      "Epoch: 939 | Discriminator Real Loss: [0.675095796585083, 0.625] | Discriminator Fake Loss: [0.6210246086120605, 0.625] | GAN_Loss: 1.1069719791412354\n",
      "Epoch: 940 | Discriminator Real Loss: [0.7171676158905029, 0.53125] | Discriminator Fake Loss: [0.5626108050346375, 0.8125] | GAN_Loss: 1.1795122623443604\n",
      "Epoch: 941 | Discriminator Real Loss: [0.8826436996459961, 0.3125] | Discriminator Fake Loss: [0.5159763097763062, 0.71875] | GAN_Loss: 1.0554630756378174\n",
      "Epoch: 942 | Discriminator Real Loss: [0.6463668942451477, 0.59375] | Discriminator Fake Loss: [0.6667566299438477, 0.59375] | GAN_Loss: 0.8066748380661011\n",
      "Epoch: 943 | Discriminator Real Loss: [0.634463369846344, 0.75] | Discriminator Fake Loss: [0.5849933624267578, 0.6875] | GAN_Loss: 0.8941093683242798\n",
      "Epoch: 944 | Discriminator Real Loss: [0.6308640241622925, 0.71875] | Discriminator Fake Loss: [0.6116598844528198, 0.71875] | GAN_Loss: 0.879216194152832\n",
      "Epoch: 945 | Discriminator Real Loss: [0.6408675312995911, 0.6875] | Discriminator Fake Loss: [0.5936980247497559, 0.59375] | GAN_Loss: 1.0917532444000244\n",
      "Epoch: 946 | Discriminator Real Loss: [0.6992173194885254, 0.53125] | Discriminator Fake Loss: [0.5677086114883423, 0.6875] | GAN_Loss: 1.090273141860962\n",
      "Epoch: 947 | Discriminator Real Loss: [0.6590878963470459, 0.625] | Discriminator Fake Loss: [0.5650457739830017, 0.65625] | GAN_Loss: 1.0388529300689697\n",
      "Epoch: 948 | Discriminator Real Loss: [0.794483482837677, 0.5] | Discriminator Fake Loss: [0.6504796743392944, 0.625] | GAN_Loss: 0.9734866619110107\n",
      "Epoch: 949 | Discriminator Real Loss: [0.6806086301803589, 0.59375] | Discriminator Fake Loss: [0.6874910593032837, 0.53125] | GAN_Loss: 0.8518486022949219\n",
      "Epoch: 950 | Discriminator Real Loss: [0.6452088356018066, 0.59375] | Discriminator Fake Loss: [0.6413205862045288, 0.6875] | GAN_Loss: 0.7901546955108643\n",
      "Epoch: 951 | Discriminator Real Loss: [0.6468092203140259, 0.65625] | Discriminator Fake Loss: [0.6672297716140747, 0.40625] | GAN_Loss: 0.8974740505218506\n",
      "Epoch: 952 | Discriminator Real Loss: [0.7069388628005981, 0.4375] | Discriminator Fake Loss: [0.6272764801979065, 0.625] | GAN_Loss: 0.9090344905853271\n",
      "Epoch: 953 | Discriminator Real Loss: [0.7475485801696777, 0.40625] | Discriminator Fake Loss: [0.5464763641357422, 0.78125] | GAN_Loss: 0.998469352722168\n",
      "Epoch: 954 | Discriminator Real Loss: [0.7830297946929932, 0.46875] | Discriminator Fake Loss: [0.6058802604675293, 0.6875] | GAN_Loss: 0.8524638414382935\n",
      "Epoch: 955 | Discriminator Real Loss: [0.6759163737297058, 0.5625] | Discriminator Fake Loss: [0.6748170852661133, 0.46875] | GAN_Loss: 0.8604743480682373\n",
      "Epoch: 956 | Discriminator Real Loss: [0.6337450742721558, 0.5] | Discriminator Fake Loss: [0.7714630365371704, 0.40625] | GAN_Loss: 0.8629209995269775\n",
      "Epoch: 957 | Discriminator Real Loss: [0.6356645226478577, 0.8125] | Discriminator Fake Loss: [0.6767749786376953, 0.46875] | GAN_Loss: 0.7999222278594971\n",
      "Epoch: 958 | Discriminator Real Loss: [0.6380173563957214, 0.6875] | Discriminator Fake Loss: [0.6864213943481445, 0.5] | GAN_Loss: 0.7798559069633484\n",
      "Epoch: 959 | Discriminator Real Loss: [0.6503084897994995, 0.625] | Discriminator Fake Loss: [0.7113940119743347, 0.46875] | GAN_Loss: 0.751872181892395\n",
      "Epoch: 960 | Discriminator Real Loss: [0.6280646324157715, 0.65625] | Discriminator Fake Loss: [0.7421277761459351, 0.34375] | GAN_Loss: 0.70688796043396\n",
      "Epoch: 961 | Discriminator Real Loss: [0.5965467095375061, 0.8125] | Discriminator Fake Loss: [0.7356504201889038, 0.46875] | GAN_Loss: 0.7477679252624512\n",
      "Epoch: 962 | Discriminator Real Loss: [0.6264592409133911, 0.71875] | Discriminator Fake Loss: [0.6858941316604614, 0.46875] | GAN_Loss: 0.8200979828834534\n",
      "Epoch: 963 | Discriminator Real Loss: [0.6135002374649048, 0.6875] | Discriminator Fake Loss: [0.7297991514205933, 0.40625] | GAN_Loss: 0.8449054956436157\n",
      "Epoch: 964 | Discriminator Real Loss: [0.7275888919830322, 0.5] | Discriminator Fake Loss: [0.6583677530288696, 0.46875] | GAN_Loss: 0.8777143955230713\n",
      "Epoch: 965 | Discriminator Real Loss: [0.6478874087333679, 0.75] | Discriminator Fake Loss: [0.6629725098609924, 0.53125] | GAN_Loss: 0.8322181105613708\n",
      "Epoch: 966 | Discriminator Real Loss: [0.692772388458252, 0.5625] | Discriminator Fake Loss: [0.6289726495742798, 0.65625] | GAN_Loss: 0.8687034249305725\n",
      "Epoch: 967 | Discriminator Real Loss: [0.6534312963485718, 0.625] | Discriminator Fake Loss: [0.5319612622261047, 0.78125] | GAN_Loss: 1.005237340927124\n",
      "Epoch: 968 | Discriminator Real Loss: [0.7493566274642944, 0.53125] | Discriminator Fake Loss: [0.5795677304267883, 0.78125] | GAN_Loss: 0.8974873423576355\n",
      "Epoch: 969 | Discriminator Real Loss: [0.6877709627151489, 0.59375] | Discriminator Fake Loss: [0.6127829551696777, 0.65625] | GAN_Loss: 0.910039484500885\n",
      "Epoch: 970 | Discriminator Real Loss: [0.6623601913452148, 0.59375] | Discriminator Fake Loss: [0.6020961403846741, 0.75] | GAN_Loss: 0.8752701878547668\n",
      "Epoch: 971 | Discriminator Real Loss: [0.6804801225662231, 0.5625] | Discriminator Fake Loss: [0.5946923494338989, 0.65625] | GAN_Loss: 0.8825725317001343\n",
      "Epoch: 972 | Discriminator Real Loss: [0.6706656813621521, 0.625] | Discriminator Fake Loss: [0.64546799659729, 0.59375] | GAN_Loss: 0.8726915717124939\n",
      "Epoch: 973 | Discriminator Real Loss: [0.6559607982635498, 0.625] | Discriminator Fake Loss: [0.5866247415542603, 0.625] | GAN_Loss: 0.8464818596839905\n",
      "Epoch: 974 | Discriminator Real Loss: [0.591254711151123, 0.71875] | Discriminator Fake Loss: [0.6077414751052856, 0.5625] | GAN_Loss: 0.9359129667282104\n",
      "Epoch: 975 | Discriminator Real Loss: [0.6577675342559814, 0.625] | Discriminator Fake Loss: [0.6636612415313721, 0.5625] | GAN_Loss: 1.0570802688598633\n",
      "Epoch: 976 | Discriminator Real Loss: [0.6694778800010681, 0.625] | Discriminator Fake Loss: [0.581742525100708, 0.71875] | GAN_Loss: 0.9796168208122253\n",
      "Epoch: 977 | Discriminator Real Loss: [0.7568818926811218, 0.4375] | Discriminator Fake Loss: [0.7065862417221069, 0.59375] | GAN_Loss: 0.9184406399726868\n",
      "Epoch: 978 | Discriminator Real Loss: [0.641191840171814, 0.6875] | Discriminator Fake Loss: [0.6426911354064941, 0.53125] | GAN_Loss: 0.8787921667098999\n",
      "Epoch: 979 | Discriminator Real Loss: [0.7105748057365417, 0.59375] | Discriminator Fake Loss: [0.6074165105819702, 0.65625] | GAN_Loss: 0.8553469777107239\n",
      "Epoch: 980 | Discriminator Real Loss: [0.7197399735450745, 0.59375] | Discriminator Fake Loss: [0.5964002013206482, 0.625] | GAN_Loss: 1.0121886730194092\n",
      "Epoch: 981 | Discriminator Real Loss: [0.761094331741333, 0.59375] | Discriminator Fake Loss: [0.6183130741119385, 0.65625] | GAN_Loss: 1.058040976524353\n",
      "Epoch: 982 | Discriminator Real Loss: [0.7763686180114746, 0.5625] | Discriminator Fake Loss: [0.5883919596672058, 0.6875] | GAN_Loss: 0.9424524307250977\n",
      "Epoch: 983 | Discriminator Real Loss: [0.7321264743804932, 0.5] | Discriminator Fake Loss: [0.662442684173584, 0.5] | GAN_Loss: 0.9280364513397217\n",
      "Epoch: 984 | Discriminator Real Loss: [0.6975580453872681, 0.625] | Discriminator Fake Loss: [0.6960715651512146, 0.46875] | GAN_Loss: 0.8740318417549133\n",
      "Epoch: 985 | Discriminator Real Loss: [0.6793009638786316, 0.59375] | Discriminator Fake Loss: [0.6504340171813965, 0.4375] | GAN_Loss: 0.8034106492996216\n",
      "Epoch: 986 | Discriminator Real Loss: [0.5986021161079407, 0.75] | Discriminator Fake Loss: [0.7979085445404053, 0.25] | GAN_Loss: 0.7231594324111938\n",
      "Epoch: 987 | Discriminator Real Loss: [0.4856712222099304, 0.96875] | Discriminator Fake Loss: [0.8694179058074951, 0.0625] | GAN_Loss: 0.6024123430252075\n",
      "Epoch: 988 | Discriminator Real Loss: [0.5116859674453735, 0.96875] | Discriminator Fake Loss: [0.8816577196121216, 0.03125] | GAN_Loss: 0.6923695206642151\n",
      "Epoch: 989 | Discriminator Real Loss: [0.584110677242279, 0.875] | Discriminator Fake Loss: [0.7931058406829834, 0.28125] | GAN_Loss: 0.7090786695480347\n",
      "Epoch: 990 | Discriminator Real Loss: [0.5977533459663391, 0.75] | Discriminator Fake Loss: [0.7148536443710327, 0.3125] | GAN_Loss: 0.7185131907463074\n",
      "Epoch: 991 | Discriminator Real Loss: [0.6098666787147522, 0.8125] | Discriminator Fake Loss: [0.6039074659347534, 0.65625] | GAN_Loss: 0.9849504828453064\n",
      "Epoch: 992 | Discriminator Real Loss: [0.7592787742614746, 0.53125] | Discriminator Fake Loss: [0.5539988279342651, 0.78125] | GAN_Loss: 1.0956683158874512\n",
      "Epoch: 993 | Discriminator Real Loss: [0.745362401008606, 0.4375] | Discriminator Fake Loss: [0.6011309623718262, 0.59375] | GAN_Loss: 1.0307867527008057\n",
      "Epoch: 994 | Discriminator Real Loss: [0.7843226790428162, 0.40625] | Discriminator Fake Loss: [0.5735373497009277, 0.65625] | GAN_Loss: 0.9343907237052917\n",
      "Epoch: 995 | Discriminator Real Loss: [0.6838211417198181, 0.59375] | Discriminator Fake Loss: [0.5653586387634277, 0.625] | GAN_Loss: 0.910888671875\n",
      "Epoch: 996 | Discriminator Real Loss: [0.6487954258918762, 0.625] | Discriminator Fake Loss: [0.6704087257385254, 0.40625] | GAN_Loss: 0.856203556060791\n",
      "Epoch: 997 | Discriminator Real Loss: [0.6145319938659668, 0.75] | Discriminator Fake Loss: [0.5999695062637329, 0.65625] | GAN_Loss: 0.9177265167236328\n",
      "Epoch: 998 | Discriminator Real Loss: [0.646852970123291, 0.75] | Discriminator Fake Loss: [0.5833178758621216, 0.625] | GAN_Loss: 0.9362086653709412\n",
      "Epoch: 999 | Discriminator Real Loss: [0.6777979135513306, 0.59375] | Discriminator Fake Loss: [0.6246570348739624, 0.59375] | GAN_Loss: 0.9867792725563049\n",
      "Epoch: 1000 | Discriminator Real Loss: [0.7604239583015442, 0.46875] | Discriminator Fake Loss: [0.5508466958999634, 0.71875] | GAN_Loss: 1.0111608505249023\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Generating 46 synthetic samples to make sure both the classes have the same 145 classes**",
   "id": "aaad11014eb73f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.648689Z",
     "start_time": "2024-10-01T22:46:36.459024Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "execution_count": 28,
   "source": [
    "#Now generating synthetic samples\n",
    "#Majority class 0 has 145 samples while minority class has 99 samples\n",
    "#46 samples have to be generated\n",
    "tf.random.set_seed(42)\n",
    "num_samples = 46\n",
    "noise_vec = tf.random.normal((num_samples, input_dim))\n",
    "synth_min_samples = gen_model.predict(noise_vec)\n",
    "\n",
    "synth_min_df = pd.DataFrame(synth_min_samples, columns = minority_dataset.columns)\n",
    "synth_min_df['TUMOR_STAGE'] = 1"
   ],
   "id": "448ceb4989f94115"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.686390Z",
     "start_time": "2024-10-01T22:46:36.673038Z"
    }
   },
   "cell_type": "code",
   "source": "synth_min_df",
   "id": "94f3cc51606e31e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "0   0.237651  0.729190   0.605940    0.679013   0.451559  0.831169   \n",
       "1   0.369828  0.930027   0.800286    0.798110   0.131688  0.677325   \n",
       "2   0.308023  0.429953   0.721172    0.357352   0.854355  0.366576   \n",
       "3   0.347061  0.700261   0.743680    0.679450   0.525843  0.636494   \n",
       "4   0.379348  0.485735   0.902474    0.301700   0.758239  0.409764   \n",
       "5   0.565790  0.569892   0.707079    0.383006   0.587917  0.707839   \n",
       "6   0.629074  0.651405   0.846473    0.486864   0.688865  0.547808   \n",
       "7   0.396178  0.655764   0.741329    0.518046   0.294009  0.785328   \n",
       "8   0.386981  0.582482   0.625047    0.629923   0.651290  0.848843   \n",
       "9   0.421718  0.598622   0.631353    0.490685   0.685284  0.695042   \n",
       "10  0.326387  0.552856   0.702903    0.565050   0.578337  0.767844   \n",
       "11  0.241876  0.848357   0.705584    0.574364   0.349749  0.735385   \n",
       "12  0.358768  0.478006   0.881538    0.159423   0.966694  0.138005   \n",
       "13  0.267484  0.629719   0.694270    0.572713   0.520980  0.808036   \n",
       "14  0.276822  0.604952   0.607718    0.339509   0.459687  0.573373   \n",
       "15  0.293833  0.528527   0.878200    0.164991   0.846099  0.129798   \n",
       "16  0.545178  0.515135   0.865187    0.292131   0.804161  0.581020   \n",
       "17  0.666762  0.600657   0.868512    0.300797   0.520435  0.360254   \n",
       "18  0.442381  0.612713   0.767811    0.526242   0.588791  0.717802   \n",
       "19  0.251392  0.643047   0.558224    0.459966   0.205370  0.810277   \n",
       "20  0.361785  0.585448   0.818406    0.193644   0.767002  0.313980   \n",
       "21  0.593003  0.639107   0.821441    0.412521   0.690143  0.606098   \n",
       "22  0.350108  0.718883   0.871888    0.381674   0.683797  0.423932   \n",
       "23  0.278718  0.480350   0.483718    0.490582   0.418499  0.848597   \n",
       "24  0.272893  0.573101   0.778401    0.281077   0.878818  0.207817   \n",
       "25  0.440783  0.519928   0.879034    0.385832   0.811544  0.522823   \n",
       "26  0.373736  0.375571   0.619162    0.444945   0.530980  0.800940   \n",
       "27  0.290136  0.524097   0.813150    0.391404   0.768282  0.693356   \n",
       "28  0.377685  0.609953   0.785926    0.541302   0.486657  0.756326   \n",
       "29  0.539308  0.784577   0.815233    0.634706   0.642810  0.820022   \n",
       "30  0.624636  0.769732   0.895050    0.475828   0.760051  0.676930   \n",
       "31  0.404288  0.727831   0.821608    0.504695   0.496421  0.652849   \n",
       "32  0.336444  0.655455   0.800771    0.444520   0.886358  0.543182   \n",
       "33  0.544371  0.554261   0.781406    0.335528   0.386157  0.578926   \n",
       "34  0.485811  0.486084   0.712341    0.395553   0.783639  0.659570   \n",
       "35  0.517371  0.507572   0.731585    0.417628   0.735392  0.727789   \n",
       "36  0.342803  0.457861   0.818594    0.389036   0.546529  0.653052   \n",
       "37  0.406139  0.429788   0.675112    0.392159   0.676224  0.620942   \n",
       "38  0.525268  0.916472   0.848064    0.635298   0.391362  0.636274   \n",
       "39  0.530529  0.687292   0.678219    0.484790   0.655578  0.724860   \n",
       "40  0.404238  0.619757   0.825180    0.566859   0.455378  0.644695   \n",
       "41  0.405292  0.686501   0.796409    0.654468   0.423074  0.687883   \n",
       "42  0.387901  0.607557   0.721091    0.439910   0.743987  0.718245   \n",
       "43  0.269977  0.563076   0.882795    0.351648   0.261320  0.567126   \n",
       "44  0.162127  0.533088   0.618783    0.534847   0.345407  0.753220   \n",
       "45  0.375650  0.552342   0.753281    0.493070   0.679458  0.555305   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "0     0.325828  0.213020  0.603134  0.222720  ...  0.148673    0.192801   \n",
       "1     0.204672  0.720663  0.176141  0.793106  ...  0.056811    0.035842   \n",
       "2     0.640218  0.522044  0.555432  0.724144  ...  0.469516    0.646849   \n",
       "3     0.436951  0.270847  0.871431  0.292237  ...  0.426665    0.185127   \n",
       "4     0.608064  0.865734  0.453437  0.915479  ...  0.419119    0.454304   \n",
       "5     0.521907  0.559898  0.365442  0.732810  ...  0.242042    0.311391   \n",
       "6     0.468479  0.738856  0.311282  0.838537  ...  0.149459    0.255429   \n",
       "7     0.229334  0.434012  0.475705  0.651981  ...  0.220627    0.140574   \n",
       "8     0.370622  0.543530  0.454566  0.398550  ...  0.068059    0.287613   \n",
       "9     0.515173  0.594262  0.538878  0.565192  ...  0.196591    0.499814   \n",
       "10    0.393953  0.367529  0.613828  0.232676  ...  0.104450    0.345316   \n",
       "11    0.359188  0.468625  0.377813  0.572099  ...  0.208978    0.131762   \n",
       "12    0.759205  0.572877  0.385154  0.950680  ...  0.757911    0.786444   \n",
       "13    0.386335  0.525136  0.541098  0.393961  ...  0.122530    0.139119   \n",
       "14    0.163276  0.285459  0.802846  0.390080  ...  0.254628    0.262595   \n",
       "15    0.434478  0.533788  0.518813  0.879708  ...  0.707222    0.332451   \n",
       "16    0.673568  0.767693  0.264642  0.889747  ...  0.248882    0.342624   \n",
       "17    0.605129  0.738195  0.373125  0.941136  ...  0.360360    0.353803   \n",
       "18    0.383762  0.473340  0.508712  0.579058  ...  0.155236    0.364472   \n",
       "19    0.480421  0.560169  0.508363  0.678783  ...  0.257933    0.171227   \n",
       "20    0.330855  0.761924  0.484341  0.919625  ...  0.491360    0.429384   \n",
       "21    0.695898  0.723056  0.509343  0.848578  ...  0.347498    0.254766   \n",
       "22    0.516243  0.748492  0.474272  0.904725  ...  0.399785    0.152140   \n",
       "23    0.357699  0.159466  0.918111  0.124104  ...  0.164162    0.230193   \n",
       "24    0.308155  0.730882  0.483918  0.937671  ...  0.462338    0.589982   \n",
       "25    0.646083  0.668829  0.243137  0.882714  ...  0.187846    0.351444   \n",
       "26    0.562345  0.413157  0.729969  0.421593  ...  0.260799    0.353561   \n",
       "27    0.577678  0.611822  0.362581  0.718703  ...  0.137095    0.285555   \n",
       "28    0.457495  0.490369  0.383031  0.588471  ...  0.168707    0.419904   \n",
       "29    0.422096  0.761478  0.271190  0.846043  ...  0.104493    0.251409   \n",
       "30    0.597489  0.883766  0.230628  0.936862  ...  0.108385    0.371633   \n",
       "31    0.464159  0.692899  0.295981  0.834984  ...  0.132787    0.257501   \n",
       "32    0.747064  0.688248  0.286469  0.828451  ...  0.435768    0.448382   \n",
       "33    0.560789  0.698975  0.389099  0.902535  ...  0.258452    0.364763   \n",
       "34    0.562680  0.608285  0.520648  0.773193  ...  0.248074    0.521211   \n",
       "35    0.652663  0.651508  0.417071  0.601563  ...  0.133073    0.444345   \n",
       "36    0.624249  0.551393  0.581221  0.634144  ...  0.311193    0.302991   \n",
       "37    0.470348  0.442154  0.633134  0.586738  ...  0.404938    0.333120   \n",
       "38    0.427829  0.814570  0.256043  0.894241  ...  0.144845    0.068485   \n",
       "39    0.413336  0.688887  0.352727  0.702790  ...  0.117270    0.275958   \n",
       "40    0.496091  0.450806  0.738434  0.533492  ...  0.306473    0.119865   \n",
       "41    0.381280  0.523034  0.299717  0.562845  ...  0.122687    0.138823   \n",
       "42    0.691398  0.545181  0.614593  0.522004  ...  0.237433    0.341949   \n",
       "43    0.321584  0.521886  0.620781  0.783342  ...  0.541540    0.166126   \n",
       "44    0.317351  0.203723  0.888112  0.280865  ...  0.442454    0.192055   \n",
       "45    0.577510  0.493439  0.538392  0.694640  ...  0.279321    0.302901   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "0     0.825429  0.050491  0.310266  0.977595  0.230127   0.581230  0.804154   \n",
       "1     0.901164  0.038393  0.161979  0.969544  0.071375   0.195901  0.966575   \n",
       "2     0.522713  0.059618  0.368588  0.934507  0.193555   0.403459  0.640048   \n",
       "3     0.947163  0.016217  0.197161  0.973220  0.187290   0.229531  0.790565   \n",
       "4     0.777617  0.050305  0.337532  0.971554  0.229113   0.700907  0.792750   \n",
       "5     0.489828  0.091670  0.547730  0.906325  0.278722   0.576741  0.843096   \n",
       "6     0.717460  0.042664  0.489715  0.967372  0.272900   0.543963  0.834492   \n",
       "7     0.842636  0.065218  0.322324  0.936576  0.219894   0.238557  0.795408   \n",
       "8     0.503656  0.042003  0.446778  0.961520  0.322456   0.674538  0.912000   \n",
       "9     0.411801  0.094280  0.540863  0.941313  0.302134   0.626628  0.810349   \n",
       "10    0.550411  0.043712  0.493269  0.944693  0.369761   0.698894  0.924273   \n",
       "11    0.847136  0.067188  0.232569  0.970082  0.161966   0.342731  0.801249   \n",
       "12    0.522315  0.072455  0.814319  0.980400  0.235814   0.782290  0.698369   \n",
       "13    0.771875  0.042115  0.259245  0.966020  0.349711   0.417583  0.837428   \n",
       "14    0.901479  0.052275  0.158825  0.991856  0.279524   0.213771  0.641858   \n",
       "15    0.883531  0.012766  0.316914  0.974404  0.163585   0.293853  0.579762   \n",
       "16    0.446285  0.080784  0.523273  0.965441  0.261894   0.559238  0.741636   \n",
       "17    0.892824  0.055943  0.206610  0.959414  0.235307   0.297463  0.851062   \n",
       "18    0.623545  0.071723  0.397309  0.922667  0.281827   0.598153  0.874835   \n",
       "19    0.901740  0.084697  0.182178  0.964247  0.320172   0.157871  0.797468   \n",
       "20    0.632366  0.010446  0.627392  0.971712  0.213025   0.670464  0.870968   \n",
       "21    0.706167  0.034233  0.396360  0.974181  0.241822   0.519147  0.875956   \n",
       "22    0.915485  0.014012  0.300176  0.984712  0.149867   0.369192  0.839580   \n",
       "23    0.970179  0.029972  0.075803  0.992373  0.249612   0.174980  0.809574   \n",
       "24    0.703674  0.024805  0.483388  0.942374  0.201641   0.541101  0.873538   \n",
       "25    0.496693  0.033356  0.476543  0.967457  0.284449   0.526874  0.848356   \n",
       "26    0.664369  0.037880  0.391938  0.983216  0.358222   0.376162  0.710437   \n",
       "27    0.545713  0.047470  0.403515  0.967187  0.338069   0.641355  0.824254   \n",
       "28    0.580868  0.144039  0.470603  0.922717  0.318293   0.660425  0.764894   \n",
       "29    0.517697  0.025042  0.417609  0.985299  0.266928   0.713344  0.939562   \n",
       "30    0.335720  0.023923  0.549403  0.973233  0.280983   0.802107  0.926264   \n",
       "31    0.637265  0.083383  0.523254  0.956735  0.235938   0.564445  0.904321   \n",
       "32    0.566843  0.057006  0.302547  0.969938  0.190544   0.467254  0.667827   \n",
       "33    0.808999  0.160524  0.393400  0.900525  0.308438   0.427276  0.690313   \n",
       "34    0.541247  0.045146  0.554890  0.966727  0.393443   0.632178  0.804292   \n",
       "35    0.340724  0.075152  0.553361  0.953774  0.254169   0.692432  0.881569   \n",
       "36    0.770757  0.046715  0.311987  0.951744  0.227449   0.347103  0.831067   \n",
       "37    0.844538  0.068139  0.384597  0.980951  0.339410   0.266695  0.599212   \n",
       "38    0.876269  0.024556  0.290802  0.981730  0.154363   0.298306  0.918049   \n",
       "39    0.479423  0.054954  0.461462  0.926550  0.250817   0.635307  0.868913   \n",
       "40    0.929424  0.044509  0.124775  0.979052  0.147182   0.271840  0.727184   \n",
       "41    0.626385  0.036901  0.436730  0.947897  0.161460   0.553129  0.887237   \n",
       "42    0.603533  0.098974  0.363759  0.937360  0.428746   0.502110  0.786136   \n",
       "43    0.959928  0.023770  0.082784  0.976976  0.171437   0.103002  0.763975   \n",
       "44    0.991440  0.016098  0.018819  0.995489  0.054577   0.039426  0.851792   \n",
       "45    0.655170  0.048158  0.444942  0.969826  0.275901   0.420291  0.733714   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "5             1  \n",
       "6             1  \n",
       "7             1  \n",
       "8             1  \n",
       "9             1  \n",
       "10            1  \n",
       "11            1  \n",
       "12            1  \n",
       "13            1  \n",
       "14            1  \n",
       "15            1  \n",
       "16            1  \n",
       "17            1  \n",
       "18            1  \n",
       "19            1  \n",
       "20            1  \n",
       "21            1  \n",
       "22            1  \n",
       "23            1  \n",
       "24            1  \n",
       "25            1  \n",
       "26            1  \n",
       "27            1  \n",
       "28            1  \n",
       "29            1  \n",
       "30            1  \n",
       "31            1  \n",
       "32            1  \n",
       "33            1  \n",
       "34            1  \n",
       "35            1  \n",
       "36            1  \n",
       "37            1  \n",
       "38            1  \n",
       "39            1  \n",
       "40            1  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[46 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.237651</td>\n",
       "      <td>0.729190</td>\n",
       "      <td>0.605940</td>\n",
       "      <td>0.679013</td>\n",
       "      <td>0.451559</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.325828</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.603134</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148673</td>\n",
       "      <td>0.192801</td>\n",
       "      <td>0.825429</td>\n",
       "      <td>0.050491</td>\n",
       "      <td>0.310266</td>\n",
       "      <td>0.977595</td>\n",
       "      <td>0.230127</td>\n",
       "      <td>0.581230</td>\n",
       "      <td>0.804154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.369828</td>\n",
       "      <td>0.930027</td>\n",
       "      <td>0.800286</td>\n",
       "      <td>0.798110</td>\n",
       "      <td>0.131688</td>\n",
       "      <td>0.677325</td>\n",
       "      <td>0.204672</td>\n",
       "      <td>0.720663</td>\n",
       "      <td>0.176141</td>\n",
       "      <td>0.793106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>0.901164</td>\n",
       "      <td>0.038393</td>\n",
       "      <td>0.161979</td>\n",
       "      <td>0.969544</td>\n",
       "      <td>0.071375</td>\n",
       "      <td>0.195901</td>\n",
       "      <td>0.966575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.308023</td>\n",
       "      <td>0.429953</td>\n",
       "      <td>0.721172</td>\n",
       "      <td>0.357352</td>\n",
       "      <td>0.854355</td>\n",
       "      <td>0.366576</td>\n",
       "      <td>0.640218</td>\n",
       "      <td>0.522044</td>\n",
       "      <td>0.555432</td>\n",
       "      <td>0.724144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469516</td>\n",
       "      <td>0.646849</td>\n",
       "      <td>0.522713</td>\n",
       "      <td>0.059618</td>\n",
       "      <td>0.368588</td>\n",
       "      <td>0.934507</td>\n",
       "      <td>0.193555</td>\n",
       "      <td>0.403459</td>\n",
       "      <td>0.640048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.347061</td>\n",
       "      <td>0.700261</td>\n",
       "      <td>0.743680</td>\n",
       "      <td>0.679450</td>\n",
       "      <td>0.525843</td>\n",
       "      <td>0.636494</td>\n",
       "      <td>0.436951</td>\n",
       "      <td>0.270847</td>\n",
       "      <td>0.871431</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426665</td>\n",
       "      <td>0.185127</td>\n",
       "      <td>0.947163</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>0.197161</td>\n",
       "      <td>0.973220</td>\n",
       "      <td>0.187290</td>\n",
       "      <td>0.229531</td>\n",
       "      <td>0.790565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.379348</td>\n",
       "      <td>0.485735</td>\n",
       "      <td>0.902474</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.758239</td>\n",
       "      <td>0.409764</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.865734</td>\n",
       "      <td>0.453437</td>\n",
       "      <td>0.915479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419119</td>\n",
       "      <td>0.454304</td>\n",
       "      <td>0.777617</td>\n",
       "      <td>0.050305</td>\n",
       "      <td>0.337532</td>\n",
       "      <td>0.971554</td>\n",
       "      <td>0.229113</td>\n",
       "      <td>0.700907</td>\n",
       "      <td>0.792750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.565790</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.707079</td>\n",
       "      <td>0.383006</td>\n",
       "      <td>0.587917</td>\n",
       "      <td>0.707839</td>\n",
       "      <td>0.521907</td>\n",
       "      <td>0.559898</td>\n",
       "      <td>0.365442</td>\n",
       "      <td>0.732810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242042</td>\n",
       "      <td>0.311391</td>\n",
       "      <td>0.489828</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.547730</td>\n",
       "      <td>0.906325</td>\n",
       "      <td>0.278722</td>\n",
       "      <td>0.576741</td>\n",
       "      <td>0.843096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.629074</td>\n",
       "      <td>0.651405</td>\n",
       "      <td>0.846473</td>\n",
       "      <td>0.486864</td>\n",
       "      <td>0.688865</td>\n",
       "      <td>0.547808</td>\n",
       "      <td>0.468479</td>\n",
       "      <td>0.738856</td>\n",
       "      <td>0.311282</td>\n",
       "      <td>0.838537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149459</td>\n",
       "      <td>0.255429</td>\n",
       "      <td>0.717460</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>0.489715</td>\n",
       "      <td>0.967372</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.543963</td>\n",
       "      <td>0.834492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.396178</td>\n",
       "      <td>0.655764</td>\n",
       "      <td>0.741329</td>\n",
       "      <td>0.518046</td>\n",
       "      <td>0.294009</td>\n",
       "      <td>0.785328</td>\n",
       "      <td>0.229334</td>\n",
       "      <td>0.434012</td>\n",
       "      <td>0.475705</td>\n",
       "      <td>0.651981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.140574</td>\n",
       "      <td>0.842636</td>\n",
       "      <td>0.065218</td>\n",
       "      <td>0.322324</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.219894</td>\n",
       "      <td>0.238557</td>\n",
       "      <td>0.795408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.386981</td>\n",
       "      <td>0.582482</td>\n",
       "      <td>0.625047</td>\n",
       "      <td>0.629923</td>\n",
       "      <td>0.651290</td>\n",
       "      <td>0.848843</td>\n",
       "      <td>0.370622</td>\n",
       "      <td>0.543530</td>\n",
       "      <td>0.454566</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068059</td>\n",
       "      <td>0.287613</td>\n",
       "      <td>0.503656</td>\n",
       "      <td>0.042003</td>\n",
       "      <td>0.446778</td>\n",
       "      <td>0.961520</td>\n",
       "      <td>0.322456</td>\n",
       "      <td>0.674538</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.421718</td>\n",
       "      <td>0.598622</td>\n",
       "      <td>0.631353</td>\n",
       "      <td>0.490685</td>\n",
       "      <td>0.685284</td>\n",
       "      <td>0.695042</td>\n",
       "      <td>0.515173</td>\n",
       "      <td>0.594262</td>\n",
       "      <td>0.538878</td>\n",
       "      <td>0.565192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196591</td>\n",
       "      <td>0.499814</td>\n",
       "      <td>0.411801</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>0.540863</td>\n",
       "      <td>0.941313</td>\n",
       "      <td>0.302134</td>\n",
       "      <td>0.626628</td>\n",
       "      <td>0.810349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.326387</td>\n",
       "      <td>0.552856</td>\n",
       "      <td>0.702903</td>\n",
       "      <td>0.565050</td>\n",
       "      <td>0.578337</td>\n",
       "      <td>0.767844</td>\n",
       "      <td>0.393953</td>\n",
       "      <td>0.367529</td>\n",
       "      <td>0.613828</td>\n",
       "      <td>0.232676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104450</td>\n",
       "      <td>0.345316</td>\n",
       "      <td>0.550411</td>\n",
       "      <td>0.043712</td>\n",
       "      <td>0.493269</td>\n",
       "      <td>0.944693</td>\n",
       "      <td>0.369761</td>\n",
       "      <td>0.698894</td>\n",
       "      <td>0.924273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.241876</td>\n",
       "      <td>0.848357</td>\n",
       "      <td>0.705584</td>\n",
       "      <td>0.574364</td>\n",
       "      <td>0.349749</td>\n",
       "      <td>0.735385</td>\n",
       "      <td>0.359188</td>\n",
       "      <td>0.468625</td>\n",
       "      <td>0.377813</td>\n",
       "      <td>0.572099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208978</td>\n",
       "      <td>0.131762</td>\n",
       "      <td>0.847136</td>\n",
       "      <td>0.067188</td>\n",
       "      <td>0.232569</td>\n",
       "      <td>0.970082</td>\n",
       "      <td>0.161966</td>\n",
       "      <td>0.342731</td>\n",
       "      <td>0.801249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.358768</td>\n",
       "      <td>0.478006</td>\n",
       "      <td>0.881538</td>\n",
       "      <td>0.159423</td>\n",
       "      <td>0.966694</td>\n",
       "      <td>0.138005</td>\n",
       "      <td>0.759205</td>\n",
       "      <td>0.572877</td>\n",
       "      <td>0.385154</td>\n",
       "      <td>0.950680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757911</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.522315</td>\n",
       "      <td>0.072455</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>0.235814</td>\n",
       "      <td>0.782290</td>\n",
       "      <td>0.698369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.267484</td>\n",
       "      <td>0.629719</td>\n",
       "      <td>0.694270</td>\n",
       "      <td>0.572713</td>\n",
       "      <td>0.520980</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.386335</td>\n",
       "      <td>0.525136</td>\n",
       "      <td>0.541098</td>\n",
       "      <td>0.393961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122530</td>\n",
       "      <td>0.139119</td>\n",
       "      <td>0.771875</td>\n",
       "      <td>0.042115</td>\n",
       "      <td>0.259245</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.349711</td>\n",
       "      <td>0.417583</td>\n",
       "      <td>0.837428</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.276822</td>\n",
       "      <td>0.604952</td>\n",
       "      <td>0.607718</td>\n",
       "      <td>0.339509</td>\n",
       "      <td>0.459687</td>\n",
       "      <td>0.573373</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>0.285459</td>\n",
       "      <td>0.802846</td>\n",
       "      <td>0.390080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254628</td>\n",
       "      <td>0.262595</td>\n",
       "      <td>0.901479</td>\n",
       "      <td>0.052275</td>\n",
       "      <td>0.158825</td>\n",
       "      <td>0.991856</td>\n",
       "      <td>0.279524</td>\n",
       "      <td>0.213771</td>\n",
       "      <td>0.641858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.293833</td>\n",
       "      <td>0.528527</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>0.164991</td>\n",
       "      <td>0.846099</td>\n",
       "      <td>0.129798</td>\n",
       "      <td>0.434478</td>\n",
       "      <td>0.533788</td>\n",
       "      <td>0.518813</td>\n",
       "      <td>0.879708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707222</td>\n",
       "      <td>0.332451</td>\n",
       "      <td>0.883531</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.316914</td>\n",
       "      <td>0.974404</td>\n",
       "      <td>0.163585</td>\n",
       "      <td>0.293853</td>\n",
       "      <td>0.579762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.545178</td>\n",
       "      <td>0.515135</td>\n",
       "      <td>0.865187</td>\n",
       "      <td>0.292131</td>\n",
       "      <td>0.804161</td>\n",
       "      <td>0.581020</td>\n",
       "      <td>0.673568</td>\n",
       "      <td>0.767693</td>\n",
       "      <td>0.264642</td>\n",
       "      <td>0.889747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.342624</td>\n",
       "      <td>0.446285</td>\n",
       "      <td>0.080784</td>\n",
       "      <td>0.523273</td>\n",
       "      <td>0.965441</td>\n",
       "      <td>0.261894</td>\n",
       "      <td>0.559238</td>\n",
       "      <td>0.741636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.666762</td>\n",
       "      <td>0.600657</td>\n",
       "      <td>0.868512</td>\n",
       "      <td>0.300797</td>\n",
       "      <td>0.520435</td>\n",
       "      <td>0.360254</td>\n",
       "      <td>0.605129</td>\n",
       "      <td>0.738195</td>\n",
       "      <td>0.373125</td>\n",
       "      <td>0.941136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360360</td>\n",
       "      <td>0.353803</td>\n",
       "      <td>0.892824</td>\n",
       "      <td>0.055943</td>\n",
       "      <td>0.206610</td>\n",
       "      <td>0.959414</td>\n",
       "      <td>0.235307</td>\n",
       "      <td>0.297463</td>\n",
       "      <td>0.851062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.442381</td>\n",
       "      <td>0.612713</td>\n",
       "      <td>0.767811</td>\n",
       "      <td>0.526242</td>\n",
       "      <td>0.588791</td>\n",
       "      <td>0.717802</td>\n",
       "      <td>0.383762</td>\n",
       "      <td>0.473340</td>\n",
       "      <td>0.508712</td>\n",
       "      <td>0.579058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155236</td>\n",
       "      <td>0.364472</td>\n",
       "      <td>0.623545</td>\n",
       "      <td>0.071723</td>\n",
       "      <td>0.397309</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.281827</td>\n",
       "      <td>0.598153</td>\n",
       "      <td>0.874835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.251392</td>\n",
       "      <td>0.643047</td>\n",
       "      <td>0.558224</td>\n",
       "      <td>0.459966</td>\n",
       "      <td>0.205370</td>\n",
       "      <td>0.810277</td>\n",
       "      <td>0.480421</td>\n",
       "      <td>0.560169</td>\n",
       "      <td>0.508363</td>\n",
       "      <td>0.678783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257933</td>\n",
       "      <td>0.171227</td>\n",
       "      <td>0.901740</td>\n",
       "      <td>0.084697</td>\n",
       "      <td>0.182178</td>\n",
       "      <td>0.964247</td>\n",
       "      <td>0.320172</td>\n",
       "      <td>0.157871</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.361785</td>\n",
       "      <td>0.585448</td>\n",
       "      <td>0.818406</td>\n",
       "      <td>0.193644</td>\n",
       "      <td>0.767002</td>\n",
       "      <td>0.313980</td>\n",
       "      <td>0.330855</td>\n",
       "      <td>0.761924</td>\n",
       "      <td>0.484341</td>\n",
       "      <td>0.919625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491360</td>\n",
       "      <td>0.429384</td>\n",
       "      <td>0.632366</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.627392</td>\n",
       "      <td>0.971712</td>\n",
       "      <td>0.213025</td>\n",
       "      <td>0.670464</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.593003</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.821441</td>\n",
       "      <td>0.412521</td>\n",
       "      <td>0.690143</td>\n",
       "      <td>0.606098</td>\n",
       "      <td>0.695898</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.509343</td>\n",
       "      <td>0.848578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347498</td>\n",
       "      <td>0.254766</td>\n",
       "      <td>0.706167</td>\n",
       "      <td>0.034233</td>\n",
       "      <td>0.396360</td>\n",
       "      <td>0.974181</td>\n",
       "      <td>0.241822</td>\n",
       "      <td>0.519147</td>\n",
       "      <td>0.875956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.350108</td>\n",
       "      <td>0.718883</td>\n",
       "      <td>0.871888</td>\n",
       "      <td>0.381674</td>\n",
       "      <td>0.683797</td>\n",
       "      <td>0.423932</td>\n",
       "      <td>0.516243</td>\n",
       "      <td>0.748492</td>\n",
       "      <td>0.474272</td>\n",
       "      <td>0.904725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399785</td>\n",
       "      <td>0.152140</td>\n",
       "      <td>0.915485</td>\n",
       "      <td>0.014012</td>\n",
       "      <td>0.300176</td>\n",
       "      <td>0.984712</td>\n",
       "      <td>0.149867</td>\n",
       "      <td>0.369192</td>\n",
       "      <td>0.839580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.278718</td>\n",
       "      <td>0.480350</td>\n",
       "      <td>0.483718</td>\n",
       "      <td>0.490582</td>\n",
       "      <td>0.418499</td>\n",
       "      <td>0.848597</td>\n",
       "      <td>0.357699</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.918111</td>\n",
       "      <td>0.124104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164162</td>\n",
       "      <td>0.230193</td>\n",
       "      <td>0.970179</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.992373</td>\n",
       "      <td>0.249612</td>\n",
       "      <td>0.174980</td>\n",
       "      <td>0.809574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.272893</td>\n",
       "      <td>0.573101</td>\n",
       "      <td>0.778401</td>\n",
       "      <td>0.281077</td>\n",
       "      <td>0.878818</td>\n",
       "      <td>0.207817</td>\n",
       "      <td>0.308155</td>\n",
       "      <td>0.730882</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.937671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462338</td>\n",
       "      <td>0.589982</td>\n",
       "      <td>0.703674</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>0.483388</td>\n",
       "      <td>0.942374</td>\n",
       "      <td>0.201641</td>\n",
       "      <td>0.541101</td>\n",
       "      <td>0.873538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.440783</td>\n",
       "      <td>0.519928</td>\n",
       "      <td>0.879034</td>\n",
       "      <td>0.385832</td>\n",
       "      <td>0.811544</td>\n",
       "      <td>0.522823</td>\n",
       "      <td>0.646083</td>\n",
       "      <td>0.668829</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.882714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187846</td>\n",
       "      <td>0.351444</td>\n",
       "      <td>0.496693</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.967457</td>\n",
       "      <td>0.284449</td>\n",
       "      <td>0.526874</td>\n",
       "      <td>0.848356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.373736</td>\n",
       "      <td>0.375571</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>0.444945</td>\n",
       "      <td>0.530980</td>\n",
       "      <td>0.800940</td>\n",
       "      <td>0.562345</td>\n",
       "      <td>0.413157</td>\n",
       "      <td>0.729969</td>\n",
       "      <td>0.421593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260799</td>\n",
       "      <td>0.353561</td>\n",
       "      <td>0.664369</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.391938</td>\n",
       "      <td>0.983216</td>\n",
       "      <td>0.358222</td>\n",
       "      <td>0.376162</td>\n",
       "      <td>0.710437</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.290136</td>\n",
       "      <td>0.524097</td>\n",
       "      <td>0.813150</td>\n",
       "      <td>0.391404</td>\n",
       "      <td>0.768282</td>\n",
       "      <td>0.693356</td>\n",
       "      <td>0.577678</td>\n",
       "      <td>0.611822</td>\n",
       "      <td>0.362581</td>\n",
       "      <td>0.718703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137095</td>\n",
       "      <td>0.285555</td>\n",
       "      <td>0.545713</td>\n",
       "      <td>0.047470</td>\n",
       "      <td>0.403515</td>\n",
       "      <td>0.967187</td>\n",
       "      <td>0.338069</td>\n",
       "      <td>0.641355</td>\n",
       "      <td>0.824254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.377685</td>\n",
       "      <td>0.609953</td>\n",
       "      <td>0.785926</td>\n",
       "      <td>0.541302</td>\n",
       "      <td>0.486657</td>\n",
       "      <td>0.756326</td>\n",
       "      <td>0.457495</td>\n",
       "      <td>0.490369</td>\n",
       "      <td>0.383031</td>\n",
       "      <td>0.588471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168707</td>\n",
       "      <td>0.419904</td>\n",
       "      <td>0.580868</td>\n",
       "      <td>0.144039</td>\n",
       "      <td>0.470603</td>\n",
       "      <td>0.922717</td>\n",
       "      <td>0.318293</td>\n",
       "      <td>0.660425</td>\n",
       "      <td>0.764894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.539308</td>\n",
       "      <td>0.784577</td>\n",
       "      <td>0.815233</td>\n",
       "      <td>0.634706</td>\n",
       "      <td>0.642810</td>\n",
       "      <td>0.820022</td>\n",
       "      <td>0.422096</td>\n",
       "      <td>0.761478</td>\n",
       "      <td>0.271190</td>\n",
       "      <td>0.846043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104493</td>\n",
       "      <td>0.251409</td>\n",
       "      <td>0.517697</td>\n",
       "      <td>0.025042</td>\n",
       "      <td>0.417609</td>\n",
       "      <td>0.985299</td>\n",
       "      <td>0.266928</td>\n",
       "      <td>0.713344</td>\n",
       "      <td>0.939562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.624636</td>\n",
       "      <td>0.769732</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.475828</td>\n",
       "      <td>0.760051</td>\n",
       "      <td>0.676930</td>\n",
       "      <td>0.597489</td>\n",
       "      <td>0.883766</td>\n",
       "      <td>0.230628</td>\n",
       "      <td>0.936862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108385</td>\n",
       "      <td>0.371633</td>\n",
       "      <td>0.335720</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.549403</td>\n",
       "      <td>0.973233</td>\n",
       "      <td>0.280983</td>\n",
       "      <td>0.802107</td>\n",
       "      <td>0.926264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.404288</td>\n",
       "      <td>0.727831</td>\n",
       "      <td>0.821608</td>\n",
       "      <td>0.504695</td>\n",
       "      <td>0.496421</td>\n",
       "      <td>0.652849</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>0.692899</td>\n",
       "      <td>0.295981</td>\n",
       "      <td>0.834984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132787</td>\n",
       "      <td>0.257501</td>\n",
       "      <td>0.637265</td>\n",
       "      <td>0.083383</td>\n",
       "      <td>0.523254</td>\n",
       "      <td>0.956735</td>\n",
       "      <td>0.235938</td>\n",
       "      <td>0.564445</td>\n",
       "      <td>0.904321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.336444</td>\n",
       "      <td>0.655455</td>\n",
       "      <td>0.800771</td>\n",
       "      <td>0.444520</td>\n",
       "      <td>0.886358</td>\n",
       "      <td>0.543182</td>\n",
       "      <td>0.747064</td>\n",
       "      <td>0.688248</td>\n",
       "      <td>0.286469</td>\n",
       "      <td>0.828451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435768</td>\n",
       "      <td>0.448382</td>\n",
       "      <td>0.566843</td>\n",
       "      <td>0.057006</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.969938</td>\n",
       "      <td>0.190544</td>\n",
       "      <td>0.467254</td>\n",
       "      <td>0.667827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.544371</td>\n",
       "      <td>0.554261</td>\n",
       "      <td>0.781406</td>\n",
       "      <td>0.335528</td>\n",
       "      <td>0.386157</td>\n",
       "      <td>0.578926</td>\n",
       "      <td>0.560789</td>\n",
       "      <td>0.698975</td>\n",
       "      <td>0.389099</td>\n",
       "      <td>0.902535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258452</td>\n",
       "      <td>0.364763</td>\n",
       "      <td>0.808999</td>\n",
       "      <td>0.160524</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.900525</td>\n",
       "      <td>0.308438</td>\n",
       "      <td>0.427276</td>\n",
       "      <td>0.690313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.485811</td>\n",
       "      <td>0.486084</td>\n",
       "      <td>0.712341</td>\n",
       "      <td>0.395553</td>\n",
       "      <td>0.783639</td>\n",
       "      <td>0.659570</td>\n",
       "      <td>0.562680</td>\n",
       "      <td>0.608285</td>\n",
       "      <td>0.520648</td>\n",
       "      <td>0.773193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248074</td>\n",
       "      <td>0.521211</td>\n",
       "      <td>0.541247</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>0.554890</td>\n",
       "      <td>0.966727</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.632178</td>\n",
       "      <td>0.804292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.517371</td>\n",
       "      <td>0.507572</td>\n",
       "      <td>0.731585</td>\n",
       "      <td>0.417628</td>\n",
       "      <td>0.735392</td>\n",
       "      <td>0.727789</td>\n",
       "      <td>0.652663</td>\n",
       "      <td>0.651508</td>\n",
       "      <td>0.417071</td>\n",
       "      <td>0.601563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133073</td>\n",
       "      <td>0.444345</td>\n",
       "      <td>0.340724</td>\n",
       "      <td>0.075152</td>\n",
       "      <td>0.553361</td>\n",
       "      <td>0.953774</td>\n",
       "      <td>0.254169</td>\n",
       "      <td>0.692432</td>\n",
       "      <td>0.881569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.342803</td>\n",
       "      <td>0.457861</td>\n",
       "      <td>0.818594</td>\n",
       "      <td>0.389036</td>\n",
       "      <td>0.546529</td>\n",
       "      <td>0.653052</td>\n",
       "      <td>0.624249</td>\n",
       "      <td>0.551393</td>\n",
       "      <td>0.581221</td>\n",
       "      <td>0.634144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311193</td>\n",
       "      <td>0.302991</td>\n",
       "      <td>0.770757</td>\n",
       "      <td>0.046715</td>\n",
       "      <td>0.311987</td>\n",
       "      <td>0.951744</td>\n",
       "      <td>0.227449</td>\n",
       "      <td>0.347103</td>\n",
       "      <td>0.831067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.406139</td>\n",
       "      <td>0.429788</td>\n",
       "      <td>0.675112</td>\n",
       "      <td>0.392159</td>\n",
       "      <td>0.676224</td>\n",
       "      <td>0.620942</td>\n",
       "      <td>0.470348</td>\n",
       "      <td>0.442154</td>\n",
       "      <td>0.633134</td>\n",
       "      <td>0.586738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404938</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.844538</td>\n",
       "      <td>0.068139</td>\n",
       "      <td>0.384597</td>\n",
       "      <td>0.980951</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.266695</td>\n",
       "      <td>0.599212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.525268</td>\n",
       "      <td>0.916472</td>\n",
       "      <td>0.848064</td>\n",
       "      <td>0.635298</td>\n",
       "      <td>0.391362</td>\n",
       "      <td>0.636274</td>\n",
       "      <td>0.427829</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.256043</td>\n",
       "      <td>0.894241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144845</td>\n",
       "      <td>0.068485</td>\n",
       "      <td>0.876269</td>\n",
       "      <td>0.024556</td>\n",
       "      <td>0.290802</td>\n",
       "      <td>0.981730</td>\n",
       "      <td>0.154363</td>\n",
       "      <td>0.298306</td>\n",
       "      <td>0.918049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.530529</td>\n",
       "      <td>0.687292</td>\n",
       "      <td>0.678219</td>\n",
       "      <td>0.484790</td>\n",
       "      <td>0.655578</td>\n",
       "      <td>0.724860</td>\n",
       "      <td>0.413336</td>\n",
       "      <td>0.688887</td>\n",
       "      <td>0.352727</td>\n",
       "      <td>0.702790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117270</td>\n",
       "      <td>0.275958</td>\n",
       "      <td>0.479423</td>\n",
       "      <td>0.054954</td>\n",
       "      <td>0.461462</td>\n",
       "      <td>0.926550</td>\n",
       "      <td>0.250817</td>\n",
       "      <td>0.635307</td>\n",
       "      <td>0.868913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.404238</td>\n",
       "      <td>0.619757</td>\n",
       "      <td>0.825180</td>\n",
       "      <td>0.566859</td>\n",
       "      <td>0.455378</td>\n",
       "      <td>0.644695</td>\n",
       "      <td>0.496091</td>\n",
       "      <td>0.450806</td>\n",
       "      <td>0.738434</td>\n",
       "      <td>0.533492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306473</td>\n",
       "      <td>0.119865</td>\n",
       "      <td>0.929424</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.124775</td>\n",
       "      <td>0.979052</td>\n",
       "      <td>0.147182</td>\n",
       "      <td>0.271840</td>\n",
       "      <td>0.727184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.405292</td>\n",
       "      <td>0.686501</td>\n",
       "      <td>0.796409</td>\n",
       "      <td>0.654468</td>\n",
       "      <td>0.423074</td>\n",
       "      <td>0.687883</td>\n",
       "      <td>0.381280</td>\n",
       "      <td>0.523034</td>\n",
       "      <td>0.299717</td>\n",
       "      <td>0.562845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122687</td>\n",
       "      <td>0.138823</td>\n",
       "      <td>0.626385</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.436730</td>\n",
       "      <td>0.947897</td>\n",
       "      <td>0.161460</td>\n",
       "      <td>0.553129</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.387901</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>0.721091</td>\n",
       "      <td>0.439910</td>\n",
       "      <td>0.743987</td>\n",
       "      <td>0.718245</td>\n",
       "      <td>0.691398</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>0.522004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237433</td>\n",
       "      <td>0.341949</td>\n",
       "      <td>0.603533</td>\n",
       "      <td>0.098974</td>\n",
       "      <td>0.363759</td>\n",
       "      <td>0.937360</td>\n",
       "      <td>0.428746</td>\n",
       "      <td>0.502110</td>\n",
       "      <td>0.786136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.269977</td>\n",
       "      <td>0.563076</td>\n",
       "      <td>0.882795</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.261320</td>\n",
       "      <td>0.567126</td>\n",
       "      <td>0.321584</td>\n",
       "      <td>0.521886</td>\n",
       "      <td>0.620781</td>\n",
       "      <td>0.783342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541540</td>\n",
       "      <td>0.166126</td>\n",
       "      <td>0.959928</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>0.082784</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>0.171437</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.162127</td>\n",
       "      <td>0.533088</td>\n",
       "      <td>0.618783</td>\n",
       "      <td>0.534847</td>\n",
       "      <td>0.345407</td>\n",
       "      <td>0.753220</td>\n",
       "      <td>0.317351</td>\n",
       "      <td>0.203723</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.280865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442454</td>\n",
       "      <td>0.192055</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.016098</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.995489</td>\n",
       "      <td>0.054577</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.851792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.375650</td>\n",
       "      <td>0.552342</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.493070</td>\n",
       "      <td>0.679458</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.577510</td>\n",
       "      <td>0.493439</td>\n",
       "      <td>0.538392</td>\n",
       "      <td>0.694640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279321</td>\n",
       "      <td>0.302901</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.444942</td>\n",
       "      <td>0.969826</td>\n",
       "      <td>0.275901</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>0.733714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.734529Z",
     "start_time": "2024-10-01T22:46:36.731522Z"
    }
   },
   "cell_type": "code",
   "source": "synth_min_df['TUMOR_STAGE'].value_counts()",
   "id": "e2cd7f189af8ea3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "1    46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.777223Z",
     "start_time": "2024-10-01T22:46:36.740841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "minority_dataset['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE']\n",
    "minority_dataset.describe()"
   ],
   "id": "7fbd0d70ad5d778",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         GE_SPOP   GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2   GE_NEIL1  \\\n",
       "count  99.000000  99.000000  99.000000   99.000000  99.000000  99.000000   \n",
       "mean    0.430408   0.549413   0.691829    0.422083   0.575401   0.621343   \n",
       "std     0.123333   0.125945   0.172995    0.127049   0.170970   0.145871   \n",
       "min     0.035294   0.186487   0.145150    0.104812   0.000000   0.295086   \n",
       "25%     0.355857   0.491945   0.616651    0.348417   0.465805   0.526727   \n",
       "50%     0.447820   0.572546   0.705212    0.414669   0.569090   0.630431   \n",
       "75%     0.505466   0.629884   0.824139    0.504164   0.711534   0.708140   \n",
       "max     0.760484   0.872697   0.990674    0.704124   0.907982   0.950996   \n",
       "\n",
       "       GE_PITPNM2     GE_ATM    GE_EMG1    GE_ETV3  ...   DM_NEIL1  \\\n",
       "count   99.000000  99.000000  99.000000  99.000000  ...  99.000000   \n",
       "mean     0.511619   0.526449   0.470264   0.640008  ...   0.242277   \n",
       "std      0.175946   0.178469   0.163476   0.232612  ...   0.132837   \n",
       "min      0.020762   0.091439   0.000000   0.012763  ...   0.034757   \n",
       "25%      0.409490   0.417746   0.366516   0.465799  ...   0.170191   \n",
       "50%      0.517199   0.553874   0.443815   0.718421  ...   0.223858   \n",
       "75%      0.623376   0.648361   0.553523   0.814305  ...   0.275514   \n",
       "max      0.829500   0.843633   0.878978   0.967994  ...   1.000000   \n",
       "\n",
       "       DM_SLC27A4  DM_PITPNM2    DM_PTEN    DM_EMG1    DM_ETV3    DM_BRAF  \\\n",
       "count   99.000000   99.000000  99.000000  99.000000  99.000000  99.000000   \n",
       "mean     0.365601    0.655664   0.078446   0.398783   0.920510   0.245047   \n",
       "std      0.184294    0.139925   0.100675   0.152910   0.039621   0.094361   \n",
       "min      0.000000    0.254443   0.000000   0.072432   0.763054   0.000000   \n",
       "25%      0.223645    0.560492   0.042507   0.283923   0.901510   0.185615   \n",
       "50%      0.359262    0.652253   0.060890   0.405276   0.926367   0.236818   \n",
       "75%      0.495186    0.759650   0.081639   0.508039   0.949720   0.281463   \n",
       "max      0.980276    0.915765   0.882551   0.757773   0.988591   0.541422   \n",
       "\n",
       "       DM_NKX3-1   DM_SALL1  TUMOR_STAGE  \n",
       "count  99.000000  99.000000         99.0  \n",
       "mean    0.476970   0.779241          1.0  \n",
       "std     0.184920   0.126294          0.0  \n",
       "min     0.011983   0.271312          1.0  \n",
       "25%     0.351461   0.719657          1.0  \n",
       "50%     0.493626   0.797874          1.0  \n",
       "75%     0.612733   0.858918          1.0  \n",
       "max     0.876869   0.983930          1.0  \n",
       "\n",
       "[8 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.430408</td>\n",
       "      <td>0.549413</td>\n",
       "      <td>0.691829</td>\n",
       "      <td>0.422083</td>\n",
       "      <td>0.575401</td>\n",
       "      <td>0.621343</td>\n",
       "      <td>0.511619</td>\n",
       "      <td>0.526449</td>\n",
       "      <td>0.470264</td>\n",
       "      <td>0.640008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>0.365601</td>\n",
       "      <td>0.655664</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>0.398783</td>\n",
       "      <td>0.920510</td>\n",
       "      <td>0.245047</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.779241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.125945</td>\n",
       "      <td>0.172995</td>\n",
       "      <td>0.127049</td>\n",
       "      <td>0.170970</td>\n",
       "      <td>0.145871</td>\n",
       "      <td>0.175946</td>\n",
       "      <td>0.178469</td>\n",
       "      <td>0.163476</td>\n",
       "      <td>0.232612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132837</td>\n",
       "      <td>0.184294</td>\n",
       "      <td>0.139925</td>\n",
       "      <td>0.100675</td>\n",
       "      <td>0.152910</td>\n",
       "      <td>0.039621</td>\n",
       "      <td>0.094361</td>\n",
       "      <td>0.184920</td>\n",
       "      <td>0.126294</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.186487</td>\n",
       "      <td>0.145150</td>\n",
       "      <td>0.104812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295086</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.091439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>0.763054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>0.271312</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.355857</td>\n",
       "      <td>0.491945</td>\n",
       "      <td>0.616651</td>\n",
       "      <td>0.348417</td>\n",
       "      <td>0.465805</td>\n",
       "      <td>0.526727</td>\n",
       "      <td>0.409490</td>\n",
       "      <td>0.417746</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>0.223645</td>\n",
       "      <td>0.560492</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>0.283923</td>\n",
       "      <td>0.901510</td>\n",
       "      <td>0.185615</td>\n",
       "      <td>0.351461</td>\n",
       "      <td>0.719657</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.447820</td>\n",
       "      <td>0.572546</td>\n",
       "      <td>0.705212</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>0.630431</td>\n",
       "      <td>0.517199</td>\n",
       "      <td>0.553874</td>\n",
       "      <td>0.443815</td>\n",
       "      <td>0.718421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223858</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.652253</td>\n",
       "      <td>0.060890</td>\n",
       "      <td>0.405276</td>\n",
       "      <td>0.926367</td>\n",
       "      <td>0.236818</td>\n",
       "      <td>0.493626</td>\n",
       "      <td>0.797874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.505466</td>\n",
       "      <td>0.629884</td>\n",
       "      <td>0.824139</td>\n",
       "      <td>0.504164</td>\n",
       "      <td>0.711534</td>\n",
       "      <td>0.708140</td>\n",
       "      <td>0.623376</td>\n",
       "      <td>0.648361</td>\n",
       "      <td>0.553523</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275514</td>\n",
       "      <td>0.495186</td>\n",
       "      <td>0.759650</td>\n",
       "      <td>0.081639</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.949720</td>\n",
       "      <td>0.281463</td>\n",
       "      <td>0.612733</td>\n",
       "      <td>0.858918</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.760484</td>\n",
       "      <td>0.872697</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.704124</td>\n",
       "      <td>0.907982</td>\n",
       "      <td>0.950996</td>\n",
       "      <td>0.829500</td>\n",
       "      <td>0.843633</td>\n",
       "      <td>0.878978</td>\n",
       "      <td>0.967994</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980276</td>\n",
       "      <td>0.915765</td>\n",
       "      <td>0.882551</td>\n",
       "      <td>0.757773</td>\n",
       "      <td>0.988591</td>\n",
       "      <td>0.541422</td>\n",
       "      <td>0.876869</td>\n",
       "      <td>0.983930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:36.858848Z",
     "start_time": "2024-10-01T22:46:36.814404Z"
    }
   },
   "cell_type": "code",
   "source": "synth_min_df.describe()",
   "id": "fca47c368629ba76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         GE_SPOP   GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2   GE_NEIL1  \\\n",
       "count  46.000000  46.000000  46.000000   46.000000  46.000000  46.000000   \n",
       "mean    0.398120   0.606804   0.758727    0.455497   0.595092   0.620059   \n",
       "std     0.117838   0.120087   0.100026    0.138597   0.195017   0.179663   \n",
       "min     0.162127   0.375571   0.483718    0.159423   0.131688   0.129798   \n",
       "25%     0.312614   0.525204   0.703573    0.382007   0.456455   0.558260   \n",
       "50%     0.378517   0.599640   0.779903    0.452456   0.615800   0.656311   \n",
       "75%     0.474953   0.655687   0.824287    0.539689   0.754676   0.733486   \n",
       "max     0.666762   0.930027   0.902474    0.798110   0.966694   0.848843   \n",
       "\n",
       "       GE_PITPNM2     GE_ATM    GE_EMG1    GE_ETV3  ...   DM_NEIL1  \\\n",
       "count   46.000000  46.000000  46.000000  46.000000  ...  46.000000   \n",
       "mean     0.481325   0.571389   0.483104   0.671904  ...   0.272271   \n",
       "std      0.143731   0.173719   0.179598   0.225188  ...   0.161605   \n",
       "min      0.163276   0.159466   0.176141   0.124104  ...   0.056811   \n",
       "25%      0.381900   0.477598   0.363296   0.563432  ...   0.145802   \n",
       "50%      0.469414   0.560034   0.479812   0.710746  ...   0.245058   \n",
       "75%      0.592536   0.715241   0.574774   0.871926  ...   0.389929   \n",
       "max      0.759205   0.883766   0.918111   0.950680  ...   0.757911   \n",
       "\n",
       "       DM_SLC27A4  DM_PITPNM2    DM_PTEN    DM_EMG1    DM_ETV3    DM_BRAF  \\\n",
       "count   46.000000   46.000000  46.000000  46.000000  46.000000  46.000000   \n",
       "mean     0.308870    0.695250   0.053124   0.372789   0.961706   0.249215   \n",
       "std      0.150804    0.184342   0.031314   0.160383   0.022195   0.078609   \n",
       "min      0.035842    0.335720   0.010446   0.018819   0.900525   0.054577   \n",
       "25%      0.192241    0.542364   0.033575   0.293145   0.948859   0.195577   \n",
       "50%      0.302946    0.684021   0.047092   0.392669   0.967414   0.250214   \n",
       "75%      0.364690    0.868986   0.067902   0.481676   0.976333   0.297712   \n",
       "max      0.786444    0.991440   0.160524   0.814319   0.995489   0.428746   \n",
       "\n",
       "       DM_NKX3-1   DM_SALL1  TUMOR_STAGE  \n",
       "count  46.000000  46.000000         46.0  \n",
       "mean    0.464114   0.807312          1.0  \n",
       "std     0.195333   0.091047          0.0  \n",
       "min     0.039426   0.579762          1.0  \n",
       "25%     0.297674   0.764205          1.0  \n",
       "50%     0.510629   0.817301          1.0  \n",
       "75%     0.630790   0.872896          1.0  \n",
       "max     0.802107   0.966575          1.0  \n",
       "\n",
       "[8 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.398120</td>\n",
       "      <td>0.606804</td>\n",
       "      <td>0.758727</td>\n",
       "      <td>0.455497</td>\n",
       "      <td>0.595092</td>\n",
       "      <td>0.620059</td>\n",
       "      <td>0.481325</td>\n",
       "      <td>0.571389</td>\n",
       "      <td>0.483104</td>\n",
       "      <td>0.671904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272271</td>\n",
       "      <td>0.308870</td>\n",
       "      <td>0.695250</td>\n",
       "      <td>0.053124</td>\n",
       "      <td>0.372789</td>\n",
       "      <td>0.961706</td>\n",
       "      <td>0.249215</td>\n",
       "      <td>0.464114</td>\n",
       "      <td>0.807312</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.117838</td>\n",
       "      <td>0.120087</td>\n",
       "      <td>0.100026</td>\n",
       "      <td>0.138597</td>\n",
       "      <td>0.195017</td>\n",
       "      <td>0.179663</td>\n",
       "      <td>0.143731</td>\n",
       "      <td>0.173719</td>\n",
       "      <td>0.179598</td>\n",
       "      <td>0.225188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161605</td>\n",
       "      <td>0.150804</td>\n",
       "      <td>0.184342</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>0.160383</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>0.195333</td>\n",
       "      <td>0.091047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.162127</td>\n",
       "      <td>0.375571</td>\n",
       "      <td>0.483718</td>\n",
       "      <td>0.159423</td>\n",
       "      <td>0.131688</td>\n",
       "      <td>0.129798</td>\n",
       "      <td>0.163276</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.176141</td>\n",
       "      <td>0.124104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>0.335720</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.900525</td>\n",
       "      <td>0.054577</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.579762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.312614</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.703573</td>\n",
       "      <td>0.382007</td>\n",
       "      <td>0.456455</td>\n",
       "      <td>0.558260</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>0.363296</td>\n",
       "      <td>0.563432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145802</td>\n",
       "      <td>0.192241</td>\n",
       "      <td>0.542364</td>\n",
       "      <td>0.033575</td>\n",
       "      <td>0.293145</td>\n",
       "      <td>0.948859</td>\n",
       "      <td>0.195577</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.764205</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.599640</td>\n",
       "      <td>0.779903</td>\n",
       "      <td>0.452456</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.656311</td>\n",
       "      <td>0.469414</td>\n",
       "      <td>0.560034</td>\n",
       "      <td>0.479812</td>\n",
       "      <td>0.710746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245058</td>\n",
       "      <td>0.302946</td>\n",
       "      <td>0.684021</td>\n",
       "      <td>0.047092</td>\n",
       "      <td>0.392669</td>\n",
       "      <td>0.967414</td>\n",
       "      <td>0.250214</td>\n",
       "      <td>0.510629</td>\n",
       "      <td>0.817301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.474953</td>\n",
       "      <td>0.655687</td>\n",
       "      <td>0.824287</td>\n",
       "      <td>0.539689</td>\n",
       "      <td>0.754676</td>\n",
       "      <td>0.733486</td>\n",
       "      <td>0.592536</td>\n",
       "      <td>0.715241</td>\n",
       "      <td>0.574774</td>\n",
       "      <td>0.871926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389929</td>\n",
       "      <td>0.364690</td>\n",
       "      <td>0.868986</td>\n",
       "      <td>0.067902</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.976333</td>\n",
       "      <td>0.297712</td>\n",
       "      <td>0.630790</td>\n",
       "      <td>0.872896</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.666762</td>\n",
       "      <td>0.930027</td>\n",
       "      <td>0.902474</td>\n",
       "      <td>0.798110</td>\n",
       "      <td>0.966694</td>\n",
       "      <td>0.848843</td>\n",
       "      <td>0.759205</td>\n",
       "      <td>0.883766</td>\n",
       "      <td>0.918111</td>\n",
       "      <td>0.950680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757911</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.160524</td>\n",
       "      <td>0.814319</td>\n",
       "      <td>0.995489</td>\n",
       "      <td>0.428746</td>\n",
       "      <td>0.802107</td>\n",
       "      <td>0.966575</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Using PCA to decompose the dataset into 2 principal components**",
   "id": "a7be7ed33dd51704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:46:37.396444Z",
     "start_time": "2024-10-01T22:46:37.129733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Checking if synthetic samples generated are within the permissible values of the original dataset using Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "synth_values = synth_min_df.values\n",
    "original_min_values = minority_dataset.values\n",
    "#Vertically stacking the synthetic samples with the minority class samples\n",
    "stacked_values = np.vstack([original_min_values, synth_values])\n",
    "pca_values = pca.fit_transform(stacked_values)\n",
    "\n",
    "#Plotting the values on a scatterplot\n",
    "original_pca_values = pca_values[: len(original_min_values)]\n",
    "synth_pca_values = pca_values[len(original_min_values): ]\n",
    "\n",
    "plt.scatter(x = original_pca_values[:, 0], y = original_pca_values[:, 1], alpha = 1, c = 'g', label = 'Original_Dataset')\n",
    "plt.scatter(x = synth_pca_values[:, 0], y = synth_pca_values[:, 1], alpha = 0.5, c = 'b', label = 'Synthetic_Dataset')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ],
   "id": "492c0b11748884a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Concatenating the minority class samples with the synthetic samples to get 145 samples**",
   "id": "951f8b9d170cabfc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:51:24.782431Z",
     "start_time": "2024-10-01T22:51:24.777871Z"
    }
   },
   "cell_type": "code",
   "source": "combined_min_df = pd.concat([minority_dataset, synth_min_df], axis = 0)",
   "id": "5579ea949c82685b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:51:28.400384Z",
     "start_time": "2024-10-01T22:51:28.388058Z"
    }
   },
   "cell_type": "code",
   "source": "combined_min_df",
   "id": "891f22a3f9e09103",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "1   0.373498  0.790415   0.648564    0.367439   0.000000  0.638505   \n",
       "4   0.363634  0.624428   0.892508    0.501788   0.472260  0.465397   \n",
       "9   0.418700  0.702095   0.745375    0.430890   0.487548  0.669876   \n",
       "11  0.470982  0.654804   0.799236    0.414669   0.421094  0.578125   \n",
       "15  0.535495  0.574125   0.990674    0.227795   0.686890  0.486809   \n",
       "..       ...       ...        ...         ...        ...       ...   \n",
       "41  0.405292  0.686501   0.796409    0.654468   0.423074  0.687883   \n",
       "42  0.387901  0.607557   0.721091    0.439910   0.743987  0.718245   \n",
       "43  0.269977  0.563076   0.882795    0.351648   0.261320  0.567126   \n",
       "44  0.162127  0.533088   0.618783    0.534847   0.345407  0.753220   \n",
       "45  0.375650  0.552342   0.753281    0.493070   0.679458  0.555305   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "1     0.156215  0.673907  0.384063  0.807230  ...  0.034757    0.000000   \n",
       "4     0.619784  0.562305  0.450817  0.729161  ...  0.182983    0.119817   \n",
       "9     0.441037  0.506928  0.140151  0.867808  ...  0.181650    0.299419   \n",
       "11    0.466010  0.715588  0.355391  0.813532  ...  0.172506    0.200640   \n",
       "15    0.735071  0.832965  0.387468  0.947819  ...  0.330505    0.531055   \n",
       "..         ...       ...       ...       ...  ...       ...         ...   \n",
       "41    0.381280  0.523034  0.299717  0.562845  ...  0.122687    0.138823   \n",
       "42    0.691398  0.545181  0.614593  0.522004  ...  0.237433    0.341949   \n",
       "43    0.321584  0.521886  0.620781  0.783342  ...  0.541540    0.166126   \n",
       "44    0.317351  0.203723  0.888112  0.280865  ...  0.442454    0.192055   \n",
       "45    0.577510  0.493439  0.538392  0.694640  ...  0.279321    0.302901   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "1     0.791218  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329   \n",
       "4     0.907230  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265   \n",
       "9     0.605663  0.045999  0.422079  0.885389  0.238896   0.548180  0.828444   \n",
       "11    0.782428  0.035299  0.285695  0.933832  0.175760   0.263804  0.776006   \n",
       "15    0.546693  0.048915  0.458723  0.868237  0.251513   0.654329  0.667877   \n",
       "..         ...       ...       ...       ...       ...        ...       ...   \n",
       "41    0.626385  0.036901  0.436730  0.947897  0.161460   0.553129  0.887237   \n",
       "42    0.603533  0.098974  0.363759  0.937360  0.428746   0.502110  0.786136   \n",
       "43    0.959928  0.023770  0.082784  0.976976  0.171437   0.103002  0.763975   \n",
       "44    0.991440  0.016098  0.018819  0.995489  0.054577   0.039426  0.851792   \n",
       "45    0.655170  0.048158  0.444942  0.969826  0.275901   0.420291  0.733714   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "1             1  \n",
       "4             1  \n",
       "9             1  \n",
       "11            1  \n",
       "15            1  \n",
       "..          ...  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[145 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181650</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.654804</td>\n",
       "      <td>0.799236</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.421094</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.466010</td>\n",
       "      <td>0.715588</td>\n",
       "      <td>0.355391</td>\n",
       "      <td>0.813532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>0.782428</td>\n",
       "      <td>0.035299</td>\n",
       "      <td>0.285695</td>\n",
       "      <td>0.933832</td>\n",
       "      <td>0.175760</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.776006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.535495</td>\n",
       "      <td>0.574125</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.227795</td>\n",
       "      <td>0.686890</td>\n",
       "      <td>0.486809</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.387468</td>\n",
       "      <td>0.947819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330505</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.458723</td>\n",
       "      <td>0.868237</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.654329</td>\n",
       "      <td>0.667877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.405292</td>\n",
       "      <td>0.686501</td>\n",
       "      <td>0.796409</td>\n",
       "      <td>0.654468</td>\n",
       "      <td>0.423074</td>\n",
       "      <td>0.687883</td>\n",
       "      <td>0.381280</td>\n",
       "      <td>0.523034</td>\n",
       "      <td>0.299717</td>\n",
       "      <td>0.562845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122687</td>\n",
       "      <td>0.138823</td>\n",
       "      <td>0.626385</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.436730</td>\n",
       "      <td>0.947897</td>\n",
       "      <td>0.161460</td>\n",
       "      <td>0.553129</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.387901</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>0.721091</td>\n",
       "      <td>0.439910</td>\n",
       "      <td>0.743987</td>\n",
       "      <td>0.718245</td>\n",
       "      <td>0.691398</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>0.522004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237433</td>\n",
       "      <td>0.341949</td>\n",
       "      <td>0.603533</td>\n",
       "      <td>0.098974</td>\n",
       "      <td>0.363759</td>\n",
       "      <td>0.937360</td>\n",
       "      <td>0.428746</td>\n",
       "      <td>0.502110</td>\n",
       "      <td>0.786136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.269977</td>\n",
       "      <td>0.563076</td>\n",
       "      <td>0.882795</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.261320</td>\n",
       "      <td>0.567126</td>\n",
       "      <td>0.321584</td>\n",
       "      <td>0.521886</td>\n",
       "      <td>0.620781</td>\n",
       "      <td>0.783342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541540</td>\n",
       "      <td>0.166126</td>\n",
       "      <td>0.959928</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>0.082784</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>0.171437</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.162127</td>\n",
       "      <td>0.533088</td>\n",
       "      <td>0.618783</td>\n",
       "      <td>0.534847</td>\n",
       "      <td>0.345407</td>\n",
       "      <td>0.753220</td>\n",
       "      <td>0.317351</td>\n",
       "      <td>0.203723</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.280865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442454</td>\n",
       "      <td>0.192055</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.016098</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.995489</td>\n",
       "      <td>0.054577</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.851792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.375650</td>\n",
       "      <td>0.552342</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.493070</td>\n",
       "      <td>0.679458</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.577510</td>\n",
       "      <td>0.493439</td>\n",
       "      <td>0.538392</td>\n",
       "      <td>0.694640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279321</td>\n",
       "      <td>0.302901</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.444942</td>\n",
       "      <td>0.969826</td>\n",
       "      <td>0.275901</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>0.733714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Finally combining the synthetic samples to the gleason dataset**",
   "id": "145be9902dab4442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:55:33.200279Z",
     "start_time": "2024-10-01T22:55:33.195512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_gleason_df = pd.concat([df1_gleason, synth_min_df], axis = 0)\n",
    "processed_gleason_df['TUMOR_STAGE'].value_counts()"
   ],
   "id": "15ed97d9d3ea44d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "1    145\n",
       "0    145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:55:49.550434Z",
     "start_time": "2024-10-01T22:55:49.537777Z"
    }
   },
   "cell_type": "code",
   "source": "processed_gleason_df",
   "id": "85ea13d4f2e6260c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "1   0.373498  0.790415   0.648564    0.367439   0.000000  0.638505   \n",
       "3   0.433556  0.637116   0.777338    0.559615   0.476760  0.623626   \n",
       "4   0.363634  0.624428   0.892508    0.501788   0.472260  0.465397   \n",
       "5   0.540453  0.630588   0.671140    0.275536   0.662487  0.476065   \n",
       "6   0.506989  0.590670   0.727922    0.255442   0.733651  0.691024   \n",
       "..       ...       ...        ...         ...        ...       ...   \n",
       "41  0.405292  0.686501   0.796409    0.654468   0.423074  0.687883   \n",
       "42  0.387901  0.607557   0.721091    0.439910   0.743987  0.718245   \n",
       "43  0.269977  0.563076   0.882795    0.351648   0.261320  0.567126   \n",
       "44  0.162127  0.533088   0.618783    0.534847   0.345407  0.753220   \n",
       "45  0.375650  0.552342   0.753281    0.493070   0.679458  0.555305   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "1     0.156215  0.673907  0.384063  0.807230  ...  0.034757    0.000000   \n",
       "3     0.415425  0.539748  0.466929  0.599673  ...  0.111825    0.085465   \n",
       "4     0.619784  0.562305  0.450817  0.729161  ...  0.182983    0.119817   \n",
       "5     0.577890  0.824079  0.297729  0.899286  ...  0.228759    0.230212   \n",
       "6     0.398213  0.644989  0.383090  0.713497  ...  0.129223    0.502728   \n",
       "..         ...       ...       ...       ...  ...       ...         ...   \n",
       "41    0.381280  0.523034  0.299717  0.562845  ...  0.122687    0.138823   \n",
       "42    0.691398  0.545181  0.614593  0.522004  ...  0.237433    0.341949   \n",
       "43    0.321584  0.521886  0.620781  0.783342  ...  0.541540    0.166126   \n",
       "44    0.317351  0.203723  0.888112  0.280865  ...  0.442454    0.192055   \n",
       "45    0.577510  0.493439  0.538392  0.694640  ...  0.279321    0.302901   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "1     0.791218  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329   \n",
       "3     0.569465  0.047191  0.101831  0.930270  0.221800   0.257343  1.000000   \n",
       "4     0.907230  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265   \n",
       "5     0.831449  0.040868  0.266358  0.943859  0.178990   0.505571  0.831493   \n",
       "6     0.836422  0.036482  0.328812  0.954992  0.185582   0.391752  0.892649   \n",
       "..         ...       ...       ...       ...       ...        ...       ...   \n",
       "41    0.626385  0.036901  0.436730  0.947897  0.161460   0.553129  0.887237   \n",
       "42    0.603533  0.098974  0.363759  0.937360  0.428746   0.502110  0.786136   \n",
       "43    0.959928  0.023770  0.082784  0.976976  0.171437   0.103002  0.763975   \n",
       "44    0.991440  0.016098  0.018819  0.995489  0.054577   0.039426  0.851792   \n",
       "45    0.655170  0.048158  0.444942  0.969826  0.275901   0.420291  0.733714   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "1             1  \n",
       "3             0  \n",
       "4             1  \n",
       "5             0  \n",
       "6             0  \n",
       "..          ...  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[290 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.476760</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111825</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.662487</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228759</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.405292</td>\n",
       "      <td>0.686501</td>\n",
       "      <td>0.796409</td>\n",
       "      <td>0.654468</td>\n",
       "      <td>0.423074</td>\n",
       "      <td>0.687883</td>\n",
       "      <td>0.381280</td>\n",
       "      <td>0.523034</td>\n",
       "      <td>0.299717</td>\n",
       "      <td>0.562845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122687</td>\n",
       "      <td>0.138823</td>\n",
       "      <td>0.626385</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.436730</td>\n",
       "      <td>0.947897</td>\n",
       "      <td>0.161460</td>\n",
       "      <td>0.553129</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.387901</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>0.721091</td>\n",
       "      <td>0.439910</td>\n",
       "      <td>0.743987</td>\n",
       "      <td>0.718245</td>\n",
       "      <td>0.691398</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>0.522004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237433</td>\n",
       "      <td>0.341949</td>\n",
       "      <td>0.603533</td>\n",
       "      <td>0.098974</td>\n",
       "      <td>0.363759</td>\n",
       "      <td>0.937360</td>\n",
       "      <td>0.428746</td>\n",
       "      <td>0.502110</td>\n",
       "      <td>0.786136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.269977</td>\n",
       "      <td>0.563076</td>\n",
       "      <td>0.882795</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.261320</td>\n",
       "      <td>0.567126</td>\n",
       "      <td>0.321584</td>\n",
       "      <td>0.521886</td>\n",
       "      <td>0.620781</td>\n",
       "      <td>0.783342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541540</td>\n",
       "      <td>0.166126</td>\n",
       "      <td>0.959928</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>0.082784</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>0.171437</td>\n",
       "      <td>0.103002</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.162127</td>\n",
       "      <td>0.533088</td>\n",
       "      <td>0.618783</td>\n",
       "      <td>0.534847</td>\n",
       "      <td>0.345407</td>\n",
       "      <td>0.753220</td>\n",
       "      <td>0.317351</td>\n",
       "      <td>0.203723</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.280865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442454</td>\n",
       "      <td>0.192055</td>\n",
       "      <td>0.991440</td>\n",
       "      <td>0.016098</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.995489</td>\n",
       "      <td>0.054577</td>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.851792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.375650</td>\n",
       "      <td>0.552342</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.493070</td>\n",
       "      <td>0.679458</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.577510</td>\n",
       "      <td>0.493439</td>\n",
       "      <td>0.538392</td>\n",
       "      <td>0.694640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279321</td>\n",
       "      <td>0.302901</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.444942</td>\n",
       "      <td>0.969826</td>\n",
       "      <td>0.275901</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>0.733714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Shuffling the dataset to stop a neural network from capturing patterns associated with the order of the samples**",
   "id": "abadd30d68899210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:58:30.099041Z",
     "start_time": "2024-10-01T22:58:30.081546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_gleason_df = processed_gleason_df.sample(frac = 1).reset_index(drop = True)\n",
    "processed_gleason_df"
   ],
   "id": "a3176da14769317e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "0    0.575079  0.587133   0.922717    0.234906   0.748240  0.472291   \n",
       "1    0.537323  0.651585   0.854902    0.292120   0.490203  0.545553   \n",
       "2    0.386981  0.582482   0.625047    0.629923   0.651290  0.848843   \n",
       "3    0.227620  0.672938   0.530553    0.381428   0.175972  0.824114   \n",
       "4    0.391386  0.539218   0.905188    0.375629   0.809590  0.510654   \n",
       "..        ...       ...        ...         ...        ...       ...   \n",
       "285  0.557279  0.668062   0.716778    0.461685   0.447715  0.665308   \n",
       "286  0.654281  0.436268   0.704380    0.359377   0.698625  0.605353   \n",
       "287  0.406139  0.429788   0.675112    0.392159   0.676224  0.620942   \n",
       "288  0.605638  0.566355   0.847963    0.246558   0.776904  0.595867   \n",
       "289  0.702393  0.671474   0.686184    0.301494   0.590044  0.668380   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "0      0.554445  0.800431  0.209846  0.968074  ...  0.211153    0.510356   \n",
       "1      0.473597  0.775988  0.363620  0.821883  ...  0.308363    1.000000   \n",
       "2      0.370622  0.543530  0.454566  0.398550  ...  0.068059    0.287613   \n",
       "3      0.690394  0.603873  0.581930  0.815111  ...  0.007223    0.288589   \n",
       "4      0.483783  0.744336  0.323944  0.919157  ...  0.300174    0.267242   \n",
       "..          ...       ...       ...       ...  ...       ...         ...   \n",
       "285    0.685921  0.623949  0.386367  0.769133  ...  0.135824    0.344780   \n",
       "286    0.908942  0.456691  0.472161  0.709531  ...  0.263871    0.560218   \n",
       "287    0.470348  0.442154  0.633134  0.586738  ...  0.404938    0.333120   \n",
       "288    0.545772  0.722747  0.337414  0.876418  ...  0.267997    0.563866   \n",
       "289    0.451255  0.595180  0.467640  0.795701  ...  0.155540    0.256553   \n",
       "\n",
       "     DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "0      0.759293  0.071506  0.387823  0.931180  0.163421   0.385668  0.866652   \n",
       "1      0.814994  0.033682  0.302847  0.922685  0.165114   0.386124  0.915704   \n",
       "2      0.503656  0.042003  0.446778  0.961520  0.322456   0.674538  0.912000   \n",
       "3      0.872192  0.069334  0.125432  0.929922  0.232844   0.000000  0.942248   \n",
       "4      0.539900  0.127723  0.409610  0.925696  0.192795   0.520819  0.866199   \n",
       "..          ...       ...       ...       ...       ...        ...       ...   \n",
       "285    0.761523  0.105973  0.165160  0.901504  0.277806   0.284653  0.828625   \n",
       "286    0.649019  0.046348  0.612370  0.925217  0.223666   0.519095  0.723633   \n",
       "287    0.844538  0.068139  0.384597  0.980951  0.339410   0.266695  0.599212   \n",
       "288    0.544564  0.048979  0.530474  0.925479  0.190595   0.626709  0.718878   \n",
       "289    0.590413  0.067871  0.207316  0.920632  0.305769   0.204766  0.980335   \n",
       "\n",
       "     TUMOR_STAGE  \n",
       "0              0  \n",
       "1              0  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  \n",
       "..           ...  \n",
       "285            0  \n",
       "286            0  \n",
       "287            1  \n",
       "288            1  \n",
       "289            0  \n",
       "\n",
       "[290 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575079</td>\n",
       "      <td>0.587133</td>\n",
       "      <td>0.922717</td>\n",
       "      <td>0.234906</td>\n",
       "      <td>0.748240</td>\n",
       "      <td>0.472291</td>\n",
       "      <td>0.554445</td>\n",
       "      <td>0.800431</td>\n",
       "      <td>0.209846</td>\n",
       "      <td>0.968074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211153</td>\n",
       "      <td>0.510356</td>\n",
       "      <td>0.759293</td>\n",
       "      <td>0.071506</td>\n",
       "      <td>0.387823</td>\n",
       "      <td>0.931180</td>\n",
       "      <td>0.163421</td>\n",
       "      <td>0.385668</td>\n",
       "      <td>0.866652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.537323</td>\n",
       "      <td>0.651585</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.292120</td>\n",
       "      <td>0.490203</td>\n",
       "      <td>0.545553</td>\n",
       "      <td>0.473597</td>\n",
       "      <td>0.775988</td>\n",
       "      <td>0.363620</td>\n",
       "      <td>0.821883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.814994</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.302847</td>\n",
       "      <td>0.922685</td>\n",
       "      <td>0.165114</td>\n",
       "      <td>0.386124</td>\n",
       "      <td>0.915704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.386981</td>\n",
       "      <td>0.582482</td>\n",
       "      <td>0.625047</td>\n",
       "      <td>0.629923</td>\n",
       "      <td>0.651290</td>\n",
       "      <td>0.848843</td>\n",
       "      <td>0.370622</td>\n",
       "      <td>0.543530</td>\n",
       "      <td>0.454566</td>\n",
       "      <td>0.398550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068059</td>\n",
       "      <td>0.287613</td>\n",
       "      <td>0.503656</td>\n",
       "      <td>0.042003</td>\n",
       "      <td>0.446778</td>\n",
       "      <td>0.961520</td>\n",
       "      <td>0.322456</td>\n",
       "      <td>0.674538</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.672938</td>\n",
       "      <td>0.530553</td>\n",
       "      <td>0.381428</td>\n",
       "      <td>0.175972</td>\n",
       "      <td>0.824114</td>\n",
       "      <td>0.690394</td>\n",
       "      <td>0.603873</td>\n",
       "      <td>0.581930</td>\n",
       "      <td>0.815111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.288589</td>\n",
       "      <td>0.872192</td>\n",
       "      <td>0.069334</td>\n",
       "      <td>0.125432</td>\n",
       "      <td>0.929922</td>\n",
       "      <td>0.232844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.391386</td>\n",
       "      <td>0.539218</td>\n",
       "      <td>0.905188</td>\n",
       "      <td>0.375629</td>\n",
       "      <td>0.809590</td>\n",
       "      <td>0.510654</td>\n",
       "      <td>0.483783</td>\n",
       "      <td>0.744336</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.919157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300174</td>\n",
       "      <td>0.267242</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.127723</td>\n",
       "      <td>0.409610</td>\n",
       "      <td>0.925696</td>\n",
       "      <td>0.192795</td>\n",
       "      <td>0.520819</td>\n",
       "      <td>0.866199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.557279</td>\n",
       "      <td>0.668062</td>\n",
       "      <td>0.716778</td>\n",
       "      <td>0.461685</td>\n",
       "      <td>0.447715</td>\n",
       "      <td>0.665308</td>\n",
       "      <td>0.685921</td>\n",
       "      <td>0.623949</td>\n",
       "      <td>0.386367</td>\n",
       "      <td>0.769133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135824</td>\n",
       "      <td>0.344780</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.105973</td>\n",
       "      <td>0.165160</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.277806</td>\n",
       "      <td>0.284653</td>\n",
       "      <td>0.828625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.654281</td>\n",
       "      <td>0.436268</td>\n",
       "      <td>0.704380</td>\n",
       "      <td>0.359377</td>\n",
       "      <td>0.698625</td>\n",
       "      <td>0.605353</td>\n",
       "      <td>0.908942</td>\n",
       "      <td>0.456691</td>\n",
       "      <td>0.472161</td>\n",
       "      <td>0.709531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263871</td>\n",
       "      <td>0.560218</td>\n",
       "      <td>0.649019</td>\n",
       "      <td>0.046348</td>\n",
       "      <td>0.612370</td>\n",
       "      <td>0.925217</td>\n",
       "      <td>0.223666</td>\n",
       "      <td>0.519095</td>\n",
       "      <td>0.723633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.406139</td>\n",
       "      <td>0.429788</td>\n",
       "      <td>0.675112</td>\n",
       "      <td>0.392159</td>\n",
       "      <td>0.676224</td>\n",
       "      <td>0.620942</td>\n",
       "      <td>0.470348</td>\n",
       "      <td>0.442154</td>\n",
       "      <td>0.633134</td>\n",
       "      <td>0.586738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404938</td>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.844538</td>\n",
       "      <td>0.068139</td>\n",
       "      <td>0.384597</td>\n",
       "      <td>0.980951</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.266695</td>\n",
       "      <td>0.599212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.605638</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>0.847963</td>\n",
       "      <td>0.246558</td>\n",
       "      <td>0.776904</td>\n",
       "      <td>0.595867</td>\n",
       "      <td>0.545772</td>\n",
       "      <td>0.722747</td>\n",
       "      <td>0.337414</td>\n",
       "      <td>0.876418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267997</td>\n",
       "      <td>0.563866</td>\n",
       "      <td>0.544564</td>\n",
       "      <td>0.048979</td>\n",
       "      <td>0.530474</td>\n",
       "      <td>0.925479</td>\n",
       "      <td>0.190595</td>\n",
       "      <td>0.626709</td>\n",
       "      <td>0.718878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.702393</td>\n",
       "      <td>0.671474</td>\n",
       "      <td>0.686184</td>\n",
       "      <td>0.301494</td>\n",
       "      <td>0.590044</td>\n",
       "      <td>0.668380</td>\n",
       "      <td>0.451255</td>\n",
       "      <td>0.595180</td>\n",
       "      <td>0.467640</td>\n",
       "      <td>0.795701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155540</td>\n",
       "      <td>0.256553</td>\n",
       "      <td>0.590413</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.207316</td>\n",
       "      <td>0.920632</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>0.204766</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Balanced gleason dataset**",
   "id": "43533eeae8e3ec91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T22:58:55.233289Z",
     "start_time": "2024-10-01T22:58:55.226429Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "0    145\n",
       "1    145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47,
   "source": "processed_gleason_df['TUMOR_STAGE'].value_counts()",
   "id": "998b063919993534"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Saving the final dataframe as a .CSV file**",
   "id": "7ea6a58dc3a08867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T23:04:18.240511Z",
     "start_time": "2024-10-01T23:04:18.229983Z"
    }
   },
   "cell_type": "code",
   "source": "processed_gleason_df.to_csv('Balanced_Gleason_Score_Dataset.csv', index = False)",
   "id": "23f9a32037772e88",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76c33060a69460ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
