{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4204fb2ab06d9d68",
   "metadata": {},
   "source": [
    "# **Data Handling on Multi-omics Gleason Score Dataset (Mergefile_top20(4).csv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e0b5e11da185c",
   "metadata": {},
   "source": [
    "**Importing necessary libraries and checking the availability of the GPU**"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T03:48:36.204702Z",
     "start_time": "2024-10-11T03:48:35.260381Z"
    }
   },
   "source": [
    "from xmlrpc.client import Binary\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.src.metrics import BinaryCrossentropy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 23:48:35.357147: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-10 23:48:35.377574: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 23:48:35.377594: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 23:48:35.377609: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 23:48:35.381947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 23:48:35.861899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1c6ab480a6547fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:48:43.323830Z",
     "start_time": "2024-10-11T03:48:43.320939Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "28d538a9d219b812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:48:47.293262Z",
     "start_time": "2024-10-11T03:48:47.251107Z"
    }
   },
   "source": [
    "print(tf.config.list_physical_devices())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 23:48:47.290038: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-10-10 23:48:47.290070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: pop-os\n",
      "2024-10-10 23:48:47.290076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: pop-os\n",
      "2024-10-10 23:48:47.290249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 550.67.0\n",
      "2024-10-10 23:48:47.290266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 550.67.0\n",
      "2024-10-10 23:48:47.290268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 550.67.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5c5a47bfdab1dc48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:48:52.779905Z",
     "start_time": "2024-10-11T03:48:52.777387Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "915c7016bec0ccfe",
   "metadata": {},
   "source": [
    "**Loading in the CSV file using Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6c0e8269e63da3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:27.474986Z",
     "start_time": "2024-10-11T03:49:27.444584Z"
    }
   },
   "source": [
    "#Multi_omics dataset\n",
    "df1 = pd.read_csv(r'/home/sanven0212/PycharmProjects/MultiOmicsFYP/Misc/Mergefile_top20 (4).csv')\n",
    "df1.head(10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L            GE_DPYSL2  GE_NEIL1  \\\n",
       "0  0.489097  0.749514   0.703490    0.187461   0.2973152983628573  0.744052   \n",
       "1  0.373498  0.790415   0.648564    0.367439                  0.0  0.638505   \n",
       "2  0.316560  0.714836   0.794977    0.363893    0.523421844750116  0.659055   \n",
       "3  0.433556  0.637116   0.777338    0.559615  0.47676041700945415  0.623626   \n",
       "4  0.363634  0.624428   0.892508    0.501788   0.4722603494002571  0.465397   \n",
       "5  0.540453  0.630588   0.671140    0.275536   0.6624865041294341  0.476065   \n",
       "6  0.506989  0.590670   0.727922    0.255442    0.733651313733152  0.691024   \n",
       "7  0.602648  0.780246   0.681841    0.362920   0.3562344997922704  0.631882   \n",
       "8  0.476160  0.319457   0.926185    0.172214   0.7113577651903742  0.589873   \n",
       "9  0.418700  0.702095   0.745375    0.430890     0.48754759139449  0.669876   \n",
       "\n",
       "   GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_SLC27A4  DM_PITPNM2  \\\n",
       "0    0.677605  0.736915  0.165418  0.927902  ...    0.309464    0.717767   \n",
       "1    0.156215  0.673907  0.384063  0.807230  ...    0.000000    0.791218   \n",
       "2    0.609083  0.759267  0.107208  0.827580  ...    0.493194    0.782551   \n",
       "3    0.415425  0.539748  0.466929  0.599673  ...    0.085465    0.569465   \n",
       "4    0.619784  0.562305  0.450817  0.729161  ...    0.119817    0.907230   \n",
       "5    0.577890  0.824079  0.297729  0.899286  ...    0.230212    0.831449   \n",
       "6    0.398213  0.644989  0.383090  0.713497  ...    0.502728    0.836422   \n",
       "7    0.189198  0.589256  0.521341  0.691657  ...    0.267739    0.839557   \n",
       "8    0.692148  0.797628  0.445715  0.904204  ...    0.711538    0.562896   \n",
       "9    0.441037  0.506928  0.140151  0.867808  ...    0.299419    0.605663   \n",
       "\n",
       "    DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1    PATIENT_ID  \\\n",
       "0  0.075158  0.395376  0.900153  0.228063   0.592166  0.815008  TCGA-2A-A8VT   \n",
       "1  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329  TCGA-2A-A8W1   \n",
       "2  0.086650  0.638757  0.923586  0.210818   0.472227  0.883677  TCGA-2A-A8W3   \n",
       "3  0.047191  0.101831  0.930270  0.221800   0.257343  1.000000  TCGA-2A-AAYF   \n",
       "4  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265  TCGA-CH-5737   \n",
       "5  0.040868  0.266358  0.943859  0.178990   0.505571  0.831493  TCGA-CH-5739   \n",
       "6  0.036482  0.328812  0.954992  0.185582   0.391752  0.892649  TCGA-CH-5740   \n",
       "7  0.314925  0.271794  0.858322  0.119971   0.345410  0.831250  TCGA-CH-5741   \n",
       "8  0.024710  0.636648  0.786474  0.227947   0.863551  0.622275  TCGA-CH-5743   \n",
       "9  0.045999  0.422079  0.885389  0.238896   0.548180  0.828444  TCGA-CH-5744   \n",
       "\n",
       "   TUMOR_STAGE  \n",
       "0           45  \n",
       "1           43  \n",
       "2           45  \n",
       "3           34  \n",
       "4           43  \n",
       "5           34  \n",
       "6           34  \n",
       "7           45  \n",
       "8           34  \n",
       "9           43  \n",
       "\n",
       "[10 rows x 48 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489097</td>\n",
       "      <td>0.749514</td>\n",
       "      <td>0.703490</td>\n",
       "      <td>0.187461</td>\n",
       "      <td>0.2973152983628573</td>\n",
       "      <td>0.744052</td>\n",
       "      <td>0.677605</td>\n",
       "      <td>0.736915</td>\n",
       "      <td>0.165418</td>\n",
       "      <td>0.927902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309464</td>\n",
       "      <td>0.717767</td>\n",
       "      <td>0.075158</td>\n",
       "      <td>0.395376</td>\n",
       "      <td>0.900153</td>\n",
       "      <td>0.228063</td>\n",
       "      <td>0.592166</td>\n",
       "      <td>0.815008</td>\n",
       "      <td>TCGA-2A-A8VT</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>TCGA-2A-A8W1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316560</td>\n",
       "      <td>0.714836</td>\n",
       "      <td>0.794977</td>\n",
       "      <td>0.363893</td>\n",
       "      <td>0.523421844750116</td>\n",
       "      <td>0.659055</td>\n",
       "      <td>0.609083</td>\n",
       "      <td>0.759267</td>\n",
       "      <td>0.107208</td>\n",
       "      <td>0.827580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493194</td>\n",
       "      <td>0.782551</td>\n",
       "      <td>0.086650</td>\n",
       "      <td>0.638757</td>\n",
       "      <td>0.923586</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>0.472227</td>\n",
       "      <td>0.883677</td>\n",
       "      <td>TCGA-2A-A8W3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.47676041700945415</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>TCGA-2A-AAYF</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.4722603494002571</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>TCGA-CH-5737</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.6624865041294341</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "      <td>TCGA-CH-5739</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651313733152</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "      <td>TCGA-CH-5740</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.602648</td>\n",
       "      <td>0.780246</td>\n",
       "      <td>0.681841</td>\n",
       "      <td>0.362920</td>\n",
       "      <td>0.3562344997922704</td>\n",
       "      <td>0.631882</td>\n",
       "      <td>0.189198</td>\n",
       "      <td>0.589256</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.691657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267739</td>\n",
       "      <td>0.839557</td>\n",
       "      <td>0.314925</td>\n",
       "      <td>0.271794</td>\n",
       "      <td>0.858322</td>\n",
       "      <td>0.119971</td>\n",
       "      <td>0.345410</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>TCGA-CH-5741</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.476160</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>0.172214</td>\n",
       "      <td>0.7113577651903742</td>\n",
       "      <td>0.589873</td>\n",
       "      <td>0.692148</td>\n",
       "      <td>0.797628</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.904204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.562896</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.786474</td>\n",
       "      <td>0.227947</td>\n",
       "      <td>0.863551</td>\n",
       "      <td>0.622275</td>\n",
       "      <td>TCGA-CH-5743</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.48754759139449</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "      <td>TCGA-CH-5744</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3edc5277908b11f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:33.912181Z",
     "start_time": "2024-10-11T03:49:33.893879Z"
    }
   },
   "source": [
    "df1.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 341 entries, 0 to 340\n",
      "Data columns (total 48 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      341 non-null    float64\n",
      " 1   GE_FOXA1     341 non-null    float64\n",
      " 2   GE_CTNNB1    341 non-null    float64\n",
      " 3   GE_CLPTM1L   341 non-null    float64\n",
      " 4   GE_DPYSL2    341 non-null    object \n",
      " 5   GE_NEIL1     341 non-null    float64\n",
      " 6   GE_PITPNM2   341 non-null    float64\n",
      " 7   GE_ATM       341 non-null    float64\n",
      " 8   GE_EMG1      341 non-null    float64\n",
      " 9   GE_ETV3      341 non-null    float64\n",
      " 10  GE_BRAF      341 non-null    float64\n",
      " 11  GE_NKX3-1    341 non-null    float64\n",
      " 12  GE_ZMYM3     341 non-null    float64\n",
      " 13  GE_SALL1     341 non-null    float64\n",
      " 14  CNA_SPOP     341 non-null    float64\n",
      " 15  CNA_TP53     341 non-null    float64\n",
      " 16  CNA_FOXA1    341 non-null    float64\n",
      " 17  CNA_CTNNB1   341 non-null    float64\n",
      " 18  CNA_MED12    341 non-null    float64\n",
      " 19  CNA_CLPTM1L  341 non-null    float64\n",
      " 20  CNA_DPYSL2   341 non-null    float64\n",
      " 21  CNA_NEIL1    341 non-null    float64\n",
      " 22  CNA_SLC27A4  341 non-null    float64\n",
      " 23  CNA_PITPNM2  341 non-null    float64\n",
      " 24  CNA_PTEN     341 non-null    float64\n",
      " 25  CNA_ATM      341 non-null    float64\n",
      " 26  CNA_EMG1     341 non-null    float64\n",
      " 27  CNA_ETV3     341 non-null    float64\n",
      " 28  CNA_BRAF     341 non-null    float64\n",
      " 29  CNA_ZMYM3    341 non-null    float64\n",
      " 30  CNA_OR4P4    341 non-null    float64\n",
      " 31  CNA_SALL1    341 non-null    float64\n",
      " 32  DM_SPOP      341 non-null    float64\n",
      " 33  DM_FOXA1     341 non-null    float64\n",
      " 34  DM_CTNNB1    341 non-null    float64\n",
      " 35  DM_CLPTM1L   341 non-null    float64\n",
      " 36  DM_DPYSL2    341 non-null    float64\n",
      " 37  DM_NEIL1     341 non-null    float64\n",
      " 38  DM_SLC27A4   341 non-null    float64\n",
      " 39  DM_PITPNM2   341 non-null    float64\n",
      " 40  DM_PTEN      341 non-null    float64\n",
      " 41  DM_EMG1      341 non-null    float64\n",
      " 42  DM_ETV3      341 non-null    float64\n",
      " 43  DM_BRAF      341 non-null    float64\n",
      " 44  DM_NKX3-1    341 non-null    float64\n",
      " 45  DM_SALL1     341 non-null    float64\n",
      " 46  PATIENT_ID   341 non-null    object \n",
      " 47  TUMOR_STAGE  341 non-null    int64  \n",
      "dtypes: float64(45), int64(1), object(2)\n",
      "memory usage: 128.0+ KB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "9f913af1bb28d058",
   "metadata": {},
   "source": [
    "**Checking the distribution of samples that are only present in 3 + 4 and 4 + 3 Gleason score classes**"
   ]
  },
  {
   "cell_type": "code",
   "id": "7868649478e7acc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:36.092586Z",
     "start_time": "2024-10-11T03:49:36.078287Z"
    }
   },
   "source": [
    "#Checking the no of samples beloning to tumor stage (Gleason Score): 3 + 4 and 4 + 3\n",
    "print(f\"No of samples having a Gleason score of 3 + 4: {(df1['TUMOR_STAGE'] == 34).value_counts()}\")\n",
    "print(f\"No of samples having a Gleason score of 4 + 3: {(df1['TUMOR_STAGE'] == 43).value_counts()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of samples having a Gleason score of 3 + 4: TUMOR_STAGE\n",
      "False    196\n",
      "True     145\n",
      "Name: count, dtype: int64\n",
      "No of samples having a Gleason score of 4 + 3: TUMOR_STAGE\n",
      "False    242\n",
      "True      99\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "a1e329b9af06ecb8",
   "metadata": {},
   "source": [
    "**Note: From the above output it is evident that there is a class imbalance between 3 + 4 (145) and 4 + 3 classes (99.)**"
   ]
  },
  {
   "cell_type": "code",
   "id": "602a999495dd0c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:42.911384Z",
     "start_time": "2024-10-11T03:49:42.907752Z"
    }
   },
   "source": [
    "df1.shape # Out of 341 total samples 3 + 4 and 4 + 3 Gleason score samples account for 244 samples"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 48)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "7d1a5905e4bb4558",
   "metadata": {},
   "source": [
    "**PatientID feature is dropped since it doesn't have any impact on the gleason score or any of the other Omics**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d4cffe8c3c01cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:45.881602Z",
     "start_time": "2024-10-11T03:49:45.878609Z"
    }
   },
   "source": [
    "#Dropping the patient ID column\n",
    "df1.drop(['PATIENT_ID'], axis = 1, inplace = True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "be1446b6c67499d0",
   "metadata": {},
   "source": [
    "**Reducing the gleason dataset to only have samples belonging to 3 + 4 and 4 + 3 classes**"
   ]
  },
  {
   "cell_type": "code",
   "id": "823a707800ec9a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:48.981800Z",
     "start_time": "2024-10-11T03:49:48.975067Z"
    }
   },
   "source": [
    "df1_gleason = df1[(df1['TUMOR_STAGE'] == 34) | (df1['TUMOR_STAGE'] == 43)]\n",
    "df1_gleason.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 47)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "3bf3a35aa3fa291a",
   "metadata": {},
   "source": [
    "**Converting the gleason scores from an int to an object to avoid bias**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9884bab9bfbf017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:54.449007Z",
     "start_time": "2024-10-11T03:49:54.446130Z"
    }
   },
   "source": [
    "df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].astype(object)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108742/1460778627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].astype(object)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "b96fb11eb53809e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:49:58.783113Z",
     "start_time": "2024-10-11T03:49:58.777617Z"
    }
   },
   "source": [
    "df1_gleason.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 244 entries, 1 to 320\n",
      "Data columns (total 47 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      244 non-null    float64\n",
      " 1   GE_FOXA1     244 non-null    float64\n",
      " 2   GE_CTNNB1    244 non-null    float64\n",
      " 3   GE_CLPTM1L   244 non-null    float64\n",
      " 4   GE_DPYSL2    244 non-null    object \n",
      " 5   GE_NEIL1     244 non-null    float64\n",
      " 6   GE_PITPNM2   244 non-null    float64\n",
      " 7   GE_ATM       244 non-null    float64\n",
      " 8   GE_EMG1      244 non-null    float64\n",
      " 9   GE_ETV3      244 non-null    float64\n",
      " 10  GE_BRAF      244 non-null    float64\n",
      " 11  GE_NKX3-1    244 non-null    float64\n",
      " 12  GE_ZMYM3     244 non-null    float64\n",
      " 13  GE_SALL1     244 non-null    float64\n",
      " 14  CNA_SPOP     244 non-null    float64\n",
      " 15  CNA_TP53     244 non-null    float64\n",
      " 16  CNA_FOXA1    244 non-null    float64\n",
      " 17  CNA_CTNNB1   244 non-null    float64\n",
      " 18  CNA_MED12    244 non-null    float64\n",
      " 19  CNA_CLPTM1L  244 non-null    float64\n",
      " 20  CNA_DPYSL2   244 non-null    float64\n",
      " 21  CNA_NEIL1    244 non-null    float64\n",
      " 22  CNA_SLC27A4  244 non-null    float64\n",
      " 23  CNA_PITPNM2  244 non-null    float64\n",
      " 24  CNA_PTEN     244 non-null    float64\n",
      " 25  CNA_ATM      244 non-null    float64\n",
      " 26  CNA_EMG1     244 non-null    float64\n",
      " 27  CNA_ETV3     244 non-null    float64\n",
      " 28  CNA_BRAF     244 non-null    float64\n",
      " 29  CNA_ZMYM3    244 non-null    float64\n",
      " 30  CNA_OR4P4    244 non-null    float64\n",
      " 31  CNA_SALL1    244 non-null    float64\n",
      " 32  DM_SPOP      244 non-null    float64\n",
      " 33  DM_FOXA1     244 non-null    float64\n",
      " 34  DM_CTNNB1    244 non-null    float64\n",
      " 35  DM_CLPTM1L   244 non-null    float64\n",
      " 36  DM_DPYSL2    244 non-null    float64\n",
      " 37  DM_NEIL1     244 non-null    float64\n",
      " 38  DM_SLC27A4   244 non-null    float64\n",
      " 39  DM_PITPNM2   244 non-null    float64\n",
      " 40  DM_PTEN      244 non-null    float64\n",
      " 41  DM_EMG1      244 non-null    float64\n",
      " 42  DM_ETV3      244 non-null    float64\n",
      " 43  DM_BRAF      244 non-null    float64\n",
      " 44  DM_NKX3-1    244 non-null    float64\n",
      " 45  DM_SALL1     244 non-null    float64\n",
      " 46  TUMOR_STAGE  244 non-null    object \n",
      "dtypes: float64(45), object(2)\n",
      "memory usage: 91.5+ KB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "b42efb390d053ce3",
   "metadata": {},
   "source": [
    "**Using a custom function to encode 3 + 4 (Majority) class to 0 and 4 + 3 (Minority) to 1**"
   ]
  },
  {
   "cell_type": "code",
   "id": "783e5c6e95965685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:00.206197Z",
     "start_time": "2024-10-11T03:50:00.203517Z"
    }
   },
   "source": [
    "#Encoding 34 (majority class) as 0 and 43 (minority class) as 1\n",
    "def label_encode(target):\n",
    "    if target == 34:\n",
    "        target = 0\n",
    "    else:\n",
    "        target = 1\n",
    "    return target\n",
    "        \n",
    "df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].apply(label_encode)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108742/2632578046.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_gleason['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE'].apply(label_encode)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "f41574feeab0ce46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:02.100859Z",
     "start_time": "2024-10-11T03:50:02.095282Z"
    }
   },
   "source": [
    "df1_gleason['TUMOR_STAGE'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "0    145\n",
       "1     99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "1a2de896a369db5e",
   "metadata": {},
   "source": [
    "# **Handling Class Imbalance using GTGANs (Guided Triplet GANs)**\n",
    "**Working of GTGANs**\n",
    "1. 2 main components: Generator & Discriminator\n",
    "2. Generator generates synthetic samples; Discriminator evaluates and checks the validity of the samples to check if they are fake or do they actually belong to the original dataset\n",
    "3. Both of these networks are trained in tandem in a process called `adversarial training`\n",
    "\n",
    "**Training steps**\n",
    "1. Divide the entire dataset into majority and minority class samples\n",
    "2. The majority class samples data is fed to the discriminator network as real data\n",
    "3. Generator network takes in random normal noise as input which has the input and output shape of the minority class dataset\n",
    "4. The Discriminator network takes in either minority class samples or synthetic samples generated by the generator to classify as real or fake\n",
    "5. Discriminator is trained on real minority samples and generated samples\n",
    "\n",
    "**Note: While Generator is being trained, the discriminator has to be frozen and vice-versa. In addition the input and output_dim for discriminator should match the no of columns in the dataframe and the target feature must be excluded**\n",
    "\n",
    "**Differences**\n",
    "1. GTGANs make use of triplet loss function where 3 main components namely: anchor, positive and negative sample are involved\n",
    "2. Anchor is a true sample from the minority class that requires oversampling\n",
    "3. Positive samples are the ones that closely resemble the anchor\n",
    "4. Negative samples are the ones that deviate from the anchor\n",
    "5. 2 Euclidean distances one b/w anchor and positive samples and b/w anchor and negative samples are computed\n",
    "6. Distance b/w anchor and +ve should be minimised and the distance b/w anchor and -ve should be maximised\n",
    "7. The discriminator should return embedding and a specific class rather than acting just as a binary classifier\n",
    "8. Combined loss is computed in the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179fcbf458fb51b",
   "metadata": {},
   "source": [
    "**Splitting the gleason dataset into majority and minority class samples**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f78d51e52d619e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:10.330464Z",
     "start_time": "2024-10-11T03:50:10.326605Z"
    }
   },
   "source": [
    "#Getting the majority and minority datasets\n",
    "majority_dataset = df1_gleason[df1_gleason['TUMOR_STAGE'] == 0]\n",
    "minority_dataset = df1_gleason[df1_gleason['TUMOR_STAGE'] == 1]"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f8f9a6d92f9acdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:12.100461Z",
     "start_time": "2024-10-11T03:50:12.090591Z"
    }
   },
   "source": [
    "majority_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L            GE_DPYSL2  GE_NEIL1  \\\n",
       "3    0.433556  0.637116   0.777338    0.559615  0.47676041700945415  0.623626   \n",
       "5    0.540453  0.630588   0.671140    0.275536   0.6624865041294341  0.476065   \n",
       "6    0.506989  0.590670   0.727922    0.255442    0.733651313733152  0.691024   \n",
       "8    0.476160  0.319457   0.926185    0.172214   0.7113577651903742  0.589873   \n",
       "10   0.503070  0.628456   0.872354    0.317441   0.4283272136680272  0.641957   \n",
       "..        ...       ...        ...         ...                  ...       ...   \n",
       "291  0.227620  0.672938   0.530553    0.381428  0.17597210091467486  0.824114   \n",
       "297  0.258006  0.687300   0.757727    0.470806  0.39466806443394375  0.575239   \n",
       "299  0.485115  0.338921   0.309483    0.128955   0.5104237748626894  0.891032   \n",
       "301  0.624653  0.623661   0.434110    0.445462  0.49241032177170707  0.912847   \n",
       "303  0.356728  0.592104   0.851501    0.424060  0.33765635284311823  0.613548   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "3      0.415425  0.539748  0.466929  0.599673  ...  0.111825    0.085465   \n",
       "5      0.577890  0.824079  0.297729  0.899286  ...  0.228759    0.230212   \n",
       "6      0.398213  0.644989  0.383090  0.713497  ...  0.129223    0.502728   \n",
       "8      0.692148  0.797628  0.445715  0.904204  ...  0.256412    0.711538   \n",
       "10     0.624830  0.775454  0.426589  0.880784  ...  0.153859    0.391933   \n",
       "..          ...       ...       ...       ...  ...       ...         ...   \n",
       "291    0.690394  0.603873  0.581930  0.815111  ...  0.007223    0.288589   \n",
       "297    0.302726  0.603989  0.098050  0.789135  ...  0.118404    0.060399   \n",
       "299    0.612340  0.569947  0.697590  0.351627  ...  0.188277    0.416333   \n",
       "301    0.351620  0.441806  0.503556  0.462003  ...  0.112395    0.178172   \n",
       "303    0.443205  0.632808  0.187181  0.847817  ...  0.194689    0.349741   \n",
       "\n",
       "     DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "3      0.569465  0.047191  0.101831  0.930270  0.221800   0.257343  1.000000   \n",
       "5      0.831449  0.040868  0.266358  0.943859  0.178990   0.505571  0.831493   \n",
       "6      0.836422  0.036482  0.328812  0.954992  0.185582   0.391752  0.892649   \n",
       "8      0.562896  0.024710  0.636648  0.786474  0.227947   0.863551  0.622275   \n",
       "10     0.866063  0.027305  0.472842  0.917519  0.216519   0.427699  0.835883   \n",
       "..          ...       ...       ...       ...       ...        ...       ...   \n",
       "291    0.872192  0.069334  0.125432  0.929922  0.232844   0.000000  0.942248   \n",
       "297    0.655711  0.097734  0.271404  0.947257  0.292598   0.266765  0.856432   \n",
       "299    0.627777  0.085632  0.461510  0.930563  0.284727   0.549605  0.696472   \n",
       "301    0.669154  0.097933  0.662839  0.963417  0.212591   0.444958  0.955913   \n",
       "303    0.732153  0.122011  0.586201  0.889234  0.208477   0.560052  0.751077   \n",
       "\n",
       "     TUMOR_STAGE  \n",
       "3              0  \n",
       "5              0  \n",
       "6              0  \n",
       "8              0  \n",
       "10             0  \n",
       "..           ...  \n",
       "291            0  \n",
       "297            0  \n",
       "299            0  \n",
       "301            0  \n",
       "303            0  \n",
       "\n",
       "[145 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.47676041700945415</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111825</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.6624865041294341</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228759</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651313733152</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.476160</td>\n",
       "      <td>0.319457</td>\n",
       "      <td>0.926185</td>\n",
       "      <td>0.172214</td>\n",
       "      <td>0.7113577651903742</td>\n",
       "      <td>0.589873</td>\n",
       "      <td>0.692148</td>\n",
       "      <td>0.797628</td>\n",
       "      <td>0.445715</td>\n",
       "      <td>0.904204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256412</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.562896</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>0.786474</td>\n",
       "      <td>0.227947</td>\n",
       "      <td>0.863551</td>\n",
       "      <td>0.622275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.503070</td>\n",
       "      <td>0.628456</td>\n",
       "      <td>0.872354</td>\n",
       "      <td>0.317441</td>\n",
       "      <td>0.4283272136680272</td>\n",
       "      <td>0.641957</td>\n",
       "      <td>0.624830</td>\n",
       "      <td>0.775454</td>\n",
       "      <td>0.426589</td>\n",
       "      <td>0.880784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153859</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>0.866063</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.472842</td>\n",
       "      <td>0.917519</td>\n",
       "      <td>0.216519</td>\n",
       "      <td>0.427699</td>\n",
       "      <td>0.835883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.672938</td>\n",
       "      <td>0.530553</td>\n",
       "      <td>0.381428</td>\n",
       "      <td>0.17597210091467486</td>\n",
       "      <td>0.824114</td>\n",
       "      <td>0.690394</td>\n",
       "      <td>0.603873</td>\n",
       "      <td>0.581930</td>\n",
       "      <td>0.815111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.288589</td>\n",
       "      <td>0.872192</td>\n",
       "      <td>0.069334</td>\n",
       "      <td>0.125432</td>\n",
       "      <td>0.929922</td>\n",
       "      <td>0.232844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.258006</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.757727</td>\n",
       "      <td>0.470806</td>\n",
       "      <td>0.39466806443394375</td>\n",
       "      <td>0.575239</td>\n",
       "      <td>0.302726</td>\n",
       "      <td>0.603989</td>\n",
       "      <td>0.098050</td>\n",
       "      <td>0.789135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118404</td>\n",
       "      <td>0.060399</td>\n",
       "      <td>0.655711</td>\n",
       "      <td>0.097734</td>\n",
       "      <td>0.271404</td>\n",
       "      <td>0.947257</td>\n",
       "      <td>0.292598</td>\n",
       "      <td>0.266765</td>\n",
       "      <td>0.856432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.485115</td>\n",
       "      <td>0.338921</td>\n",
       "      <td>0.309483</td>\n",
       "      <td>0.128955</td>\n",
       "      <td>0.5104237748626894</td>\n",
       "      <td>0.891032</td>\n",
       "      <td>0.612340</td>\n",
       "      <td>0.569947</td>\n",
       "      <td>0.697590</td>\n",
       "      <td>0.351627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188277</td>\n",
       "      <td>0.416333</td>\n",
       "      <td>0.627777</td>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.461510</td>\n",
       "      <td>0.930563</td>\n",
       "      <td>0.284727</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.696472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.624653</td>\n",
       "      <td>0.623661</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.445462</td>\n",
       "      <td>0.49241032177170707</td>\n",
       "      <td>0.912847</td>\n",
       "      <td>0.351620</td>\n",
       "      <td>0.441806</td>\n",
       "      <td>0.503556</td>\n",
       "      <td>0.462003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112395</td>\n",
       "      <td>0.178172</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.662839</td>\n",
       "      <td>0.963417</td>\n",
       "      <td>0.212591</td>\n",
       "      <td>0.444958</td>\n",
       "      <td>0.955913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.356728</td>\n",
       "      <td>0.592104</td>\n",
       "      <td>0.851501</td>\n",
       "      <td>0.424060</td>\n",
       "      <td>0.33765635284311823</td>\n",
       "      <td>0.613548</td>\n",
       "      <td>0.443205</td>\n",
       "      <td>0.632808</td>\n",
       "      <td>0.187181</td>\n",
       "      <td>0.847817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194689</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.732153</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.586201</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.208477</td>\n",
       "      <td>0.560052</td>\n",
       "      <td>0.751077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "93080f56f384deb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:19.608250Z",
     "start_time": "2024-10-11T03:50:19.598155Z"
    }
   },
   "source": [
    "minority_dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L           GE_DPYSL2  GE_NEIL1  \\\n",
       "1    0.373498  0.790415   0.648564    0.367439                 0.0  0.638505   \n",
       "4    0.363634  0.624428   0.892508    0.501788  0.4722603494002571  0.465397   \n",
       "9    0.418700  0.702095   0.745375    0.430890    0.48754759139449  0.669876   \n",
       "11   0.470982  0.654804   0.799236    0.414669  0.4210942809110083  0.578125   \n",
       "15   0.535495  0.574125   0.990674    0.227795  0.6868898490859685  0.486809   \n",
       "..        ...       ...        ...         ...                 ...       ...   \n",
       "307  0.678217  0.379478   0.651014    0.313037  0.7273142326471103  0.697200   \n",
       "308  0.570659  0.513166   0.756434    0.326435  0.6368927742777322  0.703464   \n",
       "309  0.405538  0.608860   0.570281    0.589831  0.4083234498029089  0.720283   \n",
       "315  0.372651  0.261140   0.643123    0.389611  0.9079820216874572  0.579044   \n",
       "320  0.540018  0.512559   0.602297    0.452478  0.6215400971027303  0.749010   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "1      0.156215  0.673907  0.384063  0.807230  ...  0.034757    0.000000   \n",
       "4      0.619784  0.562305  0.450817  0.729161  ...  0.182983    0.119817   \n",
       "9      0.441037  0.506928  0.140151  0.867808  ...  0.181650    0.299419   \n",
       "11     0.466010  0.715588  0.355391  0.813532  ...  0.172506    0.200640   \n",
       "15     0.735071  0.832965  0.387468  0.947819  ...  0.330505    0.531055   \n",
       "..          ...       ...       ...       ...  ...       ...         ...   \n",
       "307    0.609731  0.463844  0.473988  0.731562  ...  0.155282    0.363361   \n",
       "308    0.657117  0.559125  0.441622  0.738266  ...  0.149038    0.445528   \n",
       "309    0.288630  0.456215  0.602279  0.361531  ...  0.155776    0.039771   \n",
       "315    0.608729  0.577335  0.446111  0.763712  ...  0.641021    0.367391   \n",
       "320    0.749921  0.533543  0.472266  0.488479  ...  0.226134    0.495551   \n",
       "\n",
       "     DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "1      0.791218  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329   \n",
       "4      0.907230  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265   \n",
       "9      0.605663  0.045999  0.422079  0.885389  0.238896   0.548180  0.828444   \n",
       "11     0.782428  0.035299  0.285695  0.933832  0.175760   0.263804  0.776006   \n",
       "15     0.546693  0.048915  0.458723  0.868237  0.251513   0.654329  0.667877   \n",
       "..          ...       ...       ...       ...       ...        ...       ...   \n",
       "307    0.564151  0.044499  0.272052  0.953258  0.111529   0.416038  0.811600   \n",
       "308    0.621179  0.045135  0.693765  0.926586  0.133146   0.672396  0.737615   \n",
       "309    0.750137  0.054077  0.262873  0.958069  0.289268   0.356099  0.469004   \n",
       "315    0.477116  0.079954  0.354398  0.883972  0.342184   0.509671  0.810917   \n",
       "320    0.639774  0.072104  0.484919  0.949089  0.337387   0.444336  0.630323   \n",
       "\n",
       "     TUMOR_STAGE  \n",
       "1              1  \n",
       "4              1  \n",
       "9              1  \n",
       "11             1  \n",
       "15             1  \n",
       "..           ...  \n",
       "307            1  \n",
       "308            1  \n",
       "309            1  \n",
       "315            1  \n",
       "320            1  \n",
       "\n",
       "[99 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.4722603494002571</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.48754759139449</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181650</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.654804</td>\n",
       "      <td>0.799236</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.4210942809110083</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.466010</td>\n",
       "      <td>0.715588</td>\n",
       "      <td>0.355391</td>\n",
       "      <td>0.813532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>0.782428</td>\n",
       "      <td>0.035299</td>\n",
       "      <td>0.285695</td>\n",
       "      <td>0.933832</td>\n",
       "      <td>0.175760</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.776006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.535495</td>\n",
       "      <td>0.574125</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.227795</td>\n",
       "      <td>0.6868898490859685</td>\n",
       "      <td>0.486809</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.387468</td>\n",
       "      <td>0.947819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330505</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.458723</td>\n",
       "      <td>0.868237</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.654329</td>\n",
       "      <td>0.667877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.678217</td>\n",
       "      <td>0.379478</td>\n",
       "      <td>0.651014</td>\n",
       "      <td>0.313037</td>\n",
       "      <td>0.7273142326471103</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.609731</td>\n",
       "      <td>0.463844</td>\n",
       "      <td>0.473988</td>\n",
       "      <td>0.731562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155282</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>0.564151</td>\n",
       "      <td>0.044499</td>\n",
       "      <td>0.272052</td>\n",
       "      <td>0.953258</td>\n",
       "      <td>0.111529</td>\n",
       "      <td>0.416038</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.570659</td>\n",
       "      <td>0.513166</td>\n",
       "      <td>0.756434</td>\n",
       "      <td>0.326435</td>\n",
       "      <td>0.6368927742777322</td>\n",
       "      <td>0.703464</td>\n",
       "      <td>0.657117</td>\n",
       "      <td>0.559125</td>\n",
       "      <td>0.441622</td>\n",
       "      <td>0.738266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149038</td>\n",
       "      <td>0.445528</td>\n",
       "      <td>0.621179</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.693765</td>\n",
       "      <td>0.926586</td>\n",
       "      <td>0.133146</td>\n",
       "      <td>0.672396</td>\n",
       "      <td>0.737615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.405538</td>\n",
       "      <td>0.608860</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>0.589831</td>\n",
       "      <td>0.4083234498029089</td>\n",
       "      <td>0.720283</td>\n",
       "      <td>0.288630</td>\n",
       "      <td>0.456215</td>\n",
       "      <td>0.602279</td>\n",
       "      <td>0.361531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155776</td>\n",
       "      <td>0.039771</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>0.054077</td>\n",
       "      <td>0.262873</td>\n",
       "      <td>0.958069</td>\n",
       "      <td>0.289268</td>\n",
       "      <td>0.356099</td>\n",
       "      <td>0.469004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.372651</td>\n",
       "      <td>0.261140</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.389611</td>\n",
       "      <td>0.9079820216874572</td>\n",
       "      <td>0.579044</td>\n",
       "      <td>0.608729</td>\n",
       "      <td>0.577335</td>\n",
       "      <td>0.446111</td>\n",
       "      <td>0.763712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641021</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.477116</td>\n",
       "      <td>0.079954</td>\n",
       "      <td>0.354398</td>\n",
       "      <td>0.883972</td>\n",
       "      <td>0.342184</td>\n",
       "      <td>0.509671</td>\n",
       "      <td>0.810917</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.540018</td>\n",
       "      <td>0.512559</td>\n",
       "      <td>0.602297</td>\n",
       "      <td>0.452478</td>\n",
       "      <td>0.6215400971027303</td>\n",
       "      <td>0.749010</td>\n",
       "      <td>0.749921</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>0.472266</td>\n",
       "      <td>0.488479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226134</td>\n",
       "      <td>0.495551</td>\n",
       "      <td>0.639774</td>\n",
       "      <td>0.072104</td>\n",
       "      <td>0.484919</td>\n",
       "      <td>0.949089</td>\n",
       "      <td>0.337387</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.630323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "e43b9fdc993e4b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:25.374906Z",
     "start_time": "2024-10-11T03:50:25.368811Z"
    }
   },
   "source": [
    "minority_dataset.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99 entries, 1 to 320\n",
      "Data columns (total 47 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      99 non-null     float64\n",
      " 1   GE_FOXA1     99 non-null     float64\n",
      " 2   GE_CTNNB1    99 non-null     float64\n",
      " 3   GE_CLPTM1L   99 non-null     float64\n",
      " 4   GE_DPYSL2    99 non-null     object \n",
      " 5   GE_NEIL1     99 non-null     float64\n",
      " 6   GE_PITPNM2   99 non-null     float64\n",
      " 7   GE_ATM       99 non-null     float64\n",
      " 8   GE_EMG1      99 non-null     float64\n",
      " 9   GE_ETV3      99 non-null     float64\n",
      " 10  GE_BRAF      99 non-null     float64\n",
      " 11  GE_NKX3-1    99 non-null     float64\n",
      " 12  GE_ZMYM3     99 non-null     float64\n",
      " 13  GE_SALL1     99 non-null     float64\n",
      " 14  CNA_SPOP     99 non-null     float64\n",
      " 15  CNA_TP53     99 non-null     float64\n",
      " 16  CNA_FOXA1    99 non-null     float64\n",
      " 17  CNA_CTNNB1   99 non-null     float64\n",
      " 18  CNA_MED12    99 non-null     float64\n",
      " 19  CNA_CLPTM1L  99 non-null     float64\n",
      " 20  CNA_DPYSL2   99 non-null     float64\n",
      " 21  CNA_NEIL1    99 non-null     float64\n",
      " 22  CNA_SLC27A4  99 non-null     float64\n",
      " 23  CNA_PITPNM2  99 non-null     float64\n",
      " 24  CNA_PTEN     99 non-null     float64\n",
      " 25  CNA_ATM      99 non-null     float64\n",
      " 26  CNA_EMG1     99 non-null     float64\n",
      " 27  CNA_ETV3     99 non-null     float64\n",
      " 28  CNA_BRAF     99 non-null     float64\n",
      " 29  CNA_ZMYM3    99 non-null     float64\n",
      " 30  CNA_OR4P4    99 non-null     float64\n",
      " 31  CNA_SALL1    99 non-null     float64\n",
      " 32  DM_SPOP      99 non-null     float64\n",
      " 33  DM_FOXA1     99 non-null     float64\n",
      " 34  DM_CTNNB1    99 non-null     float64\n",
      " 35  DM_CLPTM1L   99 non-null     float64\n",
      " 36  DM_DPYSL2    99 non-null     float64\n",
      " 37  DM_NEIL1     99 non-null     float64\n",
      " 38  DM_SLC27A4   99 non-null     float64\n",
      " 39  DM_PITPNM2   99 non-null     float64\n",
      " 40  DM_PTEN      99 non-null     float64\n",
      " 41  DM_EMG1      99 non-null     float64\n",
      " 42  DM_ETV3      99 non-null     float64\n",
      " 43  DM_BRAF      99 non-null     float64\n",
      " 44  DM_NKX3-1    99 non-null     float64\n",
      " 45  DM_SALL1     99 non-null     float64\n",
      " 46  TUMOR_STAGE  99 non-null     int64  \n",
      "dtypes: float64(45), int64(1), object(1)\n",
      "memory usage: 37.1+ KB\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "59924e6d5a683794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:27.623543Z",
     "start_time": "2024-10-11T03:50:27.619706Z"
    }
   },
   "source": [
    "minority_dataset['GE_DPYSL2'] = minority_dataset['GE_DPYSL2'].astype(float)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108742/924810706.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  minority_dataset['GE_DPYSL2'] = minority_dataset['GE_DPYSL2'].astype(float)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "e089466f9cf7aa0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:29.861796Z",
     "start_time": "2024-10-11T03:50:29.856094Z"
    }
   },
   "source": [
    "majority_dataset.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 145 entries, 3 to 303\n",
      "Data columns (total 47 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      145 non-null    float64\n",
      " 1   GE_FOXA1     145 non-null    float64\n",
      " 2   GE_CTNNB1    145 non-null    float64\n",
      " 3   GE_CLPTM1L   145 non-null    float64\n",
      " 4   GE_DPYSL2    145 non-null    object \n",
      " 5   GE_NEIL1     145 non-null    float64\n",
      " 6   GE_PITPNM2   145 non-null    float64\n",
      " 7   GE_ATM       145 non-null    float64\n",
      " 8   GE_EMG1      145 non-null    float64\n",
      " 9   GE_ETV3      145 non-null    float64\n",
      " 10  GE_BRAF      145 non-null    float64\n",
      " 11  GE_NKX3-1    145 non-null    float64\n",
      " 12  GE_ZMYM3     145 non-null    float64\n",
      " 13  GE_SALL1     145 non-null    float64\n",
      " 14  CNA_SPOP     145 non-null    float64\n",
      " 15  CNA_TP53     145 non-null    float64\n",
      " 16  CNA_FOXA1    145 non-null    float64\n",
      " 17  CNA_CTNNB1   145 non-null    float64\n",
      " 18  CNA_MED12    145 non-null    float64\n",
      " 19  CNA_CLPTM1L  145 non-null    float64\n",
      " 20  CNA_DPYSL2   145 non-null    float64\n",
      " 21  CNA_NEIL1    145 non-null    float64\n",
      " 22  CNA_SLC27A4  145 non-null    float64\n",
      " 23  CNA_PITPNM2  145 non-null    float64\n",
      " 24  CNA_PTEN     145 non-null    float64\n",
      " 25  CNA_ATM      145 non-null    float64\n",
      " 26  CNA_EMG1     145 non-null    float64\n",
      " 27  CNA_ETV3     145 non-null    float64\n",
      " 28  CNA_BRAF     145 non-null    float64\n",
      " 29  CNA_ZMYM3    145 non-null    float64\n",
      " 30  CNA_OR4P4    145 non-null    float64\n",
      " 31  CNA_SALL1    145 non-null    float64\n",
      " 32  DM_SPOP      145 non-null    float64\n",
      " 33  DM_FOXA1     145 non-null    float64\n",
      " 34  DM_CTNNB1    145 non-null    float64\n",
      " 35  DM_CLPTM1L   145 non-null    float64\n",
      " 36  DM_DPYSL2    145 non-null    float64\n",
      " 37  DM_NEIL1     145 non-null    float64\n",
      " 38  DM_SLC27A4   145 non-null    float64\n",
      " 39  DM_PITPNM2   145 non-null    float64\n",
      " 40  DM_PTEN      145 non-null    float64\n",
      " 41  DM_EMG1      145 non-null    float64\n",
      " 42  DM_ETV3      145 non-null    float64\n",
      " 43  DM_BRAF      145 non-null    float64\n",
      " 44  DM_NKX3-1    145 non-null    float64\n",
      " 45  DM_SALL1     145 non-null    float64\n",
      " 46  TUMOR_STAGE  145 non-null    int64  \n",
      "dtypes: float64(45), int64(1), object(1)\n",
      "memory usage: 54.4+ KB\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "6ab214056bc2c6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:31.746702Z",
     "start_time": "2024-10-11T03:50:31.741720Z"
    }
   },
   "source": [
    "majority_dataset['GE_DPYSL2'].info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 145 entries, 3 to 303\n",
      "Series name: GE_DPYSL2\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "145 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "d87a4d97c2b935f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:33.261060Z",
     "start_time": "2024-10-11T03:50:33.256215Z"
    }
   },
   "source": [
    "majority_dataset['GE_DPYSL2'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GE_DPYSL2\n",
       "0.47676041700945415    1\n",
       "0.5062889309146499     1\n",
       "0.40908523711864175    1\n",
       "0.655254592186036      1\n",
       "0.2976809803586109     1\n",
       "                      ..\n",
       "0.7473071737224499     1\n",
       "0.4966240543051046     1\n",
       "0.43397426120131843    1\n",
       "0.7415390122783143     1\n",
       "0.33765635284311823    1\n",
       "Name: count, Length: 145, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "e91bd33f6e0de28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:34.879663Z",
     "start_time": "2024-10-11T03:50:34.876903Z"
    }
   },
   "source": [
    "#replace strings with just numbers\n",
    "def replace(value):\n",
    "    if isinstance(value, str):\n",
    "        return ''.join(filter(str.isdigit, value))\n",
    "    return value"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "443732388060f3e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:37.312010Z",
     "start_time": "2024-10-11T03:50:37.308730Z"
    }
   },
   "source": [
    "majority_dataset['GE_DPYSL2'] = majority_dataset['GE_DPYSL2'].apply(replace)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108742/2199232684.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  majority_dataset['GE_DPYSL2'] = majority_dataset['GE_DPYSL2'].apply(replace)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "246db3292bb73947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:39.116129Z",
     "start_time": "2024-10-11T03:50:39.111469Z"
    }
   },
   "source": [
    "majority_dataset['GE_DPYSL2'] = majority_dataset['GE_DPYSL2'].astype(float)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108742/4091301698.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  majority_dataset['GE_DPYSL2'] = majority_dataset['GE_DPYSL2'].astype(float)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "7127aa542d3a735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:40.173155Z",
     "start_time": "2024-10-11T03:50:40.168673Z"
    }
   },
   "source": [
    "majority_dataset['GE_DPYSL2'].info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 145 entries, 3 to 303\n",
      "Series name: GE_DPYSL2\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "145 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 2.3 KB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "a6345edec693b6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:41.975880Z",
     "start_time": "2024-10-11T03:50:41.972308Z"
    }
   },
   "source": [
    "minority_features = minority_dataset.drop('TUMOR_STAGE', axis = 1)\n",
    "minority_target = minority_dataset['TUMOR_STAGE']"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "418d1f422a289665",
   "metadata": {},
   "source": [
    "**Generator Network**"
   ]
  },
  {
   "cell_type": "code",
   "id": "77901552c023530b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:51.224136Z",
     "start_time": "2024-10-11T03:50:51.123356Z"
    }
   },
   "source": [
    "#Generator\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LeakyReLU, BatchNormalization, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "input_dim = 100\n",
    "output_dim = minority_features.shape[1] \n",
    "n_classes = len(minority_dataset['TUMOR_STAGE'].unique())\n",
    "gen_input = Input(shape = (input_dim, ))\n",
    "gen_class_input = Input(shape = (n_classes, ))\n",
    "gen_total_input = Concatenate()([gen_input, gen_class_input]) \n",
    "fc_1 = Dense(128)(gen_total_input)\n",
    "lr_1 = LeakyReLU(alpha = 0.2)(fc_1)\n",
    "bn_1 = BatchNormalization(momentum = 0.4)(lr_1)\n",
    "fc_2 = Dense(256)(bn_1)\n",
    "lr_2 = LeakyReLU(alpha = 0.2)(fc_2)\n",
    "bn_2 = BatchNormalization(momentum = 0.4)(lr_2)\n",
    "fc_3 = Dense(512)(bn_2)\n",
    "lr_3 = LeakyReLU(alpha = 0.2)(fc_3)\n",
    "bn_3 = BatchNormalization(momentum = 0.4)(lr_3)\n",
    "gen_output = Dense(output_dim, activation = 'sigmoid')(bn_3)\n",
    "gen_model = Model([gen_input, gen_class_input], gen_output)\n",
    "gen_model.summary()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 101)                  0         ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  13056     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 128)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 128)                  512       ['leaky_re_lu[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  33024     ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 256)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 256)                  1024      ['leaky_re_lu_1[0][0]']       \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 512)                  131584    ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 512)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 512)                  2048      ['leaky_re_lu_2[0][0]']       \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 46)                   23598     ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 204846 (800.18 KB)\n",
      "Trainable params: 203054 (793.18 KB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "49f8f40589251564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:52.404076Z",
     "start_time": "2024-10-11T03:50:52.400522Z"
    }
   },
   "source": [
    "len(minority_dataset['TUMOR_STAGE'].unique())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "a80834d50ee00d12",
   "metadata": {},
   "source": [
    "**Discriminator Network**"
   ]
  },
  {
   "cell_type": "code",
   "id": "27f18890f949c9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:55.130267Z",
     "start_time": "2024-10-11T03:50:55.080199Z"
    }
   },
   "source": [
    "#Discriminator\n",
    "dis_input = Input(shape = (output_dim, ))\n",
    "dis_class_input = Input(shape = (n_classes, ))\n",
    "dis_total_input = Concatenate()([dis_input, dis_class_input])\n",
    "dis_fc_1 = Dense(512)(dis_total_input)\n",
    "dis_lr_1 = LeakyReLU(alpha = 0.2)(dis_fc_1)\n",
    "dis_dp_1 = Dropout(0.2)(dis_lr_1)\n",
    "dis_fc_2 = Dense(512)(dis_dp_1)\n",
    "dis_lr_2 = LeakyReLU(alpha = 0.2)(dis_fc_2)\n",
    "dis_dp_2 = Dropout(0.2)(dis_lr_2)\n",
    "dis_fc_3 = Dense(256)(dis_dp_2)\n",
    "dis_lr_3 = LeakyReLU(alpha = 0.2)(dis_fc_3)\n",
    "dis_dp_3 = Dropout(0.2)(dis_lr_3)\n",
    "#Layers for getting the embeddings from the discriminator\n",
    "dis_lr_4 = Dense(128)(dis_dp_3)\n",
    "dis_output = Dense(1, activation = 'sigmoid')(dis_lr_4)\n",
    "dis_model = Model([dis_input, dis_class_input], dis_output)\n",
    "dis_model.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 46)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 47)                   0         ['input_3[0][0]',             \n",
      " )                                                                   'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 512)                  24576     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 512)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512)                  0         ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 512)                  262656    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 512)                  0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 512)                  0         ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 256)                  131328    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 256)                  0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 256)                  0         ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  32896     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    129       ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 451585 (1.72 MB)\n",
      "Trainable params: 451585 (1.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "4077cfb21be0a9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:50:59.810881Z",
     "start_time": "2024-10-11T03:50:59.651186Z"
    }
   },
   "source": [
    "#Compiling the discriminator\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "dis_model.compile(loss = BinaryCrossentropy(), optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\n",
    "dis_model.trainable = False\n",
    "latent_vec_layer = Input(shape = (input_dim, ))\n",
    "latent_class = Input(shape = (n_classes, ))\n",
    "gen_sample = gen_model([latent_vec_layer, latent_class])\n",
    "dis_fp = dis_model([gen_sample, latent_class]) \n",
    "CTGAN_model = Model([latent_vec_layer, latent_class], dis_fp)\n",
    "CTGAN_model.compile(loss = BinaryCrossentropy(), optimizer = Adam(learning_rate = 0.001))"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "b397d28c3f591d37",
   "metadata": {},
   "source": [
    "**Custom function for Triplet Loss computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387615a5f9a22668",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "id": "90fc2ccb674a388a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:52:06.647Z",
     "start_time": "2024-10-11T03:51:40.398222Z"
    }
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "lambda_t = 1.0\n",
    "from tqdm.auto import tqdm\n",
    "#Training phase for GAN\n",
    "for i in tqdm(range(epochs)):\n",
    "    #Getting a batch of samples from minority class\n",
    "    real_minority_samples = tf.convert_to_tensor(minority_features.sample(batch_size).values)\n",
    "    real_minority_labels = tf.convert_to_tensor(minority_target.sample(batch_size).values)\n",
    "    #Fake samples by forward pass of generator\n",
    "    random_latent_vector = tf.random.normal((batch_size, input_dim))\n",
    "    random_label = tf.ones((batch_size, 1))\n",
    "    fake_minority_samples = gen_model([random_latent_vector, random_label])\n",
    "    #Creating tensors of 1s and 0s for true and fake samples respectively\n",
    "    true_labels = tf.ones((batch_size, 1))\n",
    "    fake_labels = tf.zeros((batch_size, 1))\n",
    "    #Training Discriminator to get the losses\n",
    "    dis_real_loss = dis_model.train_on_batch([real_minority_samples, real_minority_labels], true_labels)\n",
    "    dis_fake_loss = dis_model.train_on_batch([fake_minority_samples, random_label], fake_labels)\n",
    "    #Training the generator\n",
    "    gen_latent_vec = tf.random.normal((batch_size, input_dim))\n",
    "    gen_label = tf.ones((batch_size, 1))\n",
    "    random_label = tf.ones((batch_size, 1))\n",
    "    CTGAN_model_loss = CTGAN_model.train_on_batch([gen_latent_vec, gen_label], random_label)\n",
    "    print(f'Epoch: {i + 1} | Discriminator Real Loss: {dis_real_loss} | Discriminator Fake Loss: {dis_fake_loss} | CTGAN Loss: {CTGAN_model_loss}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95306f450a1740c7a8dd5666c65fd06e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Discriminator Real Loss: [0.6241405010223389, 0.71875] | Discriminator Fake Loss: [2.1981940269470215, 0.0] | CTGAN Loss: 0.24453182518482208\n",
      "Epoch: 2 | Discriminator Real Loss: [0.18278978765010834, 1.0] | Discriminator Fake Loss: [1.1995134353637695, 0.0] | CTGAN Loss: 0.5366188883781433\n",
      "Epoch: 3 | Discriminator Real Loss: [0.3845592737197876, 1.0] | Discriminator Fake Loss: [0.7384728789329529, 0.1875] | CTGAN Loss: 0.8255638480186462\n",
      "Epoch: 4 | Discriminator Real Loss: [0.5918558835983276, 0.96875] | Discriminator Fake Loss: [0.525309145450592, 0.9375] | CTGAN Loss: 1.0672187805175781\n",
      "Epoch: 5 | Discriminator Real Loss: [0.6832147240638733, 0.5] | Discriminator Fake Loss: [0.426006555557251, 1.0] | CTGAN Loss: 1.207632064819336\n",
      "Epoch: 6 | Discriminator Real Loss: [0.6229738593101501, 0.90625] | Discriminator Fake Loss: [0.38593828678131104, 1.0] | CTGAN Loss: 1.2700843811035156\n",
      "Epoch: 7 | Discriminator Real Loss: [0.5006460547447205, 0.96875] | Discriminator Fake Loss: [0.3970787525177002, 0.9375] | CTGAN Loss: 1.4278838634490967\n",
      "Epoch: 8 | Discriminator Real Loss: [0.345356285572052, 1.0] | Discriminator Fake Loss: [0.3932031989097595, 0.9375] | CTGAN Loss: 1.6129984855651855\n",
      "Epoch: 9 | Discriminator Real Loss: [0.21996894478797913, 1.0] | Discriminator Fake Loss: [0.23975855112075806, 0.9375] | CTGAN Loss: 2.362978935241699\n",
      "Epoch: 10 | Discriminator Real Loss: [0.15545552968978882, 1.0] | Discriminator Fake Loss: [0.12633292376995087, 0.96875] | CTGAN Loss: 3.202630043029785\n",
      "Epoch: 11 | Discriminator Real Loss: [0.13432829082012177, 1.0] | Discriminator Fake Loss: [0.07741428911685944, 0.96875] | CTGAN Loss: 4.225795269012451\n",
      "Epoch: 12 | Discriminator Real Loss: [0.08132694661617279, 1.0] | Discriminator Fake Loss: [0.046189114451408386, 0.96875] | CTGAN Loss: 5.010759353637695\n",
      "Epoch: 13 | Discriminator Real Loss: [0.020777516067028046, 1.0] | Discriminator Fake Loss: [0.053920481353998184, 1.0] | CTGAN Loss: 6.199450492858887\n",
      "Epoch: 14 | Discriminator Real Loss: [0.02135496959090233, 1.0] | Discriminator Fake Loss: [0.02727285958826542, 1.0] | CTGAN Loss: 7.664416790008545\n",
      "Epoch: 15 | Discriminator Real Loss: [0.0233866386115551, 1.0] | Discriminator Fake Loss: [0.05124358832836151, 0.96875] | CTGAN Loss: 9.416854858398438\n",
      "Epoch: 16 | Discriminator Real Loss: [0.046654727309942245, 0.96875] | Discriminator Fake Loss: [0.1866699755191803, 0.9375] | CTGAN Loss: 10.780441284179688\n",
      "Epoch: 17 | Discriminator Real Loss: [0.2333686798810959, 0.90625] | Discriminator Fake Loss: [0.07223071157932281, 0.96875] | CTGAN Loss: 9.78211784362793\n",
      "Epoch: 18 | Discriminator Real Loss: [0.006897678133100271, 1.0] | Discriminator Fake Loss: [0.43100303411483765, 0.90625] | CTGAN Loss: 11.644214630126953\n",
      "Epoch: 19 | Discriminator Real Loss: [0.015314770862460136, 1.0] | Discriminator Fake Loss: [0.42324182391166687, 0.9375] | CTGAN Loss: 13.599359512329102\n",
      "Epoch: 20 | Discriminator Real Loss: [0.22372488677501678, 0.90625] | Discriminator Fake Loss: [0.07222137600183487, 0.96875] | CTGAN Loss: 12.56297492980957\n",
      "Epoch: 21 | Discriminator Real Loss: [0.14228790998458862, 0.9375] | Discriminator Fake Loss: [0.3640030026435852, 0.9375] | CTGAN Loss: 10.070356369018555\n",
      "Epoch: 22 | Discriminator Real Loss: [0.005332788918167353, 1.0] | Discriminator Fake Loss: [0.9710719585418701, 0.6875] | CTGAN Loss: 11.243093490600586\n",
      "Epoch: 23 | Discriminator Real Loss: [0.13767948746681213, 0.9375] | Discriminator Fake Loss: [0.10655477643013, 0.9375] | CTGAN Loss: 14.590173721313477\n",
      "Epoch: 24 | Discriminator Real Loss: [0.8942786455154419, 0.6875] | Discriminator Fake Loss: [0.04631582647562027, 0.96875] | CTGAN Loss: 10.525266647338867\n",
      "Epoch: 25 | Discriminator Real Loss: [0.05750701203942299, 1.0] | Discriminator Fake Loss: [1.3635027408599854, 0.6875] | CTGAN Loss: 8.881829261779785\n",
      "Epoch: 26 | Discriminator Real Loss: [0.01844114437699318, 1.0] | Discriminator Fake Loss: [0.46624627709388733, 0.875] | CTGAN Loss: 9.039756774902344\n",
      "Epoch: 27 | Discriminator Real Loss: [0.08080744743347168, 1.0] | Discriminator Fake Loss: [0.4671541452407837, 0.8125] | CTGAN Loss: 8.934211730957031\n",
      "Epoch: 28 | Discriminator Real Loss: [0.40678107738494873, 0.75] | Discriminator Fake Loss: [0.11559262126684189, 0.96875] | CTGAN Loss: 8.042006492614746\n",
      "Epoch: 29 | Discriminator Real Loss: [0.40752020478248596, 0.84375] | Discriminator Fake Loss: [0.11112558096647263, 0.96875] | CTGAN Loss: 6.457110404968262\n",
      "Epoch: 30 | Discriminator Real Loss: [0.11720462888479233, 1.0] | Discriminator Fake Loss: [0.4059532582759857, 0.75] | CTGAN Loss: 5.8363037109375\n",
      "Epoch: 31 | Discriminator Real Loss: [0.09138631075620651, 1.0] | Discriminator Fake Loss: [0.29517674446105957, 0.875] | CTGAN Loss: 5.806478500366211\n",
      "Epoch: 32 | Discriminator Real Loss: [0.11320933699607849, 1.0] | Discriminator Fake Loss: [0.10024648904800415, 0.9375] | CTGAN Loss: 6.015856742858887\n",
      "Epoch: 33 | Discriminator Real Loss: [0.1941206455230713, 0.96875] | Discriminator Fake Loss: [0.11790087074041367, 0.96875] | CTGAN Loss: 5.8647003173828125\n",
      "Epoch: 34 | Discriminator Real Loss: [0.23258614540100098, 0.96875] | Discriminator Fake Loss: [0.1420070379972458, 0.9375] | CTGAN Loss: 5.933669090270996\n",
      "Epoch: 35 | Discriminator Real Loss: [0.08616743981838226, 1.0] | Discriminator Fake Loss: [0.32869765162467957, 0.90625] | CTGAN Loss: 4.910736083984375\n",
      "Epoch: 36 | Discriminator Real Loss: [0.12572605907917023, 1.0] | Discriminator Fake Loss: [0.08580327033996582, 0.9375] | CTGAN Loss: 6.236218452453613\n",
      "Epoch: 37 | Discriminator Real Loss: [0.10111008584499359, 1.0] | Discriminator Fake Loss: [0.2750858962535858, 0.90625] | CTGAN Loss: 5.532329559326172\n",
      "Epoch: 38 | Discriminator Real Loss: [0.08319965749979019, 1.0] | Discriminator Fake Loss: [0.25670307874679565, 0.90625] | CTGAN Loss: 6.914121627807617\n",
      "Epoch: 39 | Discriminator Real Loss: [0.1377047300338745, 1.0] | Discriminator Fake Loss: [0.10852012038230896, 0.96875] | CTGAN Loss: 6.706466197967529\n",
      "Epoch: 40 | Discriminator Real Loss: [0.2678432762622833, 0.9375] | Discriminator Fake Loss: [0.12222335487604141, 0.9375] | CTGAN Loss: 6.81546688079834\n",
      "Epoch: 41 | Discriminator Real Loss: [0.12109939754009247, 1.0] | Discriminator Fake Loss: [0.3497140407562256, 0.84375] | CTGAN Loss: 6.073084831237793\n",
      "Epoch: 42 | Discriminator Real Loss: [0.12357904762029648, 0.96875] | Discriminator Fake Loss: [0.24324311316013336, 0.84375] | CTGAN Loss: 6.674907684326172\n",
      "Epoch: 43 | Discriminator Real Loss: [0.09587156772613525, 1.0] | Discriminator Fake Loss: [0.22155451774597168, 0.875] | CTGAN Loss: 8.289396286010742\n",
      "Epoch: 44 | Discriminator Real Loss: [0.2153194546699524, 0.9375] | Discriminator Fake Loss: [0.10315848141908646, 0.96875] | CTGAN Loss: 7.836747646331787\n",
      "Epoch: 45 | Discriminator Real Loss: [0.3146248757839203, 0.90625] | Discriminator Fake Loss: [0.4235369563102722, 0.75] | CTGAN Loss: 8.050512313842773\n",
      "Epoch: 46 | Discriminator Real Loss: [0.10668979585170746, 1.0] | Discriminator Fake Loss: [0.53313809633255, 0.75] | CTGAN Loss: 9.152393341064453\n",
      "Epoch: 47 | Discriminator Real Loss: [0.49831685423851013, 0.75] | Discriminator Fake Loss: [0.12240014225244522, 0.90625] | CTGAN Loss: 8.645904541015625\n",
      "Epoch: 48 | Discriminator Real Loss: [0.24259799718856812, 0.84375] | Discriminator Fake Loss: [0.23576968908309937, 0.875] | CTGAN Loss: 7.7513203620910645\n",
      "Epoch: 49 | Discriminator Real Loss: [0.17305244505405426, 0.9375] | Discriminator Fake Loss: [0.3630398213863373, 0.84375] | CTGAN Loss: 7.488055229187012\n",
      "Epoch: 50 | Discriminator Real Loss: [0.3024211525917053, 0.875] | Discriminator Fake Loss: [0.14169716835021973, 0.90625] | CTGAN Loss: 7.618776321411133\n",
      "Epoch: 51 | Discriminator Real Loss: [0.20231226086616516, 0.9375] | Discriminator Fake Loss: [0.17475059628486633, 0.9375] | CTGAN Loss: 7.130704402923584\n",
      "Epoch: 52 | Discriminator Real Loss: [0.21438118815422058, 0.9375] | Discriminator Fake Loss: [0.33501115441322327, 0.8125] | CTGAN Loss: 6.286169528961182\n",
      "Epoch: 53 | Discriminator Real Loss: [0.2666293978691101, 0.875] | Discriminator Fake Loss: [0.3668457567691803, 0.84375] | CTGAN Loss: 6.967406749725342\n",
      "Epoch: 54 | Discriminator Real Loss: [0.23344910144805908, 0.96875] | Discriminator Fake Loss: [0.2989949882030487, 0.84375] | CTGAN Loss: 6.3395609855651855\n",
      "Epoch: 55 | Discriminator Real Loss: [0.23863157629966736, 0.90625] | Discriminator Fake Loss: [0.2385999709367752, 0.9375] | CTGAN Loss: 7.224215030670166\n",
      "Epoch: 56 | Discriminator Real Loss: [0.22687804698944092, 0.90625] | Discriminator Fake Loss: [0.281375527381897, 0.875] | CTGAN Loss: 5.758955001831055\n",
      "Epoch: 57 | Discriminator Real Loss: [0.174889475107193, 0.9375] | Discriminator Fake Loss: [0.4561852216720581, 0.84375] | CTGAN Loss: 6.434859275817871\n",
      "Epoch: 58 | Discriminator Real Loss: [0.14090126752853394, 1.0] | Discriminator Fake Loss: [0.23711815476417542, 0.875] | CTGAN Loss: 7.24913215637207\n",
      "Epoch: 59 | Discriminator Real Loss: [0.4509333372116089, 0.71875] | Discriminator Fake Loss: [0.24804627895355225, 0.875] | CTGAN Loss: 6.440460681915283\n",
      "Epoch: 60 | Discriminator Real Loss: [0.21042093634605408, 0.96875] | Discriminator Fake Loss: [0.3849315643310547, 0.8125] | CTGAN Loss: 5.600805282592773\n",
      "Epoch: 61 | Discriminator Real Loss: [0.13839149475097656, 0.96875] | Discriminator Fake Loss: [0.3887840509414673, 0.84375] | CTGAN Loss: 7.004462242126465\n",
      "Epoch: 62 | Discriminator Real Loss: [0.2110031545162201, 0.96875] | Discriminator Fake Loss: [0.23254308104515076, 0.84375] | CTGAN Loss: 8.171289443969727\n",
      "Epoch: 63 | Discriminator Real Loss: [0.5700547099113464, 0.75] | Discriminator Fake Loss: [0.045493531972169876, 1.0] | CTGAN Loss: 5.803125381469727\n",
      "Epoch: 64 | Discriminator Real Loss: [0.13356703519821167, 0.96875] | Discriminator Fake Loss: [0.3365548253059387, 0.84375] | CTGAN Loss: 5.0966691970825195\n",
      "Epoch: 65 | Discriminator Real Loss: [0.13121505081653595, 0.9375] | Discriminator Fake Loss: [0.6285629868507385, 0.78125] | CTGAN Loss: 4.8986029624938965\n",
      "Epoch: 66 | Discriminator Real Loss: [0.11117707192897797, 0.96875] | Discriminator Fake Loss: [0.2774169445037842, 0.90625] | CTGAN Loss: 6.658409118652344\n",
      "Epoch: 67 | Discriminator Real Loss: [0.44354116916656494, 0.78125] | Discriminator Fake Loss: [0.43501201272010803, 0.78125] | CTGAN Loss: 7.360348701477051\n",
      "Epoch: 68 | Discriminator Real Loss: [0.21872147917747498, 0.9375] | Discriminator Fake Loss: [0.1555052101612091, 0.90625] | CTGAN Loss: 5.7881178855896\n",
      "Epoch: 69 | Discriminator Real Loss: [0.2953927516937256, 0.90625] | Discriminator Fake Loss: [0.40510058403015137, 0.8125] | CTGAN Loss: 5.007420539855957\n",
      "Epoch: 70 | Discriminator Real Loss: [0.24482090771198273, 0.90625] | Discriminator Fake Loss: [0.31853026151657104, 0.84375] | CTGAN Loss: 6.552663803100586\n",
      "Epoch: 71 | Discriminator Real Loss: [0.24050277471542358, 0.875] | Discriminator Fake Loss: [0.4198994040489197, 0.75] | CTGAN Loss: 6.6691789627075195\n",
      "Epoch: 72 | Discriminator Real Loss: [0.44890496134757996, 0.78125] | Discriminator Fake Loss: [0.28420984745025635, 0.84375] | CTGAN Loss: 6.017965316772461\n",
      "Epoch: 73 | Discriminator Real Loss: [0.4661614000797272, 0.75] | Discriminator Fake Loss: [0.3279092013835907, 0.84375] | CTGAN Loss: 5.063179969787598\n",
      "Epoch: 74 | Discriminator Real Loss: [0.12349936366081238, 0.96875] | Discriminator Fake Loss: [0.6609790325164795, 0.65625] | CTGAN Loss: 4.841668605804443\n",
      "Epoch: 75 | Discriminator Real Loss: [0.24482649564743042, 0.9375] | Discriminator Fake Loss: [0.1817229986190796, 0.90625] | CTGAN Loss: 6.318141937255859\n",
      "Epoch: 76 | Discriminator Real Loss: [0.5611394643783569, 0.71875] | Discriminator Fake Loss: [0.15770742297172546, 0.90625] | CTGAN Loss: 4.843897819519043\n",
      "Epoch: 77 | Discriminator Real Loss: [0.4063749313354492, 0.84375] | Discriminator Fake Loss: [0.37855806946754456, 0.78125] | CTGAN Loss: 3.864335775375366\n",
      "Epoch: 78 | Discriminator Real Loss: [0.18477106094360352, 0.96875] | Discriminator Fake Loss: [0.4790847599506378, 0.78125] | CTGAN Loss: 3.417786121368408\n",
      "Epoch: 79 | Discriminator Real Loss: [0.1107289046049118, 1.0] | Discriminator Fake Loss: [0.5490171909332275, 0.78125] | CTGAN Loss: 4.311676979064941\n",
      "Epoch: 80 | Discriminator Real Loss: [0.3337169885635376, 0.90625] | Discriminator Fake Loss: [0.13819168508052826, 0.9375] | CTGAN Loss: 5.865729331970215\n",
      "Epoch: 81 | Discriminator Real Loss: [0.6338736414909363, 0.59375] | Discriminator Fake Loss: [0.3961167335510254, 0.78125] | CTGAN Loss: 4.765976905822754\n",
      "Epoch: 82 | Discriminator Real Loss: [0.2756798565387726, 0.90625] | Discriminator Fake Loss: [0.5598991513252258, 0.625] | CTGAN Loss: 4.753725051879883\n",
      "Epoch: 83 | Discriminator Real Loss: [0.3373607397079468, 0.875] | Discriminator Fake Loss: [0.3942568898200989, 0.75] | CTGAN Loss: 5.365847587585449\n",
      "Epoch: 84 | Discriminator Real Loss: [0.522985577583313, 0.78125] | Discriminator Fake Loss: [0.16848787665367126, 0.9375] | CTGAN Loss: 4.456899642944336\n",
      "Epoch: 85 | Discriminator Real Loss: [0.49076420068740845, 0.84375] | Discriminator Fake Loss: [0.5450753569602966, 0.6875] | CTGAN Loss: 3.4158973693847656\n",
      "Epoch: 86 | Discriminator Real Loss: [0.25203561782836914, 0.90625] | Discriminator Fake Loss: [0.4311802089214325, 0.8125] | CTGAN Loss: 3.469208002090454\n",
      "Epoch: 87 | Discriminator Real Loss: [0.3261457681655884, 0.875] | Discriminator Fake Loss: [0.2481106072664261, 0.90625] | CTGAN Loss: 3.928773880004883\n",
      "Epoch: 88 | Discriminator Real Loss: [0.2539016604423523, 0.9375] | Discriminator Fake Loss: [0.45855432748794556, 0.6875] | CTGAN Loss: 4.058476448059082\n",
      "Epoch: 89 | Discriminator Real Loss: [0.5034310817718506, 0.78125] | Discriminator Fake Loss: [0.3329613208770752, 0.78125] | CTGAN Loss: 3.874314785003662\n",
      "Epoch: 90 | Discriminator Real Loss: [0.3707355260848999, 0.875] | Discriminator Fake Loss: [0.389408141374588, 0.78125] | CTGAN Loss: 3.459857940673828\n",
      "Epoch: 91 | Discriminator Real Loss: [0.3085898160934448, 0.90625] | Discriminator Fake Loss: [0.2384023517370224, 0.9375] | CTGAN Loss: 3.266082286834717\n",
      "Epoch: 92 | Discriminator Real Loss: [0.38257765769958496, 0.90625] | Discriminator Fake Loss: [0.4134879410266876, 0.78125] | CTGAN Loss: 3.2322998046875\n",
      "Epoch: 93 | Discriminator Real Loss: [0.34534603357315063, 0.875] | Discriminator Fake Loss: [0.23640289902687073, 0.875] | CTGAN Loss: 3.441270351409912\n",
      "Epoch: 94 | Discriminator Real Loss: [0.22849702835083008, 0.90625] | Discriminator Fake Loss: [0.48542511463165283, 0.6875] | CTGAN Loss: 2.992851495742798\n",
      "Epoch: 95 | Discriminator Real Loss: [0.3750983476638794, 0.8125] | Discriminator Fake Loss: [0.23636531829833984, 0.875] | CTGAN Loss: 5.053672790527344\n",
      "Epoch: 96 | Discriminator Real Loss: [0.43144580721855164, 0.78125] | Discriminator Fake Loss: [0.23275594413280487, 0.90625] | CTGAN Loss: 3.6850602626800537\n",
      "Epoch: 97 | Discriminator Real Loss: [0.2256183624267578, 0.9375] | Discriminator Fake Loss: [0.33360201120376587, 0.84375] | CTGAN Loss: 3.0315403938293457\n",
      "Epoch: 98 | Discriminator Real Loss: [0.10532866418361664, 1.0] | Discriminator Fake Loss: [0.3431563377380371, 0.8125] | CTGAN Loss: 4.306561470031738\n",
      "Epoch: 99 | Discriminator Real Loss: [0.2644883394241333, 0.90625] | Discriminator Fake Loss: [0.21255522966384888, 0.875] | CTGAN Loss: 4.984797954559326\n",
      "Epoch: 100 | Discriminator Real Loss: [0.3963848352432251, 0.84375] | Discriminator Fake Loss: [0.14515307545661926, 0.9375] | CTGAN Loss: 3.7751173973083496\n",
      "Epoch: 101 | Discriminator Real Loss: [0.22723451256752014, 0.96875] | Discriminator Fake Loss: [0.512082576751709, 0.75] | CTGAN Loss: 3.291141986846924\n",
      "Epoch: 102 | Discriminator Real Loss: [0.18573057651519775, 0.9375] | Discriminator Fake Loss: [0.18983536958694458, 0.90625] | CTGAN Loss: 4.562346458435059\n",
      "Epoch: 103 | Discriminator Real Loss: [0.3736063838005066, 0.8125] | Discriminator Fake Loss: [0.2801806628704071, 0.875] | CTGAN Loss: 3.8445897102355957\n",
      "Epoch: 104 | Discriminator Real Loss: [0.29417866468429565, 0.875] | Discriminator Fake Loss: [0.48930129408836365, 0.71875] | CTGAN Loss: 4.110197067260742\n",
      "Epoch: 105 | Discriminator Real Loss: [0.21847635507583618, 0.9375] | Discriminator Fake Loss: [0.34395653009414673, 0.84375] | CTGAN Loss: 4.330934047698975\n",
      "Epoch: 106 | Discriminator Real Loss: [0.5025590658187866, 0.71875] | Discriminator Fake Loss: [0.33939579129219055, 0.875] | CTGAN Loss: 3.5750107765197754\n",
      "Epoch: 107 | Discriminator Real Loss: [0.23876196146011353, 0.90625] | Discriminator Fake Loss: [0.7892501354217529, 0.53125] | CTGAN Loss: 4.322575569152832\n",
      "Epoch: 108 | Discriminator Real Loss: [0.25563517212867737, 0.96875] | Discriminator Fake Loss: [0.09453796595335007, 0.96875] | CTGAN Loss: 5.292106628417969\n",
      "Epoch: 109 | Discriminator Real Loss: [0.7473348379135132, 0.59375] | Discriminator Fake Loss: [0.4977259635925293, 0.75] | CTGAN Loss: 2.9462053775787354\n",
      "Epoch: 110 | Discriminator Real Loss: [0.1281270682811737, 1.0] | Discriminator Fake Loss: [0.4257846772670746, 0.78125] | CTGAN Loss: 3.0573787689208984\n",
      "Epoch: 111 | Discriminator Real Loss: [0.24197284877300262, 0.96875] | Discriminator Fake Loss: [0.4108272194862366, 0.75] | CTGAN Loss: 4.2466936111450195\n",
      "Epoch: 112 | Discriminator Real Loss: [0.517882227897644, 0.71875] | Discriminator Fake Loss: [0.19462241232395172, 0.90625] | CTGAN Loss: 3.4182753562927246\n",
      "Epoch: 113 | Discriminator Real Loss: [0.3878781199455261, 0.84375] | Discriminator Fake Loss: [0.46255937218666077, 0.78125] | CTGAN Loss: 3.4848179817199707\n",
      "Epoch: 114 | Discriminator Real Loss: [0.14963078498840332, 1.0] | Discriminator Fake Loss: [0.4369654357433319, 0.78125] | CTGAN Loss: 3.0935776233673096\n",
      "Epoch: 115 | Discriminator Real Loss: [0.14949949085712433, 0.96875] | Discriminator Fake Loss: [0.14980216324329376, 0.90625] | CTGAN Loss: 4.198757648468018\n",
      "Epoch: 116 | Discriminator Real Loss: [0.32865801453590393, 0.875] | Discriminator Fake Loss: [0.20502984523773193, 0.90625] | CTGAN Loss: 4.100135803222656\n",
      "Epoch: 117 | Discriminator Real Loss: [0.26592421531677246, 0.90625] | Discriminator Fake Loss: [0.27783000469207764, 0.90625] | CTGAN Loss: 2.793752670288086\n",
      "Epoch: 118 | Discriminator Real Loss: [0.11661352217197418, 0.96875] | Discriminator Fake Loss: [0.3706611096858978, 0.8125] | CTGAN Loss: 4.226142406463623\n",
      "Epoch: 119 | Discriminator Real Loss: [0.15803326666355133, 0.96875] | Discriminator Fake Loss: [0.14790168404579163, 0.9375] | CTGAN Loss: 5.686352252960205\n",
      "Epoch: 120 | Discriminator Real Loss: [0.3042539358139038, 0.96875] | Discriminator Fake Loss: [0.14419472217559814, 0.9375] | CTGAN Loss: 4.466257095336914\n",
      "Epoch: 121 | Discriminator Real Loss: [0.31476128101348877, 0.875] | Discriminator Fake Loss: [0.24829259514808655, 0.90625] | CTGAN Loss: 4.452002048492432\n",
      "Epoch: 122 | Discriminator Real Loss: [0.06001892313361168, 1.0] | Discriminator Fake Loss: [0.22814200818538666, 0.875] | CTGAN Loss: 5.689118385314941\n",
      "Epoch: 123 | Discriminator Real Loss: [0.21041926741600037, 0.9375] | Discriminator Fake Loss: [0.038966104388237, 1.0] | CTGAN Loss: 7.098789215087891\n",
      "Epoch: 124 | Discriminator Real Loss: [0.26642167568206787, 0.875] | Discriminator Fake Loss: [0.13123761117458344, 0.90625] | CTGAN Loss: 5.604374885559082\n",
      "Epoch: 125 | Discriminator Real Loss: [0.09445415437221527, 1.0] | Discriminator Fake Loss: [0.2179340124130249, 0.9375] | CTGAN Loss: 4.454436302185059\n",
      "Epoch: 126 | Discriminator Real Loss: [0.03227457404136658, 1.0] | Discriminator Fake Loss: [0.38514161109924316, 0.875] | CTGAN Loss: 6.852507591247559\n",
      "Epoch: 127 | Discriminator Real Loss: [0.3004177212715149, 0.90625] | Discriminator Fake Loss: [0.030539415776729584, 1.0] | CTGAN Loss: 6.099201679229736\n",
      "Epoch: 128 | Discriminator Real Loss: [0.31530600786209106, 0.84375] | Discriminator Fake Loss: [0.6706665754318237, 0.71875] | CTGAN Loss: 5.26041316986084\n",
      "Epoch: 129 | Discriminator Real Loss: [0.2859678268432617, 0.84375] | Discriminator Fake Loss: [0.4155418574810028, 0.84375] | CTGAN Loss: 7.655340194702148\n",
      "Epoch: 130 | Discriminator Real Loss: [0.7453933358192444, 0.59375] | Discriminator Fake Loss: [0.2583844065666199, 0.90625] | CTGAN Loss: 4.103501796722412\n",
      "Epoch: 131 | Discriminator Real Loss: [0.04970185458660126, 1.0] | Discriminator Fake Loss: [0.811234176158905, 0.75] | CTGAN Loss: 5.524820327758789\n",
      "Epoch: 132 | Discriminator Real Loss: [0.1666555404663086, 0.96875] | Discriminator Fake Loss: [0.2851746082305908, 0.875] | CTGAN Loss: 8.213626861572266\n",
      "Epoch: 133 | Discriminator Real Loss: [0.8748025894165039, 0.59375] | Discriminator Fake Loss: [0.30137360095977783, 0.8125] | CTGAN Loss: 4.826641082763672\n",
      "Epoch: 134 | Discriminator Real Loss: [0.12935099005699158, 0.96875] | Discriminator Fake Loss: [0.48142680525779724, 0.84375] | CTGAN Loss: 3.2920775413513184\n",
      "Epoch: 135 | Discriminator Real Loss: [0.05647991597652435, 1.0] | Discriminator Fake Loss: [0.20418883860111237, 0.84375] | CTGAN Loss: 4.153968811035156\n",
      "Epoch: 136 | Discriminator Real Loss: [0.28760138154029846, 0.875] | Discriminator Fake Loss: [0.3510076403617859, 0.8125] | CTGAN Loss: 4.810962677001953\n",
      "Epoch: 137 | Discriminator Real Loss: [0.2791137099266052, 0.90625] | Discriminator Fake Loss: [0.43378883600234985, 0.71875] | CTGAN Loss: 4.697638511657715\n",
      "Epoch: 138 | Discriminator Real Loss: [0.5130088329315186, 0.8125] | Discriminator Fake Loss: [0.3073262572288513, 0.8125] | CTGAN Loss: 4.6834211349487305\n",
      "Epoch: 139 | Discriminator Real Loss: [0.31858545541763306, 0.84375] | Discriminator Fake Loss: [0.34163999557495117, 0.875] | CTGAN Loss: 3.9365673065185547\n",
      "Epoch: 140 | Discriminator Real Loss: [0.36566901206970215, 0.84375] | Discriminator Fake Loss: [0.21097137033939362, 0.90625] | CTGAN Loss: 4.450137138366699\n",
      "Epoch: 141 | Discriminator Real Loss: [0.2944479286670685, 0.96875] | Discriminator Fake Loss: [0.13063591718673706, 0.96875] | CTGAN Loss: 3.568326234817505\n",
      "Epoch: 142 | Discriminator Real Loss: [0.144871786236763, 0.96875] | Discriminator Fake Loss: [0.2266698032617569, 0.90625] | CTGAN Loss: 4.84976863861084\n",
      "Epoch: 143 | Discriminator Real Loss: [0.17961104214191437, 0.96875] | Discriminator Fake Loss: [0.10862722247838974, 0.96875] | CTGAN Loss: 4.868410110473633\n",
      "Epoch: 144 | Discriminator Real Loss: [0.0936909168958664, 1.0] | Discriminator Fake Loss: [0.09962254017591476, 1.0] | CTGAN Loss: 5.645402431488037\n",
      "Epoch: 145 | Discriminator Real Loss: [0.17655101418495178, 0.96875] | Discriminator Fake Loss: [0.08084684610366821, 0.96875] | CTGAN Loss: 6.110152244567871\n",
      "Epoch: 146 | Discriminator Real Loss: [0.13826943933963776, 1.0] | Discriminator Fake Loss: [0.12281908094882965, 0.9375] | CTGAN Loss: 5.314851760864258\n",
      "Epoch: 147 | Discriminator Real Loss: [0.06964613497257233, 0.96875] | Discriminator Fake Loss: [0.5660192966461182, 0.75] | CTGAN Loss: 5.847781181335449\n",
      "Epoch: 148 | Discriminator Real Loss: [0.14191444218158722, 0.9375] | Discriminator Fake Loss: [0.13599930703639984, 0.90625] | CTGAN Loss: 8.498550415039062\n",
      "Epoch: 149 | Discriminator Real Loss: [0.41566091775894165, 0.78125] | Discriminator Fake Loss: [0.2742166817188263, 0.90625] | CTGAN Loss: 5.915228843688965\n",
      "Epoch: 150 | Discriminator Real Loss: [0.21252597868442535, 0.96875] | Discriminator Fake Loss: [0.4058220088481903, 0.8125] | CTGAN Loss: 5.625141143798828\n",
      "Epoch: 151 | Discriminator Real Loss: [0.11085966229438782, 0.96875] | Discriminator Fake Loss: [0.10427533090114594, 0.96875] | CTGAN Loss: 9.16855239868164\n",
      "Epoch: 152 | Discriminator Real Loss: [0.7551753520965576, 0.75] | Discriminator Fake Loss: [0.2363070696592331, 0.875] | CTGAN Loss: 5.352154731750488\n",
      "Epoch: 153 | Discriminator Real Loss: [0.05450907349586487, 1.0] | Discriminator Fake Loss: [0.2563955783843994, 0.84375] | CTGAN Loss: 5.673175811767578\n",
      "Epoch: 154 | Discriminator Real Loss: [0.10343153774738312, 0.96875] | Discriminator Fake Loss: [0.06935051083564758, 0.96875] | CTGAN Loss: 6.852667808532715\n",
      "Epoch: 155 | Discriminator Real Loss: [0.15208935737609863, 0.96875] | Discriminator Fake Loss: [0.23029163479804993, 0.90625] | CTGAN Loss: 9.52989673614502\n",
      "Epoch: 156 | Discriminator Real Loss: [0.18490871787071228, 0.9375] | Discriminator Fake Loss: [0.14439822733402252, 0.875] | CTGAN Loss: 8.411335945129395\n",
      "Epoch: 157 | Discriminator Real Loss: [0.23771202564239502, 0.875] | Discriminator Fake Loss: [0.19771471619606018, 0.875] | CTGAN Loss: 6.970180034637451\n",
      "Epoch: 158 | Discriminator Real Loss: [0.06228633597493172, 1.0] | Discriminator Fake Loss: [0.3627893924713135, 0.8125] | CTGAN Loss: 6.659488201141357\n",
      "Epoch: 159 | Discriminator Real Loss: [0.057653531432151794, 1.0] | Discriminator Fake Loss: [0.03641706705093384, 1.0] | CTGAN Loss: 8.203651428222656\n",
      "Epoch: 160 | Discriminator Real Loss: [0.17477500438690186, 0.96875] | Discriminator Fake Loss: [0.03875160217285156, 1.0] | CTGAN Loss: 6.654209136962891\n",
      "Epoch: 161 | Discriminator Real Loss: [0.1450573205947876, 0.9375] | Discriminator Fake Loss: [0.4799003601074219, 0.78125] | CTGAN Loss: 7.015602111816406\n",
      "Epoch: 162 | Discriminator Real Loss: [0.13401204347610474, 0.96875] | Discriminator Fake Loss: [0.07601623982191086, 0.96875] | CTGAN Loss: 9.913154602050781\n",
      "Epoch: 163 | Discriminator Real Loss: [0.4730202257633209, 0.8125] | Discriminator Fake Loss: [0.4280721843242645, 0.84375] | CTGAN Loss: 4.893247604370117\n",
      "Epoch: 164 | Discriminator Real Loss: [0.13441389799118042, 0.96875] | Discriminator Fake Loss: [0.24085760116577148, 0.84375] | CTGAN Loss: 6.414820671081543\n",
      "Epoch: 165 | Discriminator Real Loss: [0.15100151300430298, 1.0] | Discriminator Fake Loss: [0.03425971791148186, 1.0] | CTGAN Loss: 6.584079742431641\n",
      "Epoch: 166 | Discriminator Real Loss: [0.21107199788093567, 0.90625] | Discriminator Fake Loss: [0.4796718955039978, 0.71875] | CTGAN Loss: 6.857865810394287\n",
      "Epoch: 167 | Discriminator Real Loss: [0.17651712894439697, 1.0] | Discriminator Fake Loss: [0.3601626455783844, 0.8125] | CTGAN Loss: 8.20749282836914\n",
      "Epoch: 168 | Discriminator Real Loss: [0.39039474725723267, 0.8125] | Discriminator Fake Loss: [0.157500758767128, 0.96875] | CTGAN Loss: 6.9647674560546875\n",
      "Epoch: 169 | Discriminator Real Loss: [0.18815651535987854, 0.9375] | Discriminator Fake Loss: [0.45940032601356506, 0.75] | CTGAN Loss: 5.837493419647217\n",
      "Epoch: 170 | Discriminator Real Loss: [0.047662604600191116, 1.0] | Discriminator Fake Loss: [0.18464970588684082, 0.90625] | CTGAN Loss: 7.292056083679199\n",
      "Epoch: 171 | Discriminator Real Loss: [0.29980403184890747, 0.90625] | Discriminator Fake Loss: [0.032103002071380615, 1.0] | CTGAN Loss: 6.960700511932373\n",
      "Epoch: 172 | Discriminator Real Loss: [0.11710045486688614, 0.96875] | Discriminator Fake Loss: [0.0425296425819397, 1.0] | CTGAN Loss: 4.599129676818848\n",
      "Epoch: 173 | Discriminator Real Loss: [0.03858436644077301, 1.0] | Discriminator Fake Loss: [0.48579126596450806, 0.78125] | CTGAN Loss: 5.769236087799072\n",
      "Epoch: 174 | Discriminator Real Loss: [0.038069404661655426, 1.0] | Discriminator Fake Loss: [0.08816716074943542, 0.96875] | CTGAN Loss: 9.656902313232422\n",
      "Epoch: 175 | Discriminator Real Loss: [0.6577514410018921, 0.71875] | Discriminator Fake Loss: [0.07078966498374939, 0.96875] | CTGAN Loss: 5.641768455505371\n",
      "Epoch: 176 | Discriminator Real Loss: [0.039315201342105865, 1.0] | Discriminator Fake Loss: [0.24506807327270508, 0.84375] | CTGAN Loss: 4.789572715759277\n",
      "Epoch: 177 | Discriminator Real Loss: [0.0337405726313591, 1.0] | Discriminator Fake Loss: [0.59065842628479, 0.6875] | CTGAN Loss: 5.585775375366211\n",
      "Epoch: 178 | Discriminator Real Loss: [0.16185761988162994, 0.9375] | Discriminator Fake Loss: [0.04888410493731499, 1.0] | CTGAN Loss: 8.850687026977539\n",
      "Epoch: 179 | Discriminator Real Loss: [0.6744890213012695, 0.71875] | Discriminator Fake Loss: [0.020683757960796356, 1.0] | CTGAN Loss: 4.714010238647461\n",
      "Epoch: 180 | Discriminator Real Loss: [0.05029991269111633, 1.0] | Discriminator Fake Loss: [0.8215627074241638, 0.625] | CTGAN Loss: 4.365344047546387\n",
      "Epoch: 181 | Discriminator Real Loss: [0.027611784636974335, 1.0] | Discriminator Fake Loss: [0.05734610930085182, 1.0] | CTGAN Loss: 7.13841438293457\n",
      "Epoch: 182 | Discriminator Real Loss: [0.3413848876953125, 0.875] | Discriminator Fake Loss: [0.03488091006875038, 0.96875] | CTGAN Loss: 7.637557506561279\n",
      "Epoch: 183 | Discriminator Real Loss: [0.13005851209163666, 1.0] | Discriminator Fake Loss: [0.07673124223947525, 1.0] | CTGAN Loss: 5.985774040222168\n",
      "Epoch: 184 | Discriminator Real Loss: [0.1109638661146164, 1.0] | Discriminator Fake Loss: [0.16901372373104095, 0.90625] | CTGAN Loss: 6.15152645111084\n",
      "Epoch: 185 | Discriminator Real Loss: [0.1871958076953888, 0.9375] | Discriminator Fake Loss: [0.30156537890434265, 0.875] | CTGAN Loss: 5.743028163909912\n",
      "Epoch: 186 | Discriminator Real Loss: [0.22305434942245483, 0.9375] | Discriminator Fake Loss: [0.09999868273735046, 0.9375] | CTGAN Loss: 5.655657768249512\n",
      "Epoch: 187 | Discriminator Real Loss: [0.09169305860996246, 0.96875] | Discriminator Fake Loss: [0.09244713187217712, 0.9375] | CTGAN Loss: 6.290530204772949\n",
      "Epoch: 188 | Discriminator Real Loss: [0.11446420848369598, 0.96875] | Discriminator Fake Loss: [0.04023635759949684, 0.96875] | CTGAN Loss: 5.164402008056641\n",
      "Epoch: 189 | Discriminator Real Loss: [0.127903014421463, 0.9375] | Discriminator Fake Loss: [0.22395916283130646, 0.84375] | CTGAN Loss: 5.2451300621032715\n",
      "Epoch: 190 | Discriminator Real Loss: [0.11275790631771088, 0.96875] | Discriminator Fake Loss: [0.17755796015262604, 0.96875] | CTGAN Loss: 5.813858985900879\n",
      "Epoch: 191 | Discriminator Real Loss: [0.15837250649929047, 0.96875] | Discriminator Fake Loss: [0.09628959000110626, 0.96875] | CTGAN Loss: 6.075497150421143\n",
      "Epoch: 192 | Discriminator Real Loss: [0.06319525837898254, 1.0] | Discriminator Fake Loss: [0.23953558504581451, 0.8125] | CTGAN Loss: 6.770889759063721\n",
      "Epoch: 193 | Discriminator Real Loss: [0.2806777358055115, 0.84375] | Discriminator Fake Loss: [0.09421145915985107, 1.0] | CTGAN Loss: 5.862917900085449\n",
      "Epoch: 194 | Discriminator Real Loss: [0.1072671189904213, 0.96875] | Discriminator Fake Loss: [0.5240063071250916, 0.8125] | CTGAN Loss: 6.824372291564941\n",
      "Epoch: 195 | Discriminator Real Loss: [0.20483069121837616, 0.90625] | Discriminator Fake Loss: [0.1682383418083191, 0.9375] | CTGAN Loss: 6.325352668762207\n",
      "Epoch: 196 | Discriminator Real Loss: [0.4058806002140045, 0.875] | Discriminator Fake Loss: [0.2271592915058136, 0.875] | CTGAN Loss: 5.075353145599365\n",
      "Epoch: 197 | Discriminator Real Loss: [0.20322205126285553, 0.9375] | Discriminator Fake Loss: [0.6100950241088867, 0.71875] | CTGAN Loss: 5.435930252075195\n",
      "Epoch: 198 | Discriminator Real Loss: [0.15622368454933167, 0.96875] | Discriminator Fake Loss: [0.14914251863956451, 0.9375] | CTGAN Loss: 6.457726955413818\n",
      "Epoch: 199 | Discriminator Real Loss: [0.6225805282592773, 0.6875] | Discriminator Fake Loss: [0.43471771478652954, 0.8125] | CTGAN Loss: 4.5501885414123535\n",
      "Epoch: 200 | Discriminator Real Loss: [0.1000809594988823, 0.9375] | Discriminator Fake Loss: [0.6154664158821106, 0.6875] | CTGAN Loss: 4.409101963043213\n",
      "Epoch: 201 | Discriminator Real Loss: [0.18173997104167938, 0.9375] | Discriminator Fake Loss: [0.19015103578567505, 0.90625] | CTGAN Loss: 7.192013740539551\n",
      "Epoch: 202 | Discriminator Real Loss: [0.7409164905548096, 0.6875] | Discriminator Fake Loss: [0.27405959367752075, 0.875] | CTGAN Loss: 3.5873208045959473\n",
      "Epoch: 203 | Discriminator Real Loss: [0.06657296419143677, 1.0] | Discriminator Fake Loss: [0.7569273710250854, 0.6875] | CTGAN Loss: 3.1527609825134277\n",
      "Epoch: 204 | Discriminator Real Loss: [0.07063711434602737, 1.0] | Discriminator Fake Loss: [0.223646879196167, 0.875] | CTGAN Loss: 5.578311920166016\n",
      "Epoch: 205 | Discriminator Real Loss: [0.38858562707901, 0.90625] | Discriminator Fake Loss: [0.27612853050231934, 0.84375] | CTGAN Loss: 6.505040168762207\n",
      "Epoch: 206 | Discriminator Real Loss: [0.576360821723938, 0.71875] | Discriminator Fake Loss: [0.183519184589386, 0.9375] | CTGAN Loss: 3.74576997756958\n",
      "Epoch: 207 | Discriminator Real Loss: [0.16946463286876678, 0.96875] | Discriminator Fake Loss: [0.5590482950210571, 0.65625] | CTGAN Loss: 3.3362672328948975\n",
      "Epoch: 208 | Discriminator Real Loss: [0.10534889996051788, 1.0] | Discriminator Fake Loss: [0.35837599635124207, 0.78125] | CTGAN Loss: 6.673413276672363\n",
      "Epoch: 209 | Discriminator Real Loss: [0.5642983913421631, 0.71875] | Discriminator Fake Loss: [0.06852094829082489, 0.96875] | CTGAN Loss: 5.364133834838867\n",
      "Epoch: 210 | Discriminator Real Loss: [0.36365365982055664, 0.8125] | Discriminator Fake Loss: [0.4212731719017029, 0.8125] | CTGAN Loss: 4.298221111297607\n",
      "Epoch: 211 | Discriminator Real Loss: [0.13549891114234924, 0.96875] | Discriminator Fake Loss: [0.37665969133377075, 0.8125] | CTGAN Loss: 4.439922332763672\n",
      "Epoch: 212 | Discriminator Real Loss: [0.1060853973031044, 1.0] | Discriminator Fake Loss: [0.07601644843816757, 1.0] | CTGAN Loss: 5.325934410095215\n",
      "Epoch: 213 | Discriminator Real Loss: [0.19825243949890137, 0.96875] | Discriminator Fake Loss: [0.19627243280410767, 0.90625] | CTGAN Loss: 5.700018882751465\n",
      "Epoch: 214 | Discriminator Real Loss: [0.3412654995918274, 0.8125] | Discriminator Fake Loss: [0.38741809129714966, 0.78125] | CTGAN Loss: 5.733468055725098\n",
      "Epoch: 215 | Discriminator Real Loss: [0.20860135555267334, 0.9375] | Discriminator Fake Loss: [0.21736833453178406, 0.90625] | CTGAN Loss: 5.864892959594727\n",
      "Epoch: 216 | Discriminator Real Loss: [0.30357998609542847, 0.875] | Discriminator Fake Loss: [0.2458198368549347, 0.875] | CTGAN Loss: 5.647808074951172\n",
      "Epoch: 217 | Discriminator Real Loss: [0.20055457949638367, 0.96875] | Discriminator Fake Loss: [0.17677932977676392, 0.9375] | CTGAN Loss: 4.531955242156982\n",
      "Epoch: 218 | Discriminator Real Loss: [0.14197885990142822, 0.96875] | Discriminator Fake Loss: [0.4273887276649475, 0.8125] | CTGAN Loss: 4.731794357299805\n",
      "Epoch: 219 | Discriminator Real Loss: [0.17401938140392303, 0.9375] | Discriminator Fake Loss: [0.1782555878162384, 0.90625] | CTGAN Loss: 5.985049247741699\n",
      "Epoch: 220 | Discriminator Real Loss: [0.5774294137954712, 0.78125] | Discriminator Fake Loss: [0.2878625988960266, 0.84375] | CTGAN Loss: 3.3655765056610107\n",
      "Epoch: 221 | Discriminator Real Loss: [0.26111599802970886, 0.90625] | Discriminator Fake Loss: [0.543571949005127, 0.71875] | CTGAN Loss: 3.00553822517395\n",
      "Epoch: 222 | Discriminator Real Loss: [0.057020511478185654, 1.0] | Discriminator Fake Loss: [0.463545024394989, 0.71875] | CTGAN Loss: 5.549375534057617\n",
      "Epoch: 223 | Discriminator Real Loss: [0.6141489744186401, 0.6875] | Discriminator Fake Loss: [0.2091483175754547, 0.875] | CTGAN Loss: 4.513813495635986\n",
      "Epoch: 224 | Discriminator Real Loss: [0.5894443392753601, 0.6875] | Discriminator Fake Loss: [0.7264279723167419, 0.59375] | CTGAN Loss: 2.982283592224121\n",
      "Epoch: 225 | Discriminator Real Loss: [0.2815435528755188, 0.90625] | Discriminator Fake Loss: [0.7523400187492371, 0.65625] | CTGAN Loss: 3.3425607681274414\n",
      "Epoch: 226 | Discriminator Real Loss: [0.5352256298065186, 0.78125] | Discriminator Fake Loss: [0.2610194683074951, 0.90625] | CTGAN Loss: 3.380857467651367\n",
      "Epoch: 227 | Discriminator Real Loss: [0.6567535996437073, 0.71875] | Discriminator Fake Loss: [0.46137502789497375, 0.78125] | CTGAN Loss: 2.954834461212158\n",
      "Epoch: 228 | Discriminator Real Loss: [0.2732255458831787, 0.9375] | Discriminator Fake Loss: [0.5982116460800171, 0.71875] | CTGAN Loss: 2.0982956886291504\n",
      "Epoch: 229 | Discriminator Real Loss: [0.21109183132648468, 0.96875] | Discriminator Fake Loss: [0.4390788972377777, 0.78125] | CTGAN Loss: 2.703002452850342\n",
      "Epoch: 230 | Discriminator Real Loss: [0.3873260021209717, 0.875] | Discriminator Fake Loss: [0.2819059491157532, 0.84375] | CTGAN Loss: 3.0176210403442383\n",
      "Epoch: 231 | Discriminator Real Loss: [0.5631951093673706, 0.875] | Discriminator Fake Loss: [0.19348901510238647, 0.90625] | CTGAN Loss: 2.6748600006103516\n",
      "Epoch: 232 | Discriminator Real Loss: [0.31309258937835693, 0.90625] | Discriminator Fake Loss: [0.6906818151473999, 0.59375] | CTGAN Loss: 2.489055633544922\n",
      "Epoch: 233 | Discriminator Real Loss: [0.25645628571510315, 0.96875] | Discriminator Fake Loss: [0.6639343500137329, 0.625] | CTGAN Loss: 3.262312650680542\n",
      "Epoch: 234 | Discriminator Real Loss: [0.41549575328826904, 0.8125] | Discriminator Fake Loss: [0.21002158522605896, 0.875] | CTGAN Loss: 3.644129514694214\n",
      "Epoch: 235 | Discriminator Real Loss: [0.5889308452606201, 0.84375] | Discriminator Fake Loss: [0.26182663440704346, 0.9375] | CTGAN Loss: 3.948051929473877\n",
      "Epoch: 236 | Discriminator Real Loss: [0.366139680147171, 0.875] | Discriminator Fake Loss: [0.45406848192214966, 0.75] | CTGAN Loss: 2.6105265617370605\n",
      "Epoch: 237 | Discriminator Real Loss: [0.2173241376876831, 0.9375] | Discriminator Fake Loss: [0.4201464056968689, 0.78125] | CTGAN Loss: 2.58259654045105\n",
      "Epoch: 238 | Discriminator Real Loss: [0.29127073287963867, 0.90625] | Discriminator Fake Loss: [0.2279900312423706, 0.875] | CTGAN Loss: 3.4916372299194336\n",
      "Epoch: 239 | Discriminator Real Loss: [0.4241889715194702, 0.84375] | Discriminator Fake Loss: [0.16034367680549622, 0.9375] | CTGAN Loss: 2.9201507568359375\n",
      "Epoch: 240 | Discriminator Real Loss: [0.1765749156475067, 0.96875] | Discriminator Fake Loss: [0.3392443060874939, 0.84375] | CTGAN Loss: 3.1267409324645996\n",
      "Epoch: 241 | Discriminator Real Loss: [0.21489658951759338, 0.90625] | Discriminator Fake Loss: [0.499499648809433, 0.71875] | CTGAN Loss: 3.5212292671203613\n",
      "Epoch: 242 | Discriminator Real Loss: [0.22315488755702972, 0.90625] | Discriminator Fake Loss: [0.225867360830307, 0.90625] | CTGAN Loss: 5.096575736999512\n",
      "Epoch: 243 | Discriminator Real Loss: [0.4208267033100128, 0.8125] | Discriminator Fake Loss: [0.3508656620979309, 0.84375] | CTGAN Loss: 3.5729148387908936\n",
      "Epoch: 244 | Discriminator Real Loss: [0.33313506841659546, 0.875] | Discriminator Fake Loss: [0.29392117261886597, 0.84375] | CTGAN Loss: 2.6785216331481934\n",
      "Epoch: 245 | Discriminator Real Loss: [0.21196173131465912, 0.90625] | Discriminator Fake Loss: [0.4202468693256378, 0.78125] | CTGAN Loss: 3.9685890674591064\n",
      "Epoch: 246 | Discriminator Real Loss: [0.2979089319705963, 0.875] | Discriminator Fake Loss: [0.19381117820739746, 0.9375] | CTGAN Loss: 4.752452373504639\n",
      "Epoch: 247 | Discriminator Real Loss: [0.34590592980384827, 0.875] | Discriminator Fake Loss: [0.2006082832813263, 0.90625] | CTGAN Loss: 3.9321022033691406\n",
      "Epoch: 248 | Discriminator Real Loss: [0.3289048373699188, 0.84375] | Discriminator Fake Loss: [0.7392306327819824, 0.65625] | CTGAN Loss: 3.8999457359313965\n",
      "Epoch: 249 | Discriminator Real Loss: [0.39407461881637573, 0.84375] | Discriminator Fake Loss: [0.18775948882102966, 0.90625] | CTGAN Loss: 5.012744903564453\n",
      "Epoch: 250 | Discriminator Real Loss: [1.0006306171417236, 0.53125] | Discriminator Fake Loss: [0.7054805755615234, 0.71875] | CTGAN Loss: 1.2924582958221436\n",
      "Epoch: 251 | Discriminator Real Loss: [0.10284633934497833, 1.0] | Discriminator Fake Loss: [1.1199028491973877, 0.46875] | CTGAN Loss: 2.2937302589416504\n",
      "Epoch: 252 | Discriminator Real Loss: [0.15963423252105713, 0.96875] | Discriminator Fake Loss: [0.2876422703266144, 0.875] | CTGAN Loss: 4.1092023849487305\n",
      "Epoch: 253 | Discriminator Real Loss: [1.5450034141540527, 0.28125] | Discriminator Fake Loss: [0.35359686613082886, 0.78125] | CTGAN Loss: 1.6147664785385132\n",
      "Epoch: 254 | Discriminator Real Loss: [0.179086372256279, 0.96875] | Discriminator Fake Loss: [1.0781255960464478, 0.5] | CTGAN Loss: 1.6503007411956787\n",
      "Epoch: 255 | Discriminator Real Loss: [0.26071733236312866, 0.90625] | Discriminator Fake Loss: [0.3961600661277771, 0.8125] | CTGAN Loss: 2.263394355773926\n",
      "Epoch: 256 | Discriminator Real Loss: [0.6360869407653809, 0.71875] | Discriminator Fake Loss: [0.5290385484695435, 0.75] | CTGAN Loss: 2.626927137374878\n",
      "Epoch: 257 | Discriminator Real Loss: [0.8000500202178955, 0.59375] | Discriminator Fake Loss: [0.6258055567741394, 0.65625] | CTGAN Loss: 1.32822585105896\n",
      "Epoch: 258 | Discriminator Real Loss: [0.3691820502281189, 0.84375] | Discriminator Fake Loss: [0.7466179728507996, 0.5] | CTGAN Loss: 1.664133071899414\n",
      "Epoch: 259 | Discriminator Real Loss: [0.28117939829826355, 0.9375] | Discriminator Fake Loss: [0.5071996450424194, 0.71875] | CTGAN Loss: 2.0045108795166016\n",
      "Epoch: 260 | Discriminator Real Loss: [0.3954908847808838, 0.84375] | Discriminator Fake Loss: [0.31244444847106934, 0.84375] | CTGAN Loss: 2.88899564743042\n",
      "Epoch: 261 | Discriminator Real Loss: [0.6136614084243774, 0.6875] | Discriminator Fake Loss: [0.24417826533317566, 0.9375] | CTGAN Loss: 2.2309279441833496\n",
      "Epoch: 262 | Discriminator Real Loss: [0.5450125932693481, 0.75] | Discriminator Fake Loss: [0.4076057970523834, 0.84375] | CTGAN Loss: 2.378336191177368\n",
      "Epoch: 263 | Discriminator Real Loss: [0.45116516947746277, 0.78125] | Discriminator Fake Loss: [0.388396680355072, 0.8125] | CTGAN Loss: 2.0705642700195312\n",
      "Epoch: 264 | Discriminator Real Loss: [0.29910868406295776, 0.96875] | Discriminator Fake Loss: [0.37369096279144287, 0.8125] | CTGAN Loss: 2.5106239318847656\n",
      "Epoch: 265 | Discriminator Real Loss: [0.3025028705596924, 0.875] | Discriminator Fake Loss: [0.3037795424461365, 0.84375] | CTGAN Loss: 2.8223471641540527\n",
      "Epoch: 266 | Discriminator Real Loss: [0.3555217981338501, 0.875] | Discriminator Fake Loss: [0.362959623336792, 0.8125] | CTGAN Loss: 2.954639434814453\n",
      "Epoch: 267 | Discriminator Real Loss: [0.396079421043396, 0.84375] | Discriminator Fake Loss: [0.22166894376277924, 0.9375] | CTGAN Loss: 3.351591110229492\n",
      "Epoch: 268 | Discriminator Real Loss: [0.4466841220855713, 0.8125] | Discriminator Fake Loss: [0.3518579304218292, 0.84375] | CTGAN Loss: 2.8801803588867188\n",
      "Epoch: 269 | Discriminator Real Loss: [0.23690888285636902, 0.96875] | Discriminator Fake Loss: [0.4207213521003723, 0.75] | CTGAN Loss: 3.2851476669311523\n",
      "Epoch: 270 | Discriminator Real Loss: [0.44031670689582825, 0.8125] | Discriminator Fake Loss: [0.3203520178794861, 0.875] | CTGAN Loss: 3.6972649097442627\n",
      "Epoch: 271 | Discriminator Real Loss: [0.34124457836151123, 0.90625] | Discriminator Fake Loss: [0.1605471670627594, 0.9375] | CTGAN Loss: 4.151151657104492\n",
      "Epoch: 272 | Discriminator Real Loss: [0.23121735453605652, 0.96875] | Discriminator Fake Loss: [0.4284895062446594, 0.78125] | CTGAN Loss: 3.472219467163086\n",
      "Epoch: 273 | Discriminator Real Loss: [0.39663517475128174, 0.875] | Discriminator Fake Loss: [0.41447722911834717, 0.75] | CTGAN Loss: 4.38372802734375\n",
      "Epoch: 274 | Discriminator Real Loss: [0.33627840876579285, 0.875] | Discriminator Fake Loss: [0.2977292239665985, 0.875] | CTGAN Loss: 3.7041616439819336\n",
      "Epoch: 275 | Discriminator Real Loss: [0.4260810613632202, 0.875] | Discriminator Fake Loss: [0.38743239641189575, 0.875] | CTGAN Loss: 3.2310659885406494\n",
      "Epoch: 276 | Discriminator Real Loss: [0.4284398853778839, 0.875] | Discriminator Fake Loss: [0.5972798466682434, 0.6875] | CTGAN Loss: 2.6459124088287354\n",
      "Epoch: 277 | Discriminator Real Loss: [0.34199562668800354, 0.875] | Discriminator Fake Loss: [0.5954478979110718, 0.625] | CTGAN Loss: 3.9052579402923584\n",
      "Epoch: 278 | Discriminator Real Loss: [0.5817691087722778, 0.78125] | Discriminator Fake Loss: [0.43663913011550903, 0.75] | CTGAN Loss: 3.2889328002929688\n",
      "Epoch: 279 | Discriminator Real Loss: [0.5826950073242188, 0.71875] | Discriminator Fake Loss: [0.41155022382736206, 0.75] | CTGAN Loss: 2.7619552612304688\n",
      "Epoch: 280 | Discriminator Real Loss: [0.3359927237033844, 0.90625] | Discriminator Fake Loss: [0.468990683555603, 0.75] | CTGAN Loss: 3.0631015300750732\n",
      "Epoch: 281 | Discriminator Real Loss: [0.29141759872436523, 0.875] | Discriminator Fake Loss: [0.357790470123291, 0.8125] | CTGAN Loss: 3.7436776161193848\n",
      "Epoch: 282 | Discriminator Real Loss: [0.9642994403839111, 0.6875] | Discriminator Fake Loss: [0.622693657875061, 0.625] | CTGAN Loss: 2.259896755218506\n",
      "Epoch: 283 | Discriminator Real Loss: [0.3289485573768616, 0.9375] | Discriminator Fake Loss: [0.5314961671829224, 0.6875] | CTGAN Loss: 2.0019755363464355\n",
      "Epoch: 284 | Discriminator Real Loss: [0.20146803557872772, 1.0] | Discriminator Fake Loss: [0.6786181926727295, 0.59375] | CTGAN Loss: 2.663151264190674\n",
      "Epoch: 285 | Discriminator Real Loss: [0.6177918910980225, 0.6875] | Discriminator Fake Loss: [0.2749655246734619, 0.90625] | CTGAN Loss: 3.075881004333496\n",
      "Epoch: 286 | Discriminator Real Loss: [0.7460755109786987, 0.5625] | Discriminator Fake Loss: [0.7917345762252808, 0.46875] | CTGAN Loss: 2.440426826477051\n",
      "Epoch: 287 | Discriminator Real Loss: [0.3380957245826721, 0.90625] | Discriminator Fake Loss: [0.5095738768577576, 0.71875] | CTGAN Loss: 2.248732089996338\n",
      "Epoch: 288 | Discriminator Real Loss: [0.2941076159477234, 0.90625] | Discriminator Fake Loss: [0.4743061065673828, 0.6875] | CTGAN Loss: 3.426023006439209\n",
      "Epoch: 289 | Discriminator Real Loss: [0.9583407044410706, 0.53125] | Discriminator Fake Loss: [0.22671379148960114, 0.90625] | CTGAN Loss: 2.44227933883667\n",
      "Epoch: 290 | Discriminator Real Loss: [0.4779979884624481, 0.78125] | Discriminator Fake Loss: [0.4103657007217407, 0.6875] | CTGAN Loss: 1.8982948064804077\n",
      "Epoch: 291 | Discriminator Real Loss: [0.22783027589321136, 0.9375] | Discriminator Fake Loss: [0.5024229288101196, 0.78125] | CTGAN Loss: 2.1797237396240234\n",
      "Epoch: 292 | Discriminator Real Loss: [0.19836834073066711, 1.0] | Discriminator Fake Loss: [0.3115820288658142, 0.84375] | CTGAN Loss: 3.819197177886963\n",
      "Epoch: 293 | Discriminator Real Loss: [0.44990074634552, 0.84375] | Discriminator Fake Loss: [0.16498495638370514, 0.90625] | CTGAN Loss: 3.9578633308410645\n",
      "Epoch: 294 | Discriminator Real Loss: [0.6163378357887268, 0.78125] | Discriminator Fake Loss: [0.16994093358516693, 0.90625] | CTGAN Loss: 2.0952072143554688\n",
      "Epoch: 295 | Discriminator Real Loss: [0.18375472724437714, 0.96875] | Discriminator Fake Loss: [0.4755857586860657, 0.75] | CTGAN Loss: 1.9734928607940674\n",
      "Epoch: 296 | Discriminator Real Loss: [0.13772568106651306, 1.0] | Discriminator Fake Loss: [0.397524893283844, 0.78125] | CTGAN Loss: 2.499199390411377\n",
      "Epoch: 297 | Discriminator Real Loss: [0.20445796847343445, 0.9375] | Discriminator Fake Loss: [0.2626086175441742, 0.84375] | CTGAN Loss: 3.8838109970092773\n",
      "Epoch: 298 | Discriminator Real Loss: [0.27139559388160706, 0.9375] | Discriminator Fake Loss: [0.1569814383983612, 0.9375] | CTGAN Loss: 4.287318229675293\n",
      "Epoch: 299 | Discriminator Real Loss: [0.5430904030799866, 0.75] | Discriminator Fake Loss: [0.24037015438079834, 0.875] | CTGAN Loss: 3.8982174396514893\n",
      "Epoch: 300 | Discriminator Real Loss: [0.3150525987148285, 0.875] | Discriminator Fake Loss: [0.4686868190765381, 0.71875] | CTGAN Loss: 2.8794569969177246\n",
      "Epoch: 301 | Discriminator Real Loss: [0.26769179105758667, 0.9375] | Discriminator Fake Loss: [0.6940003037452698, 0.625] | CTGAN Loss: 4.789595127105713\n",
      "Epoch: 302 | Discriminator Real Loss: [0.41987287998199463, 0.78125] | Discriminator Fake Loss: [0.17911836504936218, 0.90625] | CTGAN Loss: 4.194282531738281\n",
      "Epoch: 303 | Discriminator Real Loss: [0.3841160535812378, 0.84375] | Discriminator Fake Loss: [0.15391995012760162, 0.96875] | CTGAN Loss: 2.789254665374756\n",
      "Epoch: 304 | Discriminator Real Loss: [0.17494070529937744, 0.9375] | Discriminator Fake Loss: [0.4498174786567688, 0.75] | CTGAN Loss: 2.64630126953125\n",
      "Epoch: 305 | Discriminator Real Loss: [0.23546259105205536, 0.9375] | Discriminator Fake Loss: [0.3558657169342041, 0.84375] | CTGAN Loss: 3.8165931701660156\n",
      "Epoch: 306 | Discriminator Real Loss: [0.6687904596328735, 0.6875] | Discriminator Fake Loss: [0.3577883243560791, 0.8125] | CTGAN Loss: 3.144193410873413\n",
      "Epoch: 307 | Discriminator Real Loss: [0.1959020495414734, 0.9375] | Discriminator Fake Loss: [0.8119043707847595, 0.46875] | CTGAN Loss: 3.943521499633789\n",
      "Epoch: 308 | Discriminator Real Loss: [0.5403926372528076, 0.8125] | Discriminator Fake Loss: [0.19319382309913635, 0.90625] | CTGAN Loss: 3.6247215270996094\n",
      "Epoch: 309 | Discriminator Real Loss: [0.571067214012146, 0.75] | Discriminator Fake Loss: [0.6167087554931641, 0.65625] | CTGAN Loss: 3.042325496673584\n",
      "Epoch: 310 | Discriminator Real Loss: [0.4419350028038025, 0.78125] | Discriminator Fake Loss: [0.4617578387260437, 0.71875] | CTGAN Loss: 1.8466920852661133\n",
      "Epoch: 311 | Discriminator Real Loss: [0.33612316846847534, 0.8125] | Discriminator Fake Loss: [0.6209789514541626, 0.6875] | CTGAN Loss: 2.1377856731414795\n",
      "Epoch: 312 | Discriminator Real Loss: [0.5149718523025513, 0.6875] | Discriminator Fake Loss: [0.5668085813522339, 0.6875] | CTGAN Loss: 2.5462985038757324\n",
      "Epoch: 313 | Discriminator Real Loss: [0.44522321224212646, 0.84375] | Discriminator Fake Loss: [0.48750531673431396, 0.71875] | CTGAN Loss: 2.4845356941223145\n",
      "Epoch: 314 | Discriminator Real Loss: [0.41620057821273804, 0.75] | Discriminator Fake Loss: [0.5809112191200256, 0.6875] | CTGAN Loss: 2.262995481491089\n",
      "Epoch: 315 | Discriminator Real Loss: [0.4994705021381378, 0.71875] | Discriminator Fake Loss: [0.38379913568496704, 0.75] | CTGAN Loss: 2.5320301055908203\n",
      "Epoch: 316 | Discriminator Real Loss: [0.35708099603652954, 0.90625] | Discriminator Fake Loss: [0.48887211084365845, 0.75] | CTGAN Loss: 2.2328062057495117\n",
      "Epoch: 317 | Discriminator Real Loss: [0.36302879452705383, 0.8125] | Discriminator Fake Loss: [0.4683162271976471, 0.75] | CTGAN Loss: 2.236121654510498\n",
      "Epoch: 318 | Discriminator Real Loss: [0.4995306730270386, 0.78125] | Discriminator Fake Loss: [0.5738232135772705, 0.75] | CTGAN Loss: 2.215705156326294\n",
      "Epoch: 319 | Discriminator Real Loss: [0.31370261311531067, 0.875] | Discriminator Fake Loss: [0.39664846658706665, 0.8125] | CTGAN Loss: 2.388134002685547\n",
      "Epoch: 320 | Discriminator Real Loss: [0.5362167358398438, 0.75] | Discriminator Fake Loss: [0.5142587423324585, 0.71875] | CTGAN Loss: 2.708678722381592\n",
      "Epoch: 321 | Discriminator Real Loss: [0.3583355247974396, 0.875] | Discriminator Fake Loss: [0.29554086923599243, 0.8125] | CTGAN Loss: 2.2453885078430176\n",
      "Epoch: 322 | Discriminator Real Loss: [0.4804566502571106, 0.78125] | Discriminator Fake Loss: [0.4175315499305725, 0.75] | CTGAN Loss: 2.1059935092926025\n",
      "Epoch: 323 | Discriminator Real Loss: [0.264567494392395, 0.9375] | Discriminator Fake Loss: [0.3327872157096863, 0.875] | CTGAN Loss: 2.5542068481445312\n",
      "Epoch: 324 | Discriminator Real Loss: [0.4902351200580597, 0.8125] | Discriminator Fake Loss: [0.39575517177581787, 0.78125] | CTGAN Loss: 2.3179941177368164\n",
      "Epoch: 325 | Discriminator Real Loss: [0.4290107786655426, 0.8125] | Discriminator Fake Loss: [0.5828081965446472, 0.6875] | CTGAN Loss: 2.3339176177978516\n",
      "Epoch: 326 | Discriminator Real Loss: [0.3840790390968323, 0.875] | Discriminator Fake Loss: [0.22371022403240204, 0.9375] | CTGAN Loss: 4.170166969299316\n",
      "Epoch: 327 | Discriminator Real Loss: [0.5643088817596436, 0.84375] | Discriminator Fake Loss: [0.3465098738670349, 0.8125] | CTGAN Loss: 2.4464101791381836\n",
      "Epoch: 328 | Discriminator Real Loss: [0.23363828659057617, 0.96875] | Discriminator Fake Loss: [0.44629377126693726, 0.78125] | CTGAN Loss: 2.4160494804382324\n",
      "Epoch: 329 | Discriminator Real Loss: [0.4207421839237213, 0.78125] | Discriminator Fake Loss: [0.38577526807785034, 0.75] | CTGAN Loss: 2.806783437728882\n",
      "Epoch: 330 | Discriminator Real Loss: [0.4228895902633667, 0.84375] | Discriminator Fake Loss: [0.32376629114151, 0.84375] | CTGAN Loss: 2.5677566528320312\n",
      "Epoch: 331 | Discriminator Real Loss: [0.27419111132621765, 0.90625] | Discriminator Fake Loss: [0.48842838406562805, 0.75] | CTGAN Loss: 3.239069938659668\n",
      "Epoch: 332 | Discriminator Real Loss: [0.516798734664917, 0.71875] | Discriminator Fake Loss: [0.4193728566169739, 0.75] | CTGAN Loss: 3.41483736038208\n",
      "Epoch: 333 | Discriminator Real Loss: [0.28321677446365356, 0.9375] | Discriminator Fake Loss: [0.27096837759017944, 0.90625] | CTGAN Loss: 2.7822413444519043\n",
      "Epoch: 334 | Discriminator Real Loss: [0.2069588005542755, 1.0] | Discriminator Fake Loss: [0.32626378536224365, 0.84375] | CTGAN Loss: 3.6282434463500977\n",
      "Epoch: 335 | Discriminator Real Loss: [0.3786495327949524, 0.8125] | Discriminator Fake Loss: [0.4229854941368103, 0.78125] | CTGAN Loss: 3.2231454849243164\n",
      "Epoch: 336 | Discriminator Real Loss: [0.4773087501525879, 0.78125] | Discriminator Fake Loss: [0.31992802023887634, 0.84375] | CTGAN Loss: 3.3674991130828857\n",
      "Epoch: 337 | Discriminator Real Loss: [0.2547188401222229, 0.9375] | Discriminator Fake Loss: [0.49411535263061523, 0.71875] | CTGAN Loss: 3.6807384490966797\n",
      "Epoch: 338 | Discriminator Real Loss: [0.35689085721969604, 0.84375] | Discriminator Fake Loss: [0.24205759167671204, 0.90625] | CTGAN Loss: 4.8601250648498535\n",
      "Epoch: 339 | Discriminator Real Loss: [0.5195388793945312, 0.8125] | Discriminator Fake Loss: [0.38564786314964294, 0.78125] | CTGAN Loss: 2.4705018997192383\n",
      "Epoch: 340 | Discriminator Real Loss: [0.1661205291748047, 0.96875] | Discriminator Fake Loss: [0.4948045313358307, 0.65625] | CTGAN Loss: 3.555152416229248\n",
      "Epoch: 341 | Discriminator Real Loss: [0.25133058428764343, 0.90625] | Discriminator Fake Loss: [0.35425782203674316, 0.78125] | CTGAN Loss: 4.958178520202637\n",
      "Epoch: 342 | Discriminator Real Loss: [0.9948505163192749, 0.5625] | Discriminator Fake Loss: [0.37714338302612305, 0.8125] | CTGAN Loss: 2.405179023742676\n",
      "Epoch: 343 | Discriminator Real Loss: [0.1366950273513794, 0.96875] | Discriminator Fake Loss: [1.02518892288208, 0.53125] | CTGAN Loss: 3.2689571380615234\n",
      "Epoch: 344 | Discriminator Real Loss: [0.30760306119918823, 0.90625] | Discriminator Fake Loss: [0.36488088965415955, 0.8125] | CTGAN Loss: 4.894349575042725\n",
      "Epoch: 345 | Discriminator Real Loss: [0.7113542556762695, 0.5625] | Discriminator Fake Loss: [0.3008664846420288, 0.84375] | CTGAN Loss: 3.195934295654297\n",
      "Epoch: 346 | Discriminator Real Loss: [0.35282808542251587, 0.875] | Discriminator Fake Loss: [0.4305858016014099, 0.75] | CTGAN Loss: 2.192657470703125\n",
      "Epoch: 347 | Discriminator Real Loss: [0.2097666710615158, 0.96875] | Discriminator Fake Loss: [0.6564745306968689, 0.625] | CTGAN Loss: 2.558117389678955\n",
      "Epoch: 348 | Discriminator Real Loss: [0.37449726462364197, 0.8125] | Discriminator Fake Loss: [0.35774970054626465, 0.875] | CTGAN Loss: 4.017290115356445\n",
      "Epoch: 349 | Discriminator Real Loss: [0.7536358833312988, 0.5] | Discriminator Fake Loss: [0.2591957151889801, 0.90625] | CTGAN Loss: 2.424131393432617\n",
      "Epoch: 350 | Discriminator Real Loss: [0.35407108068466187, 0.875] | Discriminator Fake Loss: [0.5639015436172485, 0.65625] | CTGAN Loss: 2.3240315914154053\n",
      "Epoch: 351 | Discriminator Real Loss: [0.17797720432281494, 0.96875] | Discriminator Fake Loss: [0.8851253390312195, 0.4375] | CTGAN Loss: 2.6533236503601074\n",
      "Epoch: 352 | Discriminator Real Loss: [0.3922007977962494, 0.8125] | Discriminator Fake Loss: [0.2418615221977234, 0.9375] | CTGAN Loss: 3.67598032951355\n",
      "Epoch: 353 | Discriminator Real Loss: [0.6804015636444092, 0.5625] | Discriminator Fake Loss: [0.40400609374046326, 0.78125] | CTGAN Loss: 2.9178149700164795\n",
      "Epoch: 354 | Discriminator Real Loss: [0.5953148603439331, 0.78125] | Discriminator Fake Loss: [0.4012497067451477, 0.78125] | CTGAN Loss: 2.3890790939331055\n",
      "Epoch: 355 | Discriminator Real Loss: [0.3399428427219391, 0.90625] | Discriminator Fake Loss: [0.6708183884620667, 0.5625] | CTGAN Loss: 1.7923508882522583\n",
      "Epoch: 356 | Discriminator Real Loss: [0.32071077823638916, 0.90625] | Discriminator Fake Loss: [0.6296133995056152, 0.625] | CTGAN Loss: 2.435197353363037\n",
      "Epoch: 357 | Discriminator Real Loss: [0.5292555093765259, 0.71875] | Discriminator Fake Loss: [0.39416682720184326, 0.75] | CTGAN Loss: 2.879718542098999\n",
      "Epoch: 358 | Discriminator Real Loss: [0.757568359375, 0.625] | Discriminator Fake Loss: [0.5010361671447754, 0.6875] | CTGAN Loss: 2.1723766326904297\n",
      "Epoch: 359 | Discriminator Real Loss: [0.41324514150619507, 0.84375] | Discriminator Fake Loss: [0.5086004734039307, 0.6875] | CTGAN Loss: 1.8587894439697266\n",
      "Epoch: 360 | Discriminator Real Loss: [0.39633840322494507, 0.84375] | Discriminator Fake Loss: [0.5676051378250122, 0.71875] | CTGAN Loss: 2.397963047027588\n",
      "Epoch: 361 | Discriminator Real Loss: [0.5765640735626221, 0.71875] | Discriminator Fake Loss: [0.5598360896110535, 0.6875] | CTGAN Loss: 2.2954132556915283\n",
      "Epoch: 362 | Discriminator Real Loss: [0.5175638198852539, 0.78125] | Discriminator Fake Loss: [0.5678114891052246, 0.625] | CTGAN Loss: 2.10968017578125\n",
      "Epoch: 363 | Discriminator Real Loss: [0.2837989628314972, 0.90625] | Discriminator Fake Loss: [0.4091077446937561, 0.75] | CTGAN Loss: 2.329195499420166\n",
      "Epoch: 364 | Discriminator Real Loss: [0.6413710117340088, 0.625] | Discriminator Fake Loss: [0.2961556613445282, 0.84375] | CTGAN Loss: 2.1852617263793945\n",
      "Epoch: 365 | Discriminator Real Loss: [0.439651757478714, 0.8125] | Discriminator Fake Loss: [0.4987839460372925, 0.65625] | CTGAN Loss: 1.5270941257476807\n",
      "Epoch: 366 | Discriminator Real Loss: [0.3087804913520813, 0.9375] | Discriminator Fake Loss: [0.4313342273235321, 0.71875] | CTGAN Loss: 1.7472193241119385\n",
      "Epoch: 367 | Discriminator Real Loss: [0.27402937412261963, 0.9375] | Discriminator Fake Loss: [0.37472134828567505, 0.78125] | CTGAN Loss: 2.361492156982422\n",
      "Epoch: 368 | Discriminator Real Loss: [0.40352389216423035, 0.90625] | Discriminator Fake Loss: [0.4114115536212921, 0.84375] | CTGAN Loss: 2.5082931518554688\n",
      "Epoch: 369 | Discriminator Real Loss: [0.5622866153717041, 0.78125] | Discriminator Fake Loss: [0.3788616359233856, 0.78125] | CTGAN Loss: 2.3519287109375\n",
      "Epoch: 370 | Discriminator Real Loss: [0.4866854250431061, 0.78125] | Discriminator Fake Loss: [0.596260130405426, 0.59375] | CTGAN Loss: 2.4443790912628174\n",
      "Epoch: 371 | Discriminator Real Loss: [0.469233900308609, 0.84375] | Discriminator Fake Loss: [0.521491527557373, 0.6875] | CTGAN Loss: 1.9972933530807495\n",
      "Epoch: 372 | Discriminator Real Loss: [0.3614402711391449, 0.875] | Discriminator Fake Loss: [0.5571112036705017, 0.59375] | CTGAN Loss: 3.338749647140503\n",
      "Epoch: 373 | Discriminator Real Loss: [0.7036089897155762, 0.625] | Discriminator Fake Loss: [0.13915278017520905, 0.96875] | CTGAN Loss: 2.482232093811035\n",
      "Epoch: 374 | Discriminator Real Loss: [0.35479599237442017, 0.90625] | Discriminator Fake Loss: [0.48524409532546997, 0.71875] | CTGAN Loss: 1.7795515060424805\n",
      "Epoch: 375 | Discriminator Real Loss: [0.21028350293636322, 0.9375] | Discriminator Fake Loss: [0.36429569125175476, 0.8125] | CTGAN Loss: 3.1994125843048096\n",
      "Epoch: 376 | Discriminator Real Loss: [0.36398327350616455, 0.84375] | Discriminator Fake Loss: [0.2301003783941269, 0.875] | CTGAN Loss: 3.828089475631714\n",
      "Epoch: 377 | Discriminator Real Loss: [0.5919959545135498, 0.75] | Discriminator Fake Loss: [0.2002090960741043, 0.875] | CTGAN Loss: 2.9889473915100098\n",
      "Epoch: 378 | Discriminator Real Loss: [0.29588818550109863, 0.90625] | Discriminator Fake Loss: [0.34491467475891113, 0.84375] | CTGAN Loss: 2.156268358230591\n",
      "Epoch: 379 | Discriminator Real Loss: [0.2655993103981018, 0.9375] | Discriminator Fake Loss: [0.386080801486969, 0.75] | CTGAN Loss: 2.546977996826172\n",
      "Epoch: 380 | Discriminator Real Loss: [0.3720816373825073, 0.84375] | Discriminator Fake Loss: [0.4662534296512604, 0.6875] | CTGAN Loss: 3.89436411857605\n",
      "Epoch: 381 | Discriminator Real Loss: [0.7379294037818909, 0.625] | Discriminator Fake Loss: [0.327075332403183, 0.84375] | CTGAN Loss: 2.8980188369750977\n",
      "Epoch: 382 | Discriminator Real Loss: [0.2777228355407715, 0.875] | Discriminator Fake Loss: [0.3533608615398407, 0.75] | CTGAN Loss: 2.790968418121338\n",
      "Epoch: 383 | Discriminator Real Loss: [0.44095364212989807, 0.8125] | Discriminator Fake Loss: [0.7208604216575623, 0.65625] | CTGAN Loss: 2.5698986053466797\n",
      "Epoch: 384 | Discriminator Real Loss: [0.2285672277212143, 0.9375] | Discriminator Fake Loss: [0.2203368842601776, 0.9375] | CTGAN Loss: 3.9544622898101807\n",
      "Epoch: 385 | Discriminator Real Loss: [0.5004936456680298, 0.75] | Discriminator Fake Loss: [0.38054540753364563, 0.78125] | CTGAN Loss: 1.9168449640274048\n",
      "Epoch: 386 | Discriminator Real Loss: [0.417932391166687, 0.8125] | Discriminator Fake Loss: [0.8928763270378113, 0.5] | CTGAN Loss: 2.9106357097625732\n",
      "Epoch: 387 | Discriminator Real Loss: [0.438312828540802, 0.875] | Discriminator Fake Loss: [0.3014056384563446, 0.875] | CTGAN Loss: 3.774017810821533\n",
      "Epoch: 388 | Discriminator Real Loss: [0.9899840354919434, 0.53125] | Discriminator Fake Loss: [0.6698174476623535, 0.6875] | CTGAN Loss: 1.3155460357666016\n",
      "Epoch: 389 | Discriminator Real Loss: [0.23701804876327515, 0.96875] | Discriminator Fake Loss: [0.9383222460746765, 0.375] | CTGAN Loss: 2.206864356994629\n",
      "Epoch: 390 | Discriminator Real Loss: [0.22798725962638855, 0.9375] | Discriminator Fake Loss: [0.22133634984493256, 0.90625] | CTGAN Loss: 4.2695183753967285\n",
      "Epoch: 391 | Discriminator Real Loss: [1.357887864112854, 0.40625] | Discriminator Fake Loss: [0.4586585462093353, 0.75] | CTGAN Loss: 1.7437591552734375\n",
      "Epoch: 392 | Discriminator Real Loss: [0.5514999628067017, 0.75] | Discriminator Fake Loss: [1.303802728652954, 0.34375] | CTGAN Loss: 1.7751039266586304\n",
      "Epoch: 393 | Discriminator Real Loss: [0.146197110414505, 0.96875] | Discriminator Fake Loss: [0.7288049459457397, 0.5625] | CTGAN Loss: 2.43796968460083\n",
      "Epoch: 394 | Discriminator Real Loss: [0.742365837097168, 0.59375] | Discriminator Fake Loss: [0.2331235408782959, 0.90625] | CTGAN Loss: 2.498262405395508\n",
      "Epoch: 395 | Discriminator Real Loss: [0.9123010635375977, 0.5625] | Discriminator Fake Loss: [0.6087825894355774, 0.59375] | CTGAN Loss: 1.256866693496704\n",
      "Epoch: 396 | Discriminator Real Loss: [0.3841627836227417, 0.875] | Discriminator Fake Loss: [1.0221686363220215, 0.40625] | CTGAN Loss: 0.9311836361885071\n",
      "Epoch: 397 | Discriminator Real Loss: [0.32015496492385864, 0.90625] | Discriminator Fake Loss: [0.6889535188674927, 0.53125] | CTGAN Loss: 1.5875251293182373\n",
      "Epoch: 398 | Discriminator Real Loss: [0.42901700735092163, 0.84375] | Discriminator Fake Loss: [0.3158823847770691, 0.8125] | CTGAN Loss: 2.155407428741455\n",
      "Epoch: 399 | Discriminator Real Loss: [0.7531644105911255, 0.65625] | Discriminator Fake Loss: [0.2876124978065491, 0.90625] | CTGAN Loss: 2.290231704711914\n",
      "Epoch: 400 | Discriminator Real Loss: [0.5952028036117554, 0.84375] | Discriminator Fake Loss: [0.4865330457687378, 0.6875] | CTGAN Loss: 1.782975673675537\n",
      "Epoch: 401 | Discriminator Real Loss: [0.41328927874565125, 0.875] | Discriminator Fake Loss: [0.634562075138092, 0.59375] | CTGAN Loss: 2.5853219032287598\n",
      "Epoch: 402 | Discriminator Real Loss: [0.3954704999923706, 0.96875] | Discriminator Fake Loss: [0.3064897954463959, 0.8125] | CTGAN Loss: 2.1424319744110107\n",
      "Epoch: 403 | Discriminator Real Loss: [0.4589071273803711, 0.78125] | Discriminator Fake Loss: [0.40978506207466125, 0.71875] | CTGAN Loss: 1.86626136302948\n",
      "Epoch: 404 | Discriminator Real Loss: [0.4774990677833557, 0.8125] | Discriminator Fake Loss: [0.22456103563308716, 0.96875] | CTGAN Loss: 2.681908130645752\n",
      "Epoch: 405 | Discriminator Real Loss: [0.49795329570770264, 0.84375] | Discriminator Fake Loss: [0.22527697682380676, 0.90625] | CTGAN Loss: 3.0559301376342773\n",
      "Epoch: 406 | Discriminator Real Loss: [0.49828198552131653, 0.90625] | Discriminator Fake Loss: [0.3198247253894806, 0.84375] | CTGAN Loss: 1.9095182418823242\n",
      "Epoch: 407 | Discriminator Real Loss: [0.34873199462890625, 0.90625] | Discriminator Fake Loss: [0.46777400374412537, 0.65625] | CTGAN Loss: 1.6098963022232056\n",
      "Epoch: 408 | Discriminator Real Loss: [0.37221455574035645, 0.9375] | Discriminator Fake Loss: [0.4947095811367035, 0.71875] | CTGAN Loss: 2.568591594696045\n",
      "Epoch: 409 | Discriminator Real Loss: [0.3438861668109894, 0.9375] | Discriminator Fake Loss: [0.5207168459892273, 0.6875] | CTGAN Loss: 3.4701590538024902\n",
      "Epoch: 410 | Discriminator Real Loss: [0.7355499267578125, 0.65625] | Discriminator Fake Loss: [0.43914172053337097, 0.78125] | CTGAN Loss: 2.635913848876953\n",
      "Epoch: 411 | Discriminator Real Loss: [0.6644725799560547, 0.625] | Discriminator Fake Loss: [0.5743246078491211, 0.65625] | CTGAN Loss: 1.6699680089950562\n",
      "Epoch: 412 | Discriminator Real Loss: [0.34479841589927673, 0.90625] | Discriminator Fake Loss: [0.9195107221603394, 0.4375] | CTGAN Loss: 1.243215799331665\n",
      "Epoch: 413 | Discriminator Real Loss: [0.3157091736793518, 0.9375] | Discriminator Fake Loss: [0.4779975116252899, 0.6875] | CTGAN Loss: 1.8891587257385254\n",
      "Epoch: 414 | Discriminator Real Loss: [0.4253857433795929, 0.84375] | Discriminator Fake Loss: [0.5299053192138672, 0.625] | CTGAN Loss: 3.0153164863586426\n",
      "Epoch: 415 | Discriminator Real Loss: [0.8727240562438965, 0.53125] | Discriminator Fake Loss: [0.26147180795669556, 0.90625] | CTGAN Loss: 2.34529972076416\n",
      "Epoch: 416 | Discriminator Real Loss: [0.47388580441474915, 0.8125] | Discriminator Fake Loss: [0.6390691995620728, 0.53125] | CTGAN Loss: 1.1906776428222656\n",
      "Epoch: 417 | Discriminator Real Loss: [0.2755882441997528, 0.9375] | Discriminator Fake Loss: [0.9993847012519836, 0.28125] | CTGAN Loss: 1.9566795825958252\n",
      "Epoch: 418 | Discriminator Real Loss: [0.3714345693588257, 0.90625] | Discriminator Fake Loss: [0.38861286640167236, 0.8125] | CTGAN Loss: 2.802258253097534\n",
      "Epoch: 419 | Discriminator Real Loss: [0.7548398971557617, 0.46875] | Discriminator Fake Loss: [0.3573186993598938, 0.84375] | CTGAN Loss: 1.9637651443481445\n",
      "Epoch: 420 | Discriminator Real Loss: [0.6232461929321289, 0.6875] | Discriminator Fake Loss: [0.6402410268783569, 0.59375] | CTGAN Loss: 1.0263676643371582\n",
      "Epoch: 421 | Discriminator Real Loss: [0.3325355648994446, 0.9375] | Discriminator Fake Loss: [0.6362122893333435, 0.625] | CTGAN Loss: 1.8664805889129639\n",
      "Epoch: 422 | Discriminator Real Loss: [0.46920767426490784, 0.78125] | Discriminator Fake Loss: [0.5936487913131714, 0.65625] | CTGAN Loss: 1.4645402431488037\n",
      "Epoch: 423 | Discriminator Real Loss: [0.7978531718254089, 0.625] | Discriminator Fake Loss: [0.5409828424453735, 0.65625] | CTGAN Loss: 1.6649386882781982\n",
      "Epoch: 424 | Discriminator Real Loss: [0.6212724447250366, 0.625] | Discriminator Fake Loss: [0.708442211151123, 0.4375] | CTGAN Loss: 0.9487608075141907\n",
      "Epoch: 425 | Discriminator Real Loss: [0.42422324419021606, 0.875] | Discriminator Fake Loss: [0.8537347316741943, 0.375] | CTGAN Loss: 1.1955761909484863\n",
      "Epoch: 426 | Discriminator Real Loss: [0.4683486223220825, 0.84375] | Discriminator Fake Loss: [0.5912351608276367, 0.65625] | CTGAN Loss: 1.6859569549560547\n",
      "Epoch: 427 | Discriminator Real Loss: [0.6020195484161377, 0.78125] | Discriminator Fake Loss: [0.6123842597007751, 0.5625] | CTGAN Loss: 1.792377233505249\n",
      "Epoch: 428 | Discriminator Real Loss: [0.6401695013046265, 0.6875] | Discriminator Fake Loss: [0.3281819820404053, 0.84375] | CTGAN Loss: 1.9290128946304321\n",
      "Epoch: 429 | Discriminator Real Loss: [0.7359246015548706, 0.625] | Discriminator Fake Loss: [0.5103310346603394, 0.6875] | CTGAN Loss: 1.5282297134399414\n",
      "Epoch: 430 | Discriminator Real Loss: [0.4060627818107605, 0.875] | Discriminator Fake Loss: [0.7150101661682129, 0.5] | CTGAN Loss: 1.1427803039550781\n",
      "Epoch: 431 | Discriminator Real Loss: [0.4929160475730896, 0.78125] | Discriminator Fake Loss: [0.7369785308837891, 0.53125] | CTGAN Loss: 1.284865379333496\n",
      "Epoch: 432 | Discriminator Real Loss: [0.37980160117149353, 0.84375] | Discriminator Fake Loss: [0.42783793807029724, 0.8125] | CTGAN Loss: 2.0709481239318848\n",
      "Epoch: 433 | Discriminator Real Loss: [0.5866602063179016, 0.6875] | Discriminator Fake Loss: [0.27474600076675415, 0.84375] | CTGAN Loss: 2.309415817260742\n",
      "Epoch: 434 | Discriminator Real Loss: [0.6150594353675842, 0.6875] | Discriminator Fake Loss: [0.33292365074157715, 0.875] | CTGAN Loss: 1.7507514953613281\n",
      "Epoch: 435 | Discriminator Real Loss: [0.6124392747879028, 0.65625] | Discriminator Fake Loss: [0.6468394994735718, 0.5] | CTGAN Loss: 1.398261308670044\n",
      "Epoch: 436 | Discriminator Real Loss: [0.4131394624710083, 0.875] | Discriminator Fake Loss: [0.474764883518219, 0.65625] | CTGAN Loss: 1.9690353870391846\n",
      "Epoch: 437 | Discriminator Real Loss: [0.41838037967681885, 0.875] | Discriminator Fake Loss: [0.3361794948577881, 0.8125] | CTGAN Loss: 2.1665916442871094\n",
      "Epoch: 438 | Discriminator Real Loss: [0.5953084826469421, 0.71875] | Discriminator Fake Loss: [0.3436700105667114, 0.8125] | CTGAN Loss: 2.664618492126465\n",
      "Epoch: 439 | Discriminator Real Loss: [0.6974173784255981, 0.5625] | Discriminator Fake Loss: [0.3194626569747925, 0.84375] | CTGAN Loss: 1.9561161994934082\n",
      "Epoch: 440 | Discriminator Real Loss: [0.30095145106315613, 0.9375] | Discriminator Fake Loss: [0.5624970197677612, 0.59375] | CTGAN Loss: 2.1322240829467773\n",
      "Epoch: 441 | Discriminator Real Loss: [0.34295880794525146, 0.9375] | Discriminator Fake Loss: [0.3591952323913574, 0.8125] | CTGAN Loss: 2.0877482891082764\n",
      "Epoch: 442 | Discriminator Real Loss: [0.41182130575180054, 0.875] | Discriminator Fake Loss: [0.4851354956626892, 0.71875] | CTGAN Loss: 2.6804938316345215\n",
      "Epoch: 443 | Discriminator Real Loss: [0.6266260743141174, 0.59375] | Discriminator Fake Loss: [0.432986319065094, 0.75] | CTGAN Loss: 2.2237586975097656\n",
      "Epoch: 444 | Discriminator Real Loss: [0.4927223324775696, 0.78125] | Discriminator Fake Loss: [0.466835081577301, 0.78125] | CTGAN Loss: 2.3507113456726074\n",
      "Epoch: 445 | Discriminator Real Loss: [0.4166051745414734, 0.78125] | Discriminator Fake Loss: [0.38053005933761597, 0.8125] | CTGAN Loss: 2.0103070735931396\n",
      "Epoch: 446 | Discriminator Real Loss: [0.47397691011428833, 0.78125] | Discriminator Fake Loss: [0.4150073528289795, 0.8125] | CTGAN Loss: 2.124650478363037\n",
      "Epoch: 447 | Discriminator Real Loss: [0.33460572361946106, 0.84375] | Discriminator Fake Loss: [0.4178149700164795, 0.78125] | CTGAN Loss: 2.31253719329834\n",
      "Epoch: 448 | Discriminator Real Loss: [0.523901641368866, 0.75] | Discriminator Fake Loss: [0.5868465900421143, 0.6875] | CTGAN Loss: 2.1950531005859375\n",
      "Epoch: 449 | Discriminator Real Loss: [0.5691136121749878, 0.75] | Discriminator Fake Loss: [0.6030374765396118, 0.75] | CTGAN Loss: 2.3776423931121826\n",
      "Epoch: 450 | Discriminator Real Loss: [0.6833100318908691, 0.625] | Discriminator Fake Loss: [0.48100337386131287, 0.65625] | CTGAN Loss: 1.9358588457107544\n",
      "Epoch: 451 | Discriminator Real Loss: [0.6379914283752441, 0.6875] | Discriminator Fake Loss: [0.7161970138549805, 0.59375] | CTGAN Loss: 1.4848706722259521\n",
      "Epoch: 452 | Discriminator Real Loss: [0.3642217218875885, 0.90625] | Discriminator Fake Loss: [0.9067217707633972, 0.40625] | CTGAN Loss: 2.799339771270752\n",
      "Epoch: 453 | Discriminator Real Loss: [0.6798853874206543, 0.6875] | Discriminator Fake Loss: [0.2296161949634552, 0.9375] | CTGAN Loss: 1.8700000047683716\n",
      "Epoch: 454 | Discriminator Real Loss: [0.5728701949119568, 0.78125] | Discriminator Fake Loss: [0.6072942614555359, 0.625] | CTGAN Loss: 1.3908568620681763\n",
      "Epoch: 455 | Discriminator Real Loss: [0.4771360754966736, 0.8125] | Discriminator Fake Loss: [0.7302951812744141, 0.53125] | CTGAN Loss: 1.3774998188018799\n",
      "Epoch: 456 | Discriminator Real Loss: [0.45970839262008667, 0.875] | Discriminator Fake Loss: [0.6364864110946655, 0.65625] | CTGAN Loss: 1.1018807888031006\n",
      "Epoch: 457 | Discriminator Real Loss: [0.6648036241531372, 0.65625] | Discriminator Fake Loss: [0.5638725757598877, 0.6875] | CTGAN Loss: 1.5170001983642578\n",
      "Epoch: 458 | Discriminator Real Loss: [0.6070874929428101, 0.78125] | Discriminator Fake Loss: [0.7016817927360535, 0.5625] | CTGAN Loss: 1.173928141593933\n",
      "Epoch: 459 | Discriminator Real Loss: [0.6299479007720947, 0.71875] | Discriminator Fake Loss: [0.6764398813247681, 0.53125] | CTGAN Loss: 1.15349543094635\n",
      "Epoch: 460 | Discriminator Real Loss: [0.5224990844726562, 0.75] | Discriminator Fake Loss: [0.5680809020996094, 0.59375] | CTGAN Loss: 2.00211763381958\n",
      "Epoch: 461 | Discriminator Real Loss: [0.7347627878189087, 0.5] | Discriminator Fake Loss: [0.4220704436302185, 0.8125] | CTGAN Loss: 1.632124423980713\n",
      "Epoch: 462 | Discriminator Real Loss: [0.3883279860019684, 0.90625] | Discriminator Fake Loss: [0.3901500701904297, 0.78125] | CTGAN Loss: 1.5329906940460205\n",
      "Epoch: 463 | Discriminator Real Loss: [0.5120149850845337, 0.78125] | Discriminator Fake Loss: [0.5044195055961609, 0.6875] | CTGAN Loss: 1.7163050174713135\n",
      "Epoch: 464 | Discriminator Real Loss: [0.3849857449531555, 0.90625] | Discriminator Fake Loss: [0.5904664397239685, 0.65625] | CTGAN Loss: 1.932508945465088\n",
      "Epoch: 465 | Discriminator Real Loss: [0.47293344140052795, 0.75] | Discriminator Fake Loss: [0.3698492646217346, 0.84375] | CTGAN Loss: 2.0891990661621094\n",
      "Epoch: 466 | Discriminator Real Loss: [0.630311906337738, 0.6875] | Discriminator Fake Loss: [0.6782572269439697, 0.59375] | CTGAN Loss: 2.13813853263855\n",
      "Epoch: 467 | Discriminator Real Loss: [0.5822063684463501, 0.71875] | Discriminator Fake Loss: [0.3547113239765167, 0.875] | CTGAN Loss: 2.3457565307617188\n",
      "Epoch: 468 | Discriminator Real Loss: [0.5030505657196045, 0.8125] | Discriminator Fake Loss: [0.4875314235687256, 0.75] | CTGAN Loss: 2.1880135536193848\n",
      "Epoch: 469 | Discriminator Real Loss: [0.4105018079280853, 0.84375] | Discriminator Fake Loss: [0.3098066449165344, 0.90625] | CTGAN Loss: 2.2612037658691406\n",
      "Epoch: 470 | Discriminator Real Loss: [0.5494178533554077, 0.78125] | Discriminator Fake Loss: [0.42896729707717896, 0.78125] | CTGAN Loss: 2.255070209503174\n",
      "Epoch: 471 | Discriminator Real Loss: [0.4221350848674774, 0.875] | Discriminator Fake Loss: [0.5451092720031738, 0.65625] | CTGAN Loss: 2.5719680786132812\n",
      "Epoch: 472 | Discriminator Real Loss: [0.4851765036582947, 0.84375] | Discriminator Fake Loss: [0.5653764009475708, 0.59375] | CTGAN Loss: 2.134209394454956\n",
      "Epoch: 473 | Discriminator Real Loss: [0.3699522614479065, 0.9375] | Discriminator Fake Loss: [0.34875065088272095, 0.78125] | CTGAN Loss: 2.5140819549560547\n",
      "Epoch: 474 | Discriminator Real Loss: [0.5764828324317932, 0.59375] | Discriminator Fake Loss: [0.4150492250919342, 0.75] | CTGAN Loss: 1.8656070232391357\n",
      "Epoch: 475 | Discriminator Real Loss: [0.4949282705783844, 0.84375] | Discriminator Fake Loss: [0.6278191208839417, 0.5625] | CTGAN Loss: 1.2150977849960327\n",
      "Epoch: 476 | Discriminator Real Loss: [0.32826855778694153, 0.9375] | Discriminator Fake Loss: [0.7406551837921143, 0.53125] | CTGAN Loss: 2.9361181259155273\n",
      "Epoch: 477 | Discriminator Real Loss: [0.9788978695869446, 0.4375] | Discriminator Fake Loss: [0.6496907472610474, 0.59375] | CTGAN Loss: 2.2046968936920166\n",
      "Epoch: 478 | Discriminator Real Loss: [0.7134219408035278, 0.71875] | Discriminator Fake Loss: [0.8240506649017334, 0.53125] | CTGAN Loss: 2.3097586631774902\n",
      "Epoch: 479 | Discriminator Real Loss: [0.7426238059997559, 0.6875] | Discriminator Fake Loss: [0.704954206943512, 0.5] | CTGAN Loss: 1.4142522811889648\n",
      "Epoch: 480 | Discriminator Real Loss: [0.4260522723197937, 0.8125] | Discriminator Fake Loss: [0.7312020659446716, 0.5] | CTGAN Loss: 1.733182668685913\n",
      "Epoch: 481 | Discriminator Real Loss: [0.3980081379413605, 0.875] | Discriminator Fake Loss: [0.3813820779323578, 0.75] | CTGAN Loss: 3.2027034759521484\n",
      "Epoch: 482 | Discriminator Real Loss: [0.9074563980102539, 0.4375] | Discriminator Fake Loss: [0.21921922266483307, 0.9375] | CTGAN Loss: 1.7111551761627197\n",
      "Epoch: 483 | Discriminator Real Loss: [0.5032528638839722, 0.8125] | Discriminator Fake Loss: [0.7398759126663208, 0.53125] | CTGAN Loss: 1.185555338859558\n",
      "Epoch: 484 | Discriminator Real Loss: [0.23121805489063263, 0.96875] | Discriminator Fake Loss: [0.679781436920166, 0.5] | CTGAN Loss: 1.7526206970214844\n",
      "Epoch: 485 | Discriminator Real Loss: [0.26596155762672424, 1.0] | Discriminator Fake Loss: [0.3594279885292053, 0.8125] | CTGAN Loss: 2.6390798091888428\n",
      "Epoch: 486 | Discriminator Real Loss: [1.1712838411331177, 0.3125] | Discriminator Fake Loss: [0.34062549471855164, 0.90625] | CTGAN Loss: 1.6678996086120605\n",
      "Epoch: 487 | Discriminator Real Loss: [0.5524413585662842, 0.90625] | Discriminator Fake Loss: [0.5947059392929077, 0.6875] | CTGAN Loss: 1.684072732925415\n",
      "Epoch: 488 | Discriminator Real Loss: [0.29543304443359375, 0.9375] | Discriminator Fake Loss: [0.8256978988647461, 0.46875] | CTGAN Loss: 1.5543065071105957\n",
      "Epoch: 489 | Discriminator Real Loss: [0.42568325996398926, 0.875] | Discriminator Fake Loss: [0.4772926867008209, 0.71875] | CTGAN Loss: 2.7943763732910156\n",
      "Epoch: 490 | Discriminator Real Loss: [0.8696263432502747, 0.53125] | Discriminator Fake Loss: [0.22348174452781677, 0.96875] | CTGAN Loss: 2.5582377910614014\n",
      "Epoch: 491 | Discriminator Real Loss: [0.5560333728790283, 0.6875] | Discriminator Fake Loss: [0.3669053316116333, 0.8125] | CTGAN Loss: 1.8896552324295044\n",
      "Epoch: 492 | Discriminator Real Loss: [0.4396003186702728, 0.84375] | Discriminator Fake Loss: [0.46087828278541565, 0.78125] | CTGAN Loss: 1.8856849670410156\n",
      "Epoch: 493 | Discriminator Real Loss: [0.33730241656303406, 0.90625] | Discriminator Fake Loss: [0.47240790724754333, 0.71875] | CTGAN Loss: 2.0747218132019043\n",
      "Epoch: 494 | Discriminator Real Loss: [0.435569167137146, 0.8125] | Discriminator Fake Loss: [0.642572283744812, 0.5] | CTGAN Loss: 2.250286817550659\n",
      "Epoch: 495 | Discriminator Real Loss: [0.3991784155368805, 0.84375] | Discriminator Fake Loss: [0.34674614667892456, 0.84375] | CTGAN Loss: 3.0378737449645996\n",
      "Epoch: 496 | Discriminator Real Loss: [0.5950799584388733, 0.75] | Discriminator Fake Loss: [0.2527318596839905, 0.84375] | CTGAN Loss: 2.688873529434204\n",
      "Epoch: 497 | Discriminator Real Loss: [0.601586639881134, 0.6875] | Discriminator Fake Loss: [0.5296732783317566, 0.71875] | CTGAN Loss: 2.3050174713134766\n",
      "Epoch: 498 | Discriminator Real Loss: [0.2699024975299835, 0.9375] | Discriminator Fake Loss: [0.5114063024520874, 0.78125] | CTGAN Loss: 2.312270164489746\n",
      "Epoch: 499 | Discriminator Real Loss: [0.31945520639419556, 0.90625] | Discriminator Fake Loss: [0.36819887161254883, 0.78125] | CTGAN Loss: 2.1281473636627197\n",
      "Epoch: 500 | Discriminator Real Loss: [0.4263153672218323, 0.78125] | Discriminator Fake Loss: [0.46943551301956177, 0.8125] | CTGAN Loss: 2.5553979873657227\n",
      "Epoch: 501 | Discriminator Real Loss: [0.42651644349098206, 0.78125] | Discriminator Fake Loss: [0.5472080707550049, 0.75] | CTGAN Loss: 2.156604051589966\n",
      "Epoch: 502 | Discriminator Real Loss: [0.5219560861587524, 0.75] | Discriminator Fake Loss: [0.5138952732086182, 0.71875] | CTGAN Loss: 2.7567737102508545\n",
      "Epoch: 503 | Discriminator Real Loss: [0.4678751230239868, 0.84375] | Discriminator Fake Loss: [0.515496015548706, 0.65625] | CTGAN Loss: 2.381685972213745\n",
      "Epoch: 504 | Discriminator Real Loss: [0.5574783086776733, 0.78125] | Discriminator Fake Loss: [0.5062183141708374, 0.75] | CTGAN Loss: 2.677558660507202\n",
      "Epoch: 505 | Discriminator Real Loss: [0.5201061964035034, 0.71875] | Discriminator Fake Loss: [0.5410417914390564, 0.6875] | CTGAN Loss: 2.1331143379211426\n",
      "Epoch: 506 | Discriminator Real Loss: [0.6410856246948242, 0.625] | Discriminator Fake Loss: [0.509161114692688, 0.71875] | CTGAN Loss: 1.4860432147979736\n",
      "Epoch: 507 | Discriminator Real Loss: [0.3516000509262085, 0.90625] | Discriminator Fake Loss: [0.8928879499435425, 0.46875] | CTGAN Loss: 1.562535047531128\n",
      "Epoch: 508 | Discriminator Real Loss: [0.45151591300964355, 0.875] | Discriminator Fake Loss: [0.7201329469680786, 0.59375] | CTGAN Loss: 3.1229095458984375\n",
      "Epoch: 509 | Discriminator Real Loss: [1.1820716857910156, 0.4375] | Discriminator Fake Loss: [0.5953015089035034, 0.625] | CTGAN Loss: 1.417309045791626\n",
      "Epoch: 510 | Discriminator Real Loss: [0.5847898721694946, 0.65625] | Discriminator Fake Loss: [0.8537677526473999, 0.40625] | CTGAN Loss: 1.247431755065918\n",
      "Epoch: 511 | Discriminator Real Loss: [0.31258684396743774, 0.96875] | Discriminator Fake Loss: [0.8909968137741089, 0.40625] | CTGAN Loss: 1.8450260162353516\n",
      "Epoch: 512 | Discriminator Real Loss: [0.62364661693573, 0.71875] | Discriminator Fake Loss: [0.31311100721359253, 0.875] | CTGAN Loss: 2.4503989219665527\n",
      "Epoch: 513 | Discriminator Real Loss: [0.9299457669258118, 0.5] | Discriminator Fake Loss: [0.46653005480766296, 0.75] | CTGAN Loss: 0.9213088750839233\n",
      "Epoch: 514 | Discriminator Real Loss: [0.4719690680503845, 0.84375] | Discriminator Fake Loss: [0.8311405181884766, 0.46875] | CTGAN Loss: 1.2285606861114502\n",
      "Epoch: 515 | Discriminator Real Loss: [0.38103002309799194, 0.875] | Discriminator Fake Loss: [0.5797435641288757, 0.71875] | CTGAN Loss: 1.361742615699768\n",
      "Epoch: 516 | Discriminator Real Loss: [0.4733920097351074, 0.8125] | Discriminator Fake Loss: [0.5722022652626038, 0.65625] | CTGAN Loss: 1.8876906633377075\n",
      "Epoch: 517 | Discriminator Real Loss: [0.6548379063606262, 0.625] | Discriminator Fake Loss: [0.3991275727748871, 0.875] | CTGAN Loss: 2.304683208465576\n",
      "Epoch: 518 | Discriminator Real Loss: [0.9156631827354431, 0.40625] | Discriminator Fake Loss: [0.2586643397808075, 0.9375] | CTGAN Loss: 1.5611048936843872\n",
      "Epoch: 519 | Discriminator Real Loss: [0.47962746024131775, 0.84375] | Discriminator Fake Loss: [0.5962777137756348, 0.59375] | CTGAN Loss: 1.2909960746765137\n",
      "Epoch: 520 | Discriminator Real Loss: [0.26938384771347046, 0.96875] | Discriminator Fake Loss: [0.8042986392974854, 0.46875] | CTGAN Loss: 1.5700359344482422\n",
      "Epoch: 521 | Discriminator Real Loss: [0.37174421548843384, 0.90625] | Discriminator Fake Loss: [0.41374683380126953, 0.78125] | CTGAN Loss: 3.002615213394165\n",
      "Epoch: 522 | Discriminator Real Loss: [0.6759226322174072, 0.625] | Discriminator Fake Loss: [0.26066720485687256, 0.875] | CTGAN Loss: 2.5763537883758545\n",
      "Epoch: 523 | Discriminator Real Loss: [0.6916890144348145, 0.625] | Discriminator Fake Loss: [0.4122314453125, 0.84375] | CTGAN Loss: 1.7581957578659058\n",
      "Epoch: 524 | Discriminator Real Loss: [0.33609676361083984, 0.9375] | Discriminator Fake Loss: [0.6188396215438843, 0.65625] | CTGAN Loss: 1.6405794620513916\n",
      "Epoch: 525 | Discriminator Real Loss: [0.3194234371185303, 0.90625] | Discriminator Fake Loss: [0.337673544883728, 0.84375] | CTGAN Loss: 2.8013365268707275\n",
      "Epoch: 526 | Discriminator Real Loss: [0.5029453039169312, 0.78125] | Discriminator Fake Loss: [0.2002803087234497, 0.90625] | CTGAN Loss: 2.869093418121338\n",
      "Epoch: 527 | Discriminator Real Loss: [0.5125375986099243, 0.75] | Discriminator Fake Loss: [0.48263663053512573, 0.75] | CTGAN Loss: 2.5204567909240723\n",
      "Epoch: 528 | Discriminator Real Loss: [0.4164443612098694, 0.78125] | Discriminator Fake Loss: [0.5425799489021301, 0.65625] | CTGAN Loss: 2.238619327545166\n",
      "Epoch: 529 | Discriminator Real Loss: [0.34807243943214417, 0.9375] | Discriminator Fake Loss: [0.37311866879463196, 0.78125] | CTGAN Loss: 2.868114471435547\n",
      "Epoch: 530 | Discriminator Real Loss: [0.39205455780029297, 0.9375] | Discriminator Fake Loss: [0.23350286483764648, 0.90625] | CTGAN Loss: 3.145949602127075\n",
      "Epoch: 531 | Discriminator Real Loss: [0.5470719337463379, 0.78125] | Discriminator Fake Loss: [0.4849063456058502, 0.6875] | CTGAN Loss: 2.2442378997802734\n",
      "Epoch: 532 | Discriminator Real Loss: [0.3343060314655304, 0.875] | Discriminator Fake Loss: [0.46470561623573303, 0.71875] | CTGAN Loss: 2.1749494075775146\n",
      "Epoch: 533 | Discriminator Real Loss: [0.4852290153503418, 0.78125] | Discriminator Fake Loss: [0.3973264694213867, 0.78125] | CTGAN Loss: 2.3234000205993652\n",
      "Epoch: 534 | Discriminator Real Loss: [0.4086727201938629, 0.84375] | Discriminator Fake Loss: [0.5122389197349548, 0.71875] | CTGAN Loss: 1.939446210861206\n",
      "Epoch: 535 | Discriminator Real Loss: [0.4633287489414215, 0.78125] | Discriminator Fake Loss: [0.6078004837036133, 0.65625] | CTGAN Loss: 2.5320258140563965\n",
      "Epoch: 536 | Discriminator Real Loss: [0.3630082607269287, 0.90625] | Discriminator Fake Loss: [0.8756656646728516, 0.40625] | CTGAN Loss: 2.877702236175537\n",
      "Epoch: 537 | Discriminator Real Loss: [1.1900173425674438, 0.3125] | Discriminator Fake Loss: [0.5000503063201904, 0.71875] | CTGAN Loss: 1.8401362895965576\n",
      "Epoch: 538 | Discriminator Real Loss: [0.3296250104904175, 0.84375] | Discriminator Fake Loss: [0.7276332974433899, 0.65625] | CTGAN Loss: 1.4824130535125732\n",
      "Epoch: 539 | Discriminator Real Loss: [0.5328032970428467, 0.75] | Discriminator Fake Loss: [0.5567256212234497, 0.6875] | CTGAN Loss: 1.6407666206359863\n",
      "Epoch: 540 | Discriminator Real Loss: [0.4860262870788574, 0.75] | Discriminator Fake Loss: [0.4683760404586792, 0.75] | CTGAN Loss: 2.593502998352051\n",
      "Epoch: 541 | Discriminator Real Loss: [0.5022984743118286, 0.71875] | Discriminator Fake Loss: [0.5955502390861511, 0.625] | CTGAN Loss: 2.5051753520965576\n",
      "Epoch: 542 | Discriminator Real Loss: [0.5549858212471008, 0.71875] | Discriminator Fake Loss: [0.3900016248226166, 0.875] | CTGAN Loss: 2.398031234741211\n",
      "Epoch: 543 | Discriminator Real Loss: [0.47696709632873535, 0.75] | Discriminator Fake Loss: [0.2586478590965271, 0.9375] | CTGAN Loss: 2.6872470378875732\n",
      "Epoch: 544 | Discriminator Real Loss: [0.4089033603668213, 0.90625] | Discriminator Fake Loss: [0.3442557454109192, 0.875] | CTGAN Loss: 2.8711929321289062\n",
      "Epoch: 545 | Discriminator Real Loss: [0.5396454930305481, 0.84375] | Discriminator Fake Loss: [0.2606830298900604, 0.875] | CTGAN Loss: 2.6816892623901367\n",
      "Epoch: 546 | Discriminator Real Loss: [0.3089058995246887, 0.90625] | Discriminator Fake Loss: [0.544007420539856, 0.65625] | CTGAN Loss: 2.6055147647857666\n",
      "Epoch: 547 | Discriminator Real Loss: [0.24673014879226685, 0.96875] | Discriminator Fake Loss: [0.24540480971336365, 0.875] | CTGAN Loss: 3.307408332824707\n",
      "Epoch: 548 | Discriminator Real Loss: [0.26986804604530334, 0.96875] | Discriminator Fake Loss: [0.31236663460731506, 0.875] | CTGAN Loss: 3.560753345489502\n",
      "Epoch: 549 | Discriminator Real Loss: [0.3925265073776245, 0.875] | Discriminator Fake Loss: [0.16088075935840607, 0.96875] | CTGAN Loss: 3.566526412963867\n",
      "Epoch: 550 | Discriminator Real Loss: [0.45221078395843506, 0.84375] | Discriminator Fake Loss: [0.2798086702823639, 0.90625] | CTGAN Loss: 1.8048839569091797\n",
      "Epoch: 551 | Discriminator Real Loss: [0.24749456346035004, 0.9375] | Discriminator Fake Loss: [0.7412027716636658, 0.53125] | CTGAN Loss: 2.2355313301086426\n",
      "Epoch: 552 | Discriminator Real Loss: [0.28422975540161133, 0.9375] | Discriminator Fake Loss: [0.309793084859848, 0.84375] | CTGAN Loss: 3.857645273208618\n",
      "Epoch: 553 | Discriminator Real Loss: [0.8222988843917847, 0.53125] | Discriminator Fake Loss: [0.2172403335571289, 0.9375] | CTGAN Loss: 2.2221055030822754\n",
      "Epoch: 554 | Discriminator Real Loss: [0.3651527762413025, 0.875] | Discriminator Fake Loss: [0.621220588684082, 0.625] | CTGAN Loss: 1.936244010925293\n",
      "Epoch: 555 | Discriminator Real Loss: [0.2396860420703888, 0.9375] | Discriminator Fake Loss: [0.8635735511779785, 0.4375] | CTGAN Loss: 2.7201809883117676\n",
      "Epoch: 556 | Discriminator Real Loss: [0.5551982522010803, 0.71875] | Discriminator Fake Loss: [0.31674325466156006, 0.90625] | CTGAN Loss: 3.9493298530578613\n",
      "Epoch: 557 | Discriminator Real Loss: [0.9160342216491699, 0.375] | Discriminator Fake Loss: [0.6944360733032227, 0.5625] | CTGAN Loss: 1.440690517425537\n",
      "Epoch: 558 | Discriminator Real Loss: [0.324160635471344, 0.84375] | Discriminator Fake Loss: [0.766785740852356, 0.625] | CTGAN Loss: 2.1819334030151367\n",
      "Epoch: 559 | Discriminator Real Loss: [0.3675960302352905, 0.84375] | Discriminator Fake Loss: [0.5977369546890259, 0.71875] | CTGAN Loss: 3.2192938327789307\n",
      "Epoch: 560 | Discriminator Real Loss: [0.9881911277770996, 0.5625] | Discriminator Fake Loss: [0.5080204010009766, 0.625] | CTGAN Loss: 2.1190385818481445\n",
      "Epoch: 561 | Discriminator Real Loss: [0.45060646533966064, 0.875] | Discriminator Fake Loss: [0.7594484090805054, 0.5625] | CTGAN Loss: 1.8342186212539673\n",
      "Epoch: 562 | Discriminator Real Loss: [0.36715710163116455, 0.9375] | Discriminator Fake Loss: [0.7321189641952515, 0.5625] | CTGAN Loss: 2.0812268257141113\n",
      "Epoch: 563 | Discriminator Real Loss: [0.8680278062820435, 0.53125] | Discriminator Fake Loss: [0.5058697462081909, 0.65625] | CTGAN Loss: 1.944994568824768\n",
      "Epoch: 564 | Discriminator Real Loss: [0.7346815466880798, 0.65625] | Discriminator Fake Loss: [0.560522198677063, 0.65625] | CTGAN Loss: 1.2019188404083252\n",
      "Epoch: 565 | Discriminator Real Loss: [0.5196099281311035, 0.78125] | Discriminator Fake Loss: [0.8855090141296387, 0.40625] | CTGAN Loss: 1.401902675628662\n",
      "Epoch: 566 | Discriminator Real Loss: [0.45869433879852295, 0.75] | Discriminator Fake Loss: [0.7208703756332397, 0.5625] | CTGAN Loss: 1.5082837343215942\n",
      "Epoch: 567 | Discriminator Real Loss: [0.39602982997894287, 0.875] | Discriminator Fake Loss: [0.7313490509986877, 0.46875] | CTGAN Loss: 1.7217073440551758\n",
      "Epoch: 568 | Discriminator Real Loss: [0.5766214728355408, 0.71875] | Discriminator Fake Loss: [0.45467740297317505, 0.75] | CTGAN Loss: 1.9643031358718872\n",
      "Epoch: 569 | Discriminator Real Loss: [0.7804001569747925, 0.65625] | Discriminator Fake Loss: [0.30369341373443604, 0.875] | CTGAN Loss: 1.9541789293289185\n",
      "Epoch: 570 | Discriminator Real Loss: [0.5885024070739746, 0.6875] | Discriminator Fake Loss: [0.5257316827774048, 0.71875] | CTGAN Loss: 1.4190423488616943\n",
      "Epoch: 571 | Discriminator Real Loss: [0.27697569131851196, 0.96875] | Discriminator Fake Loss: [0.6695276498794556, 0.59375] | CTGAN Loss: 1.1794540882110596\n",
      "Epoch: 572 | Discriminator Real Loss: [0.2309775948524475, 1.0] | Discriminator Fake Loss: [0.4432557225227356, 0.71875] | CTGAN Loss: 2.235048532485962\n",
      "Epoch: 573 | Discriminator Real Loss: [0.37195295095443726, 0.875] | Discriminator Fake Loss: [0.18983039259910583, 1.0] | CTGAN Loss: 4.049770832061768\n",
      "Epoch: 574 | Discriminator Real Loss: [0.5281146764755249, 0.78125] | Discriminator Fake Loss: [0.0795125886797905, 1.0] | CTGAN Loss: 2.6033620834350586\n",
      "Epoch: 575 | Discriminator Real Loss: [0.4402652382850647, 0.84375] | Discriminator Fake Loss: [0.2789442241191864, 0.875] | CTGAN Loss: 2.354281187057495\n",
      "Epoch: 576 | Discriminator Real Loss: [0.223041832447052, 0.96875] | Discriminator Fake Loss: [0.44582805037498474, 0.71875] | CTGAN Loss: 2.3664379119873047\n",
      "Epoch: 577 | Discriminator Real Loss: [0.23274752497673035, 0.9375] | Discriminator Fake Loss: [0.3521323502063751, 0.84375] | CTGAN Loss: 3.3446505069732666\n",
      "Epoch: 578 | Discriminator Real Loss: [0.3986675441265106, 0.84375] | Discriminator Fake Loss: [0.28783199191093445, 0.84375] | CTGAN Loss: 3.3803470134735107\n",
      "Epoch: 579 | Discriminator Real Loss: [0.3806614875793457, 0.84375] | Discriminator Fake Loss: [0.288934588432312, 0.875] | CTGAN Loss: 3.015474796295166\n",
      "Epoch: 580 | Discriminator Real Loss: [0.5329514741897583, 0.78125] | Discriminator Fake Loss: [0.3119116425514221, 0.875] | CTGAN Loss: 2.7594375610351562\n",
      "Epoch: 581 | Discriminator Real Loss: [0.34484052658081055, 0.9375] | Discriminator Fake Loss: [0.4485703110694885, 0.71875] | CTGAN Loss: 3.4259705543518066\n",
      "Epoch: 582 | Discriminator Real Loss: [0.40839603543281555, 0.8125] | Discriminator Fake Loss: [0.4241572320461273, 0.78125] | CTGAN Loss: 3.9610238075256348\n",
      "Epoch: 583 | Discriminator Real Loss: [0.583881139755249, 0.78125] | Discriminator Fake Loss: [0.4478045105934143, 0.75] | CTGAN Loss: 2.130075454711914\n",
      "Epoch: 584 | Discriminator Real Loss: [0.25181740522384644, 0.90625] | Discriminator Fake Loss: [0.5167887806892395, 0.75] | CTGAN Loss: 3.610098123550415\n",
      "Epoch: 585 | Discriminator Real Loss: [1.047013521194458, 0.5] | Discriminator Fake Loss: [0.762528657913208, 0.53125] | CTGAN Loss: 1.9107681512832642\n",
      "Epoch: 586 | Discriminator Real Loss: [0.24251788854599, 0.9375] | Discriminator Fake Loss: [1.0923058986663818, 0.34375] | CTGAN Loss: 3.1291027069091797\n",
      "Epoch: 587 | Discriminator Real Loss: [0.496902197599411, 0.75] | Discriminator Fake Loss: [0.48493438959121704, 0.75] | CTGAN Loss: 3.155879259109497\n",
      "Epoch: 588 | Discriminator Real Loss: [1.1513746976852417, 0.34375] | Discriminator Fake Loss: [0.6672421097755432, 0.59375] | CTGAN Loss: 1.4073580503463745\n",
      "Epoch: 589 | Discriminator Real Loss: [0.4109453558921814, 0.78125] | Discriminator Fake Loss: [1.029284954071045, 0.34375] | CTGAN Loss: 1.3123703002929688\n",
      "Epoch: 590 | Discriminator Real Loss: [0.3048039674758911, 0.875] | Discriminator Fake Loss: [0.6182079911231995, 0.65625] | CTGAN Loss: 2.0905494689941406\n",
      "Epoch: 591 | Discriminator Real Loss: [0.5143197178840637, 0.75] | Discriminator Fake Loss: [0.43379268050193787, 0.75] | CTGAN Loss: 2.18831205368042\n",
      "Epoch: 592 | Discriminator Real Loss: [0.7707117795944214, 0.59375] | Discriminator Fake Loss: [0.5827165842056274, 0.6875] | CTGAN Loss: 2.2138307094573975\n",
      "Epoch: 593 | Discriminator Real Loss: [0.664956271648407, 0.625] | Discriminator Fake Loss: [0.7398857474327087, 0.5] | CTGAN Loss: 1.5474507808685303\n",
      "Epoch: 594 | Discriminator Real Loss: [0.30791354179382324, 0.9375] | Discriminator Fake Loss: [0.7492372989654541, 0.5] | CTGAN Loss: 1.2382409572601318\n",
      "Epoch: 595 | Discriminator Real Loss: [0.3870602250099182, 0.90625] | Discriminator Fake Loss: [0.7557305097579956, 0.5] | CTGAN Loss: 1.649853229522705\n",
      "Epoch: 596 | Discriminator Real Loss: [0.5277592539787292, 0.78125] | Discriminator Fake Loss: [0.4559048116207123, 0.71875] | CTGAN Loss: 2.1765522956848145\n",
      "Epoch: 597 | Discriminator Real Loss: [0.8722989559173584, 0.53125] | Discriminator Fake Loss: [0.44126397371292114, 0.78125] | CTGAN Loss: 1.978258490562439\n",
      "Epoch: 598 | Discriminator Real Loss: [0.5485708713531494, 0.8125] | Discriminator Fake Loss: [0.5694360733032227, 0.625] | CTGAN Loss: 1.2709548473358154\n",
      "Epoch: 599 | Discriminator Real Loss: [0.28893646597862244, 1.0] | Discriminator Fake Loss: [0.6229257583618164, 0.5] | CTGAN Loss: 1.804021954536438\n",
      "Epoch: 600 | Discriminator Real Loss: [0.3608585298061371, 0.9375] | Discriminator Fake Loss: [0.682730495929718, 0.53125] | CTGAN Loss: 1.6667674779891968\n",
      "Epoch: 601 | Discriminator Real Loss: [0.46995723247528076, 0.84375] | Discriminator Fake Loss: [0.5100473165512085, 0.65625] | CTGAN Loss: 1.772871971130371\n",
      "Epoch: 602 | Discriminator Real Loss: [0.490136981010437, 0.78125] | Discriminator Fake Loss: [0.32966938614845276, 0.84375] | CTGAN Loss: 2.5831398963928223\n",
      "Epoch: 603 | Discriminator Real Loss: [0.6200581789016724, 0.625] | Discriminator Fake Loss: [0.359707236289978, 0.875] | CTGAN Loss: 2.483257532119751\n",
      "Epoch: 604 | Discriminator Real Loss: [0.3727668225765228, 0.96875] | Discriminator Fake Loss: [0.33066070079803467, 0.84375] | CTGAN Loss: 2.3173322677612305\n",
      "Epoch: 605 | Discriminator Real Loss: [0.5120143890380859, 0.8125] | Discriminator Fake Loss: [0.4671744108200073, 0.78125] | CTGAN Loss: 2.2846288681030273\n",
      "Epoch: 606 | Discriminator Real Loss: [0.4044797718524933, 0.9375] | Discriminator Fake Loss: [0.3059864640235901, 0.84375] | CTGAN Loss: 3.082838773727417\n",
      "Epoch: 607 | Discriminator Real Loss: [0.46586376428604126, 0.8125] | Discriminator Fake Loss: [0.1585974097251892, 0.96875] | CTGAN Loss: 3.0073227882385254\n",
      "Epoch: 608 | Discriminator Real Loss: [0.32080650329589844, 0.9375] | Discriminator Fake Loss: [0.2627224922180176, 0.90625] | CTGAN Loss: 3.023224353790283\n",
      "Epoch: 609 | Discriminator Real Loss: [0.3151061534881592, 0.9375] | Discriminator Fake Loss: [0.2683197557926178, 0.90625] | CTGAN Loss: 2.7064528465270996\n",
      "Epoch: 610 | Discriminator Real Loss: [0.27827030420303345, 0.9375] | Discriminator Fake Loss: [0.23734444379806519, 0.96875] | CTGAN Loss: 3.4497532844543457\n",
      "Epoch: 611 | Discriminator Real Loss: [0.394010454416275, 0.875] | Discriminator Fake Loss: [0.26708078384399414, 0.875] | CTGAN Loss: 3.350900173187256\n",
      "Epoch: 612 | Discriminator Real Loss: [0.35095763206481934, 0.84375] | Discriminator Fake Loss: [0.2694319486618042, 0.90625] | CTGAN Loss: 3.381859302520752\n",
      "Epoch: 613 | Discriminator Real Loss: [0.30205658078193665, 0.9375] | Discriminator Fake Loss: [0.2956504821777344, 0.90625] | CTGAN Loss: 2.714116334915161\n",
      "Epoch: 614 | Discriminator Real Loss: [0.31652843952178955, 0.90625] | Discriminator Fake Loss: [0.32712095975875854, 0.875] | CTGAN Loss: 3.235487461090088\n",
      "Epoch: 615 | Discriminator Real Loss: [0.6459596157073975, 0.65625] | Discriminator Fake Loss: [0.40470507740974426, 0.8125] | CTGAN Loss: 1.8241612911224365\n",
      "Epoch: 616 | Discriminator Real Loss: [0.21177677810192108, 0.9375] | Discriminator Fake Loss: [0.7620419263839722, 0.625] | CTGAN Loss: 2.074026584625244\n",
      "Epoch: 617 | Discriminator Real Loss: [0.45197752118110657, 0.75] | Discriminator Fake Loss: [0.5389174222946167, 0.65625] | CTGAN Loss: 2.6555986404418945\n",
      "Epoch: 618 | Discriminator Real Loss: [0.8645641803741455, 0.53125] | Discriminator Fake Loss: [0.7724734544754028, 0.5625] | CTGAN Loss: 1.854966640472412\n",
      "Epoch: 619 | Discriminator Real Loss: [0.37285086512565613, 0.8125] | Discriminator Fake Loss: [0.9457307457923889, 0.5] | CTGAN Loss: 1.6484730243682861\n",
      "Epoch: 620 | Discriminator Real Loss: [0.4861202538013458, 0.75] | Discriminator Fake Loss: [0.6453514099121094, 0.5625] | CTGAN Loss: 2.036165952682495\n",
      "Epoch: 621 | Discriminator Real Loss: [1.0099515914916992, 0.5] | Discriminator Fake Loss: [0.7910140156745911, 0.5] | CTGAN Loss: 0.816626787185669\n",
      "Epoch: 622 | Discriminator Real Loss: [0.36687302589416504, 0.90625] | Discriminator Fake Loss: [1.063934326171875, 0.40625] | CTGAN Loss: 1.0951495170593262\n",
      "Epoch: 623 | Discriminator Real Loss: [0.3929659426212311, 0.78125] | Discriminator Fake Loss: [1.1537489891052246, 0.3125] | CTGAN Loss: 1.7094106674194336\n",
      "Epoch: 624 | Discriminator Real Loss: [0.6819150447845459, 0.65625] | Discriminator Fake Loss: [0.6844121813774109, 0.5] | CTGAN Loss: 1.6108686923980713\n",
      "Epoch: 625 | Discriminator Real Loss: [0.813743531703949, 0.5] | Discriminator Fake Loss: [0.41548073291778564, 0.8125] | CTGAN Loss: 1.8528094291687012\n",
      "Epoch: 626 | Discriminator Real Loss: [0.5836012959480286, 0.8125] | Discriminator Fake Loss: [0.49666911363601685, 0.65625] | CTGAN Loss: 1.6574280261993408\n",
      "Epoch: 627 | Discriminator Real Loss: [0.5967634916305542, 0.78125] | Discriminator Fake Loss: [0.6455081701278687, 0.625] | CTGAN Loss: 1.2431983947753906\n",
      "Epoch: 628 | Discriminator Real Loss: [0.4108957052230835, 0.875] | Discriminator Fake Loss: [0.6101254820823669, 0.5625] | CTGAN Loss: 1.8473401069641113\n",
      "Epoch: 629 | Discriminator Real Loss: [0.44434618949890137, 0.875] | Discriminator Fake Loss: [0.424826979637146, 0.875] | CTGAN Loss: 1.5456514358520508\n",
      "Epoch: 630 | Discriminator Real Loss: [0.6936769485473633, 0.6875] | Discriminator Fake Loss: [0.36151954531669617, 0.875] | CTGAN Loss: 2.2645423412323\n",
      "Epoch: 631 | Discriminator Real Loss: [0.5324483513832092, 0.84375] | Discriminator Fake Loss: [0.32825684547424316, 0.8125] | CTGAN Loss: 1.6597175598144531\n",
      "Epoch: 632 | Discriminator Real Loss: [0.39729583263397217, 0.90625] | Discriminator Fake Loss: [0.3843037486076355, 0.8125] | CTGAN Loss: 1.923744559288025\n",
      "Epoch: 633 | Discriminator Real Loss: [0.40535372495651245, 0.9375] | Discriminator Fake Loss: [0.5488139390945435, 0.65625] | CTGAN Loss: 1.7231777906417847\n",
      "Epoch: 634 | Discriminator Real Loss: [0.4456881880760193, 0.8125] | Discriminator Fake Loss: [0.3610084056854248, 0.75] | CTGAN Loss: 2.4534082412719727\n",
      "Epoch: 635 | Discriminator Real Loss: [0.48319005966186523, 0.78125] | Discriminator Fake Loss: [0.41117602586746216, 0.8125] | CTGAN Loss: 1.8174998760223389\n",
      "Epoch: 636 | Discriminator Real Loss: [0.5433220863342285, 0.75] | Discriminator Fake Loss: [0.5370380878448486, 0.65625] | CTGAN Loss: 1.7448229789733887\n",
      "Epoch: 637 | Discriminator Real Loss: [0.33687400817871094, 0.90625] | Discriminator Fake Loss: [0.4363453984260559, 0.78125] | CTGAN Loss: 2.0834641456604004\n",
      "Epoch: 638 | Discriminator Real Loss: [0.3735731840133667, 0.875] | Discriminator Fake Loss: [0.32479581236839294, 0.875] | CTGAN Loss: 2.958218574523926\n",
      "Epoch: 639 | Discriminator Real Loss: [0.8167124390602112, 0.53125] | Discriminator Fake Loss: [0.31201332807540894, 0.84375] | CTGAN Loss: 1.7474174499511719\n",
      "Epoch: 640 | Discriminator Real Loss: [0.3055439591407776, 0.9375] | Discriminator Fake Loss: [0.6531383991241455, 0.59375] | CTGAN Loss: 1.4101595878601074\n",
      "Epoch: 641 | Discriminator Real Loss: [0.2535746395587921, 0.9375] | Discriminator Fake Loss: [0.41972124576568604, 0.75] | CTGAN Loss: 3.089204788208008\n",
      "Epoch: 642 | Discriminator Real Loss: [0.45866525173187256, 0.8125] | Discriminator Fake Loss: [0.3854391872882843, 0.875] | CTGAN Loss: 2.6878504753112793\n",
      "Epoch: 643 | Discriminator Real Loss: [0.8004475831985474, 0.625] | Discriminator Fake Loss: [0.4442460238933563, 0.6875] | CTGAN Loss: 1.8511162996292114\n",
      "Epoch: 644 | Discriminator Real Loss: [0.5339611768722534, 0.78125] | Discriminator Fake Loss: [0.6967743039131165, 0.625] | CTGAN Loss: 2.5621590614318848\n",
      "Epoch: 645 | Discriminator Real Loss: [0.5076732635498047, 0.71875] | Discriminator Fake Loss: [0.6213886737823486, 0.625] | CTGAN Loss: 2.1625986099243164\n",
      "Epoch: 646 | Discriminator Real Loss: [0.40779516100883484, 0.875] | Discriminator Fake Loss: [0.7990872263908386, 0.40625] | CTGAN Loss: 1.8646804094314575\n",
      "Epoch: 647 | Discriminator Real Loss: [0.624990701675415, 0.59375] | Discriminator Fake Loss: [0.5706590414047241, 0.625] | CTGAN Loss: 2.215423822402954\n",
      "Epoch: 648 | Discriminator Real Loss: [0.7891370058059692, 0.625] | Discriminator Fake Loss: [0.6926789879798889, 0.53125] | CTGAN Loss: 1.2985758781433105\n",
      "Epoch: 649 | Discriminator Real Loss: [0.345763236284256, 0.9375] | Discriminator Fake Loss: [0.862239420413971, 0.5] | CTGAN Loss: 1.5330109596252441\n",
      "Epoch: 650 | Discriminator Real Loss: [0.471335232257843, 0.8125] | Discriminator Fake Loss: [0.5423771739006042, 0.625] | CTGAN Loss: 1.9965753555297852\n",
      "Epoch: 651 | Discriminator Real Loss: [0.6605017781257629, 0.6875] | Discriminator Fake Loss: [0.6240548491477966, 0.65625] | CTGAN Loss: 1.5263725519180298\n",
      "Epoch: 652 | Discriminator Real Loss: [0.4352320730686188, 0.875] | Discriminator Fake Loss: [0.9399518966674805, 0.34375] | CTGAN Loss: 1.2146642208099365\n",
      "Epoch: 653 | Discriminator Real Loss: [0.352903813123703, 0.90625] | Discriminator Fake Loss: [0.6926019191741943, 0.59375] | CTGAN Loss: 1.7938015460968018\n",
      "Epoch: 654 | Discriminator Real Loss: [0.5254348516464233, 0.78125] | Discriminator Fake Loss: [0.4634067118167877, 0.78125] | CTGAN Loss: 2.215494394302368\n",
      "Epoch: 655 | Discriminator Real Loss: [0.9241369366645813, 0.40625] | Discriminator Fake Loss: [0.4149123430252075, 0.71875] | CTGAN Loss: 1.6746747493743896\n",
      "Epoch: 656 | Discriminator Real Loss: [0.4802871346473694, 0.875] | Discriminator Fake Loss: [0.6851576566696167, 0.5] | CTGAN Loss: 1.136899709701538\n",
      "Epoch: 657 | Discriminator Real Loss: [0.35773545503616333, 0.9375] | Discriminator Fake Loss: [0.7717382907867432, 0.40625] | CTGAN Loss: 1.6887847185134888\n",
      "Epoch: 658 | Discriminator Real Loss: [0.473289430141449, 0.875] | Discriminator Fake Loss: [0.4462411403656006, 0.78125] | CTGAN Loss: 2.396732807159424\n",
      "Epoch: 659 | Discriminator Real Loss: [0.8350595235824585, 0.5] | Discriminator Fake Loss: [0.3406899869441986, 0.90625] | CTGAN Loss: 1.8921053409576416\n",
      "Epoch: 660 | Discriminator Real Loss: [0.5504359602928162, 0.71875] | Discriminator Fake Loss: [0.48078420758247375, 0.78125] | CTGAN Loss: 1.3501724004745483\n",
      "Epoch: 661 | Discriminator Real Loss: [0.5193945169448853, 0.78125] | Discriminator Fake Loss: [0.5821264982223511, 0.625] | CTGAN Loss: 1.807844877243042\n",
      "Epoch: 662 | Discriminator Real Loss: [0.5179262161254883, 0.8125] | Discriminator Fake Loss: [0.6189678311347961, 0.59375] | CTGAN Loss: 1.4441072940826416\n",
      "Epoch: 663 | Discriminator Real Loss: [0.4943552613258362, 0.78125] | Discriminator Fake Loss: [0.4204407334327698, 0.8125] | CTGAN Loss: 1.8936563730239868\n",
      "Epoch: 664 | Discriminator Real Loss: [0.5997570753097534, 0.59375] | Discriminator Fake Loss: [0.6301177740097046, 0.5625] | CTGAN Loss: 1.822817325592041\n",
      "Epoch: 665 | Discriminator Real Loss: [0.5290384292602539, 0.75] | Discriminator Fake Loss: [0.5065251588821411, 0.75] | CTGAN Loss: 1.8672786951065063\n",
      "Epoch: 666 | Discriminator Real Loss: [0.7122370004653931, 0.625] | Discriminator Fake Loss: [0.7476692795753479, 0.46875] | CTGAN Loss: 1.8810584545135498\n",
      "Epoch: 667 | Discriminator Real Loss: [0.421170175075531, 0.84375] | Discriminator Fake Loss: [0.6202757358551025, 0.65625] | CTGAN Loss: 1.2833342552185059\n",
      "Epoch: 668 | Discriminator Real Loss: [0.4718160629272461, 0.875] | Discriminator Fake Loss: [0.7145899534225464, 0.53125] | CTGAN Loss: 1.4751625061035156\n",
      "Epoch: 669 | Discriminator Real Loss: [0.6241884827613831, 0.65625] | Discriminator Fake Loss: [0.525178074836731, 0.59375] | CTGAN Loss: 1.5341075658798218\n",
      "Epoch: 670 | Discriminator Real Loss: [0.5148262977600098, 0.6875] | Discriminator Fake Loss: [0.6645833253860474, 0.5] | CTGAN Loss: 1.5952959060668945\n",
      "Epoch: 671 | Discriminator Real Loss: [0.6216018199920654, 0.5625] | Discriminator Fake Loss: [0.6762771606445312, 0.59375] | CTGAN Loss: 1.7386534214019775\n",
      "Epoch: 672 | Discriminator Real Loss: [0.752687931060791, 0.625] | Discriminator Fake Loss: [0.5688521265983582, 0.625] | CTGAN Loss: 1.4784648418426514\n",
      "Epoch: 673 | Discriminator Real Loss: [0.7016071081161499, 0.59375] | Discriminator Fake Loss: [0.5897308588027954, 0.5625] | CTGAN Loss: 1.2640259265899658\n",
      "Epoch: 674 | Discriminator Real Loss: [0.39888453483581543, 0.9375] | Discriminator Fake Loss: [0.7128536701202393, 0.5] | CTGAN Loss: 1.3314532041549683\n",
      "Epoch: 675 | Discriminator Real Loss: [0.45746684074401855, 0.90625] | Discriminator Fake Loss: [0.3764939606189728, 0.75] | CTGAN Loss: 1.7519476413726807\n",
      "Epoch: 676 | Discriminator Real Loss: [0.7323720455169678, 0.65625] | Discriminator Fake Loss: [0.33603453636169434, 0.875] | CTGAN Loss: 1.7382330894470215\n",
      "Epoch: 677 | Discriminator Real Loss: [0.5308700799942017, 0.71875] | Discriminator Fake Loss: [0.8347660303115845, 0.46875] | CTGAN Loss: 1.086942434310913\n",
      "Epoch: 678 | Discriminator Real Loss: [0.40106865763664246, 0.875] | Discriminator Fake Loss: [0.579416036605835, 0.5625] | CTGAN Loss: 1.4613134860992432\n",
      "Epoch: 679 | Discriminator Real Loss: [0.5359880924224854, 0.75] | Discriminator Fake Loss: [0.4472895562648773, 0.8125] | CTGAN Loss: 1.9359393119812012\n",
      "Epoch: 680 | Discriminator Real Loss: [0.5763791799545288, 0.6875] | Discriminator Fake Loss: [0.6436864733695984, 0.625] | CTGAN Loss: 1.5355995893478394\n",
      "Epoch: 681 | Discriminator Real Loss: [0.6176927089691162, 0.65625] | Discriminator Fake Loss: [0.5343118906021118, 0.65625] | CTGAN Loss: 1.3580546379089355\n",
      "Epoch: 682 | Discriminator Real Loss: [0.4063645005226135, 0.90625] | Discriminator Fake Loss: [0.6036755442619324, 0.625] | CTGAN Loss: 1.4627652168273926\n",
      "Epoch: 683 | Discriminator Real Loss: [0.5768725275993347, 0.625] | Discriminator Fake Loss: [0.349074125289917, 0.90625] | CTGAN Loss: 1.3793545961380005\n",
      "Epoch: 684 | Discriminator Real Loss: [0.3934168815612793, 0.90625] | Discriminator Fake Loss: [0.4992263913154602, 0.625] | CTGAN Loss: 1.8885143995285034\n",
      "Epoch: 685 | Discriminator Real Loss: [0.4436843991279602, 0.8125] | Discriminator Fake Loss: [0.4974707067012787, 0.6875] | CTGAN Loss: 2.266653537750244\n",
      "Epoch: 686 | Discriminator Real Loss: [0.599071741104126, 0.6875] | Discriminator Fake Loss: [0.43937742710113525, 0.78125] | CTGAN Loss: 2.2781643867492676\n",
      "Epoch: 687 | Discriminator Real Loss: [0.6572494506835938, 0.625] | Discriminator Fake Loss: [0.4098351299762726, 0.8125] | CTGAN Loss: 2.0704946517944336\n",
      "Epoch: 688 | Discriminator Real Loss: [0.39990171790122986, 0.8125] | Discriminator Fake Loss: [0.35997968912124634, 0.90625] | CTGAN Loss: 2.1995389461517334\n",
      "Epoch: 689 | Discriminator Real Loss: [0.4314383864402771, 0.78125] | Discriminator Fake Loss: [0.611311674118042, 0.6875] | CTGAN Loss: 2.6148035526275635\n",
      "Epoch: 690 | Discriminator Real Loss: [0.7930296659469604, 0.65625] | Discriminator Fake Loss: [0.38052642345428467, 0.78125] | CTGAN Loss: 2.0257694721221924\n",
      "Epoch: 691 | Discriminator Real Loss: [0.27628737688064575, 0.9375] | Discriminator Fake Loss: [0.4580439031124115, 0.71875] | CTGAN Loss: 1.9521870613098145\n",
      "Epoch: 692 | Discriminator Real Loss: [0.40780404210090637, 0.90625] | Discriminator Fake Loss: [0.6460981369018555, 0.5625] | CTGAN Loss: 2.095749616622925\n",
      "Epoch: 693 | Discriminator Real Loss: [0.5017304420471191, 0.71875] | Discriminator Fake Loss: [0.4334489405155182, 0.71875] | CTGAN Loss: 2.5856854915618896\n",
      "Epoch: 694 | Discriminator Real Loss: [0.6935542225837708, 0.65625] | Discriminator Fake Loss: [0.597244143486023, 0.71875] | CTGAN Loss: 1.5697104930877686\n",
      "Epoch: 695 | Discriminator Real Loss: [0.4047452509403229, 0.90625] | Discriminator Fake Loss: [0.7439611554145813, 0.53125] | CTGAN Loss: 2.017552375793457\n",
      "Epoch: 696 | Discriminator Real Loss: [0.5453917980194092, 0.71875] | Discriminator Fake Loss: [0.41131341457366943, 0.78125] | CTGAN Loss: 2.3989930152893066\n",
      "Epoch: 697 | Discriminator Real Loss: [0.9803256392478943, 0.46875] | Discriminator Fake Loss: [1.1049158573150635, 0.25] | CTGAN Loss: 0.8878242373466492\n",
      "Epoch: 698 | Discriminator Real Loss: [0.33206671476364136, 0.90625] | Discriminator Fake Loss: [1.0987675189971924, 0.3125] | CTGAN Loss: 0.9165915846824646\n",
      "Epoch: 699 | Discriminator Real Loss: [0.38826075196266174, 0.875] | Discriminator Fake Loss: [0.5181103348731995, 0.6875] | CTGAN Loss: 2.312319278717041\n",
      "Epoch: 700 | Discriminator Real Loss: [0.9667949080467224, 0.46875] | Discriminator Fake Loss: [0.5451682806015015, 0.65625] | CTGAN Loss: 1.1794326305389404\n",
      "Epoch: 701 | Discriminator Real Loss: [0.7734162211418152, 0.65625] | Discriminator Fake Loss: [0.8515646457672119, 0.40625] | CTGAN Loss: 1.040597915649414\n",
      "Epoch: 702 | Discriminator Real Loss: [0.5136430263519287, 0.84375] | Discriminator Fake Loss: [1.234493613243103, 0.09375] | CTGAN Loss: 0.573676347732544\n",
      "Epoch: 703 | Discriminator Real Loss: [0.47201335430145264, 0.8125] | Discriminator Fake Loss: [1.101743221282959, 0.1875] | CTGAN Loss: 0.8280706405639648\n",
      "Epoch: 704 | Discriminator Real Loss: [0.4968103766441345, 0.84375] | Discriminator Fake Loss: [0.8691188097000122, 0.28125] | CTGAN Loss: 1.020388126373291\n",
      "Epoch: 705 | Discriminator Real Loss: [0.7999419569969177, 0.4375] | Discriminator Fake Loss: [0.5980449914932251, 0.59375] | CTGAN Loss: 1.4357357025146484\n",
      "Epoch: 706 | Discriminator Real Loss: [0.7527729272842407, 0.65625] | Discriminator Fake Loss: [0.6873781681060791, 0.5] | CTGAN Loss: 0.9076931476593018\n",
      "Epoch: 707 | Discriminator Real Loss: [0.6333346366882324, 0.65625] | Discriminator Fake Loss: [0.7482119798660278, 0.40625] | CTGAN Loss: 0.8310500383377075\n",
      "Epoch: 708 | Discriminator Real Loss: [0.6259586811065674, 0.65625] | Discriminator Fake Loss: [0.6116693615913391, 0.53125] | CTGAN Loss: 0.943882405757904\n",
      "Epoch: 709 | Discriminator Real Loss: [0.6573905944824219, 0.65625] | Discriminator Fake Loss: [0.5718164443969727, 0.53125] | CTGAN Loss: 0.8204779624938965\n",
      "Epoch: 710 | Discriminator Real Loss: [0.5615142583847046, 0.8125] | Discriminator Fake Loss: [0.7223826050758362, 0.375] | CTGAN Loss: 1.4834320545196533\n",
      "Epoch: 711 | Discriminator Real Loss: [0.526759684085846, 0.84375] | Discriminator Fake Loss: [0.6340914964675903, 0.5625] | CTGAN Loss: 1.3147783279418945\n",
      "Epoch: 712 | Discriminator Real Loss: [0.5874359011650085, 0.75] | Discriminator Fake Loss: [0.531092643737793, 0.5625] | CTGAN Loss: 1.4680871963500977\n",
      "Epoch: 713 | Discriminator Real Loss: [0.5332270860671997, 0.84375] | Discriminator Fake Loss: [0.6053265929222107, 0.5] | CTGAN Loss: 1.3345400094985962\n",
      "Epoch: 714 | Discriminator Real Loss: [0.5052899122238159, 0.84375] | Discriminator Fake Loss: [0.5457217693328857, 0.5625] | CTGAN Loss: 1.5361874103546143\n",
      "Epoch: 715 | Discriminator Real Loss: [0.5882186889648438, 0.71875] | Discriminator Fake Loss: [0.3794011175632477, 0.84375] | CTGAN Loss: 2.065600872039795\n",
      "Epoch: 716 | Discriminator Real Loss: [0.5956999063491821, 0.78125] | Discriminator Fake Loss: [0.47975054383277893, 0.65625] | CTGAN Loss: 1.495780110359192\n",
      "Epoch: 717 | Discriminator Real Loss: [0.582327127456665, 0.71875] | Discriminator Fake Loss: [0.34808069467544556, 0.9375] | CTGAN Loss: 1.7587978839874268\n",
      "Epoch: 718 | Discriminator Real Loss: [0.7622350454330444, 0.65625] | Discriminator Fake Loss: [0.5061991214752197, 0.71875] | CTGAN Loss: 1.526400089263916\n",
      "Epoch: 719 | Discriminator Real Loss: [0.5760172605514526, 0.84375] | Discriminator Fake Loss: [0.5201842188835144, 0.75] | CTGAN Loss: 1.5771424770355225\n",
      "Epoch: 720 | Discriminator Real Loss: [0.44464433193206787, 0.90625] | Discriminator Fake Loss: [0.3753357529640198, 0.84375] | CTGAN Loss: 1.6647169589996338\n",
      "Epoch: 721 | Discriminator Real Loss: [0.5249360799789429, 0.78125] | Discriminator Fake Loss: [0.4333210587501526, 0.6875] | CTGAN Loss: 2.159935474395752\n",
      "Epoch: 722 | Discriminator Real Loss: [0.510259747505188, 0.78125] | Discriminator Fake Loss: [0.38718181848526, 0.78125] | CTGAN Loss: 1.971299171447754\n",
      "Epoch: 723 | Discriminator Real Loss: [0.46291041374206543, 0.875] | Discriminator Fake Loss: [0.3490695357322693, 0.875] | CTGAN Loss: 1.7992490530014038\n",
      "Epoch: 724 | Discriminator Real Loss: [0.452639102935791, 0.875] | Discriminator Fake Loss: [0.35622942447662354, 0.875] | CTGAN Loss: 2.086167812347412\n",
      "Epoch: 725 | Discriminator Real Loss: [0.4414829611778259, 0.84375] | Discriminator Fake Loss: [0.4641077518463135, 0.71875] | CTGAN Loss: 2.068204164505005\n",
      "Epoch: 726 | Discriminator Real Loss: [0.48010796308517456, 0.8125] | Discriminator Fake Loss: [0.2939460575580597, 0.96875] | CTGAN Loss: 2.094714403152466\n",
      "Epoch: 727 | Discriminator Real Loss: [0.44688528776168823, 0.875] | Discriminator Fake Loss: [0.6181352138519287, 0.59375] | CTGAN Loss: 2.1633310317993164\n",
      "Epoch: 728 | Discriminator Real Loss: [0.4417528808116913, 0.875] | Discriminator Fake Loss: [0.44432157278060913, 0.8125] | CTGAN Loss: 2.2924070358276367\n",
      "Epoch: 729 | Discriminator Real Loss: [0.5244623422622681, 0.6875] | Discriminator Fake Loss: [0.5028427839279175, 0.71875] | CTGAN Loss: 1.7276177406311035\n",
      "Epoch: 730 | Discriminator Real Loss: [0.5998857617378235, 0.65625] | Discriminator Fake Loss: [0.3932848870754242, 0.8125] | CTGAN Loss: 1.3304417133331299\n",
      "Epoch: 731 | Discriminator Real Loss: [0.35068535804748535, 0.96875] | Discriminator Fake Loss: [0.49963608384132385, 0.6875] | CTGAN Loss: 1.7807892560958862\n",
      "Epoch: 732 | Discriminator Real Loss: [0.39736074209213257, 0.9375] | Discriminator Fake Loss: [0.5972773432731628, 0.65625] | CTGAN Loss: 2.591980457305908\n",
      "Epoch: 733 | Discriminator Real Loss: [0.7019847631454468, 0.59375] | Discriminator Fake Loss: [0.5664258003234863, 0.59375] | CTGAN Loss: 1.7077069282531738\n",
      "Epoch: 734 | Discriminator Real Loss: [0.4239577054977417, 0.875] | Discriminator Fake Loss: [0.7310453057289124, 0.53125] | CTGAN Loss: 2.157808303833008\n",
      "Epoch: 735 | Discriminator Real Loss: [0.42886707186698914, 0.8125] | Discriminator Fake Loss: [0.6860066056251526, 0.5] | CTGAN Loss: 1.9528478384017944\n",
      "Epoch: 736 | Discriminator Real Loss: [0.5423434972763062, 0.6875] | Discriminator Fake Loss: [0.6850004196166992, 0.5625] | CTGAN Loss: 1.8584951162338257\n",
      "Epoch: 737 | Discriminator Real Loss: [0.5805836915969849, 0.8125] | Discriminator Fake Loss: [0.6238211393356323, 0.65625] | CTGAN Loss: 1.5801132917404175\n",
      "Epoch: 738 | Discriminator Real Loss: [0.6845890283584595, 0.65625] | Discriminator Fake Loss: [0.6673652529716492, 0.59375] | CTGAN Loss: 1.4362125396728516\n",
      "Epoch: 739 | Discriminator Real Loss: [0.3832496404647827, 0.84375] | Discriminator Fake Loss: [0.9112268090248108, 0.5] | CTGAN Loss: 2.246499538421631\n",
      "Epoch: 740 | Discriminator Real Loss: [0.402070552110672, 0.84375] | Discriminator Fake Loss: [0.5422062873840332, 0.625] | CTGAN Loss: 1.671817660331726\n",
      "Epoch: 741 | Discriminator Real Loss: [0.5830170512199402, 0.71875] | Discriminator Fake Loss: [0.4465731978416443, 0.84375] | CTGAN Loss: 2.0994536876678467\n",
      "Epoch: 742 | Discriminator Real Loss: [0.6971047520637512, 0.65625] | Discriminator Fake Loss: [0.4017438292503357, 0.84375] | CTGAN Loss: 1.5507335662841797\n",
      "Epoch: 743 | Discriminator Real Loss: [0.5524277687072754, 0.75] | Discriminator Fake Loss: [0.5645891427993774, 0.6875] | CTGAN Loss: 1.6748285293579102\n",
      "Epoch: 744 | Discriminator Real Loss: [0.40778303146362305, 0.875] | Discriminator Fake Loss: [0.4801877439022064, 0.71875] | CTGAN Loss: 1.785701036453247\n",
      "Epoch: 745 | Discriminator Real Loss: [0.3706362843513489, 0.9375] | Discriminator Fake Loss: [0.4203365445137024, 0.78125] | CTGAN Loss: 1.9393044710159302\n",
      "Epoch: 746 | Discriminator Real Loss: [0.5426167845726013, 0.8125] | Discriminator Fake Loss: [0.5103341937065125, 0.71875] | CTGAN Loss: 2.3487775325775146\n",
      "Epoch: 747 | Discriminator Real Loss: [0.5143203735351562, 0.78125] | Discriminator Fake Loss: [0.3393781781196594, 0.8125] | CTGAN Loss: 2.7855167388916016\n",
      "Epoch: 748 | Discriminator Real Loss: [0.5103417634963989, 0.78125] | Discriminator Fake Loss: [0.42941081523895264, 0.8125] | CTGAN Loss: 1.6570062637329102\n",
      "Epoch: 749 | Discriminator Real Loss: [0.3658948838710785, 0.9375] | Discriminator Fake Loss: [0.5348984003067017, 0.75] | CTGAN Loss: 2.02138090133667\n",
      "Epoch: 750 | Discriminator Real Loss: [0.4306565225124359, 0.8125] | Discriminator Fake Loss: [0.49729543924331665, 0.6875] | CTGAN Loss: 2.005511522293091\n",
      "Epoch: 751 | Discriminator Real Loss: [0.7077623605728149, 0.625] | Discriminator Fake Loss: [0.5331588983535767, 0.75] | CTGAN Loss: 2.6039774417877197\n",
      "Epoch: 752 | Discriminator Real Loss: [0.4624879062175751, 0.84375] | Discriminator Fake Loss: [0.4407265782356262, 0.78125] | CTGAN Loss: 2.0029330253601074\n",
      "Epoch: 753 | Discriminator Real Loss: [0.4739342927932739, 0.8125] | Discriminator Fake Loss: [0.547337532043457, 0.71875] | CTGAN Loss: 1.6930427551269531\n",
      "Epoch: 754 | Discriminator Real Loss: [0.4464494585990906, 0.78125] | Discriminator Fake Loss: [0.6707175374031067, 0.625] | CTGAN Loss: 1.8419091701507568\n",
      "Epoch: 755 | Discriminator Real Loss: [0.5489480495452881, 0.84375] | Discriminator Fake Loss: [0.6296863555908203, 0.65625] | CTGAN Loss: 2.438185214996338\n",
      "Epoch: 756 | Discriminator Real Loss: [0.8659854531288147, 0.625] | Discriminator Fake Loss: [0.38329482078552246, 0.8125] | CTGAN Loss: 2.101655960083008\n",
      "Epoch: 757 | Discriminator Real Loss: [0.6260610818862915, 0.71875] | Discriminator Fake Loss: [0.9615501165390015, 0.34375] | CTGAN Loss: 1.8519320487976074\n",
      "Epoch: 758 | Discriminator Real Loss: [0.5092310905456543, 0.75] | Discriminator Fake Loss: [0.7168885469436646, 0.5625] | CTGAN Loss: 1.5173585414886475\n",
      "Epoch: 759 | Discriminator Real Loss: [0.43584567308425903, 0.875] | Discriminator Fake Loss: [0.7043440341949463, 0.4375] | CTGAN Loss: 2.148326873779297\n",
      "Epoch: 760 | Discriminator Real Loss: [0.883386492729187, 0.71875] | Discriminator Fake Loss: [0.7919941544532776, 0.53125] | CTGAN Loss: 1.4607806205749512\n",
      "Epoch: 761 | Discriminator Real Loss: [0.34027013182640076, 0.9375] | Discriminator Fake Loss: [0.7560610771179199, 0.5625] | CTGAN Loss: 1.8425493240356445\n",
      "Epoch: 762 | Discriminator Real Loss: [0.6979174613952637, 0.53125] | Discriminator Fake Loss: [0.39990073442459106, 0.8125] | CTGAN Loss: 2.081089973449707\n",
      "Epoch: 763 | Discriminator Real Loss: [0.7464554309844971, 0.625] | Discriminator Fake Loss: [0.4902743697166443, 0.71875] | CTGAN Loss: 1.6981950998306274\n",
      "Epoch: 764 | Discriminator Real Loss: [0.5486130118370056, 0.71875] | Discriminator Fake Loss: [0.6063708066940308, 0.625] | CTGAN Loss: 1.193933129310608\n",
      "Epoch: 765 | Discriminator Real Loss: [0.4004600942134857, 0.90625] | Discriminator Fake Loss: [0.6743292808532715, 0.5625] | CTGAN Loss: 1.5946760177612305\n",
      "Epoch: 766 | Discriminator Real Loss: [0.5887061357498169, 0.6875] | Discriminator Fake Loss: [0.36139976978302, 0.84375] | CTGAN Loss: 2.664048194885254\n",
      "Epoch: 767 | Discriminator Real Loss: [0.5435861349105835, 0.75] | Discriminator Fake Loss: [0.33271121978759766, 0.875] | CTGAN Loss: 2.581174850463867\n",
      "Epoch: 768 | Discriminator Real Loss: [0.6617860198020935, 0.71875] | Discriminator Fake Loss: [0.26392269134521484, 0.875] | CTGAN Loss: 2.4235899448394775\n",
      "Epoch: 769 | Discriminator Real Loss: [0.7188875675201416, 0.6875] | Discriminator Fake Loss: [0.7122514247894287, 0.5625] | CTGAN Loss: 1.2198143005371094\n",
      "Epoch: 770 | Discriminator Real Loss: [0.24278505146503448, 0.9375] | Discriminator Fake Loss: [0.9177799224853516, 0.375] | CTGAN Loss: 1.6241018772125244\n",
      "Epoch: 771 | Discriminator Real Loss: [0.3048211932182312, 0.9375] | Discriminator Fake Loss: [0.3607257902622223, 0.875] | CTGAN Loss: 2.719614028930664\n",
      "Epoch: 772 | Discriminator Real Loss: [0.7448804378509521, 0.71875] | Discriminator Fake Loss: [0.5368033647537231, 0.65625] | CTGAN Loss: 2.047151565551758\n",
      "Epoch: 773 | Discriminator Real Loss: [0.595948338508606, 0.75] | Discriminator Fake Loss: [0.5821713209152222, 0.65625] | CTGAN Loss: 1.9780299663543701\n",
      "Epoch: 774 | Discriminator Real Loss: [0.5965543985366821, 0.6875] | Discriminator Fake Loss: [0.6640795469284058, 0.625] | CTGAN Loss: 1.7532944679260254\n",
      "Epoch: 775 | Discriminator Real Loss: [0.5237594246864319, 0.78125] | Discriminator Fake Loss: [0.507839560508728, 0.78125] | CTGAN Loss: 1.4623253345489502\n",
      "Epoch: 776 | Discriminator Real Loss: [0.49346017837524414, 0.8125] | Discriminator Fake Loss: [0.5079770088195801, 0.6875] | CTGAN Loss: 1.891048789024353\n",
      "Epoch: 777 | Discriminator Real Loss: [0.5380593538284302, 0.71875] | Discriminator Fake Loss: [0.612053394317627, 0.71875] | CTGAN Loss: 1.7576501369476318\n",
      "Epoch: 778 | Discriminator Real Loss: [0.5457419157028198, 0.78125] | Discriminator Fake Loss: [0.6406724452972412, 0.625] | CTGAN Loss: 1.3154159784317017\n",
      "Epoch: 779 | Discriminator Real Loss: [0.3479655981063843, 0.875] | Discriminator Fake Loss: [0.6287778615951538, 0.59375] | CTGAN Loss: 1.712397813796997\n",
      "Epoch: 780 | Discriminator Real Loss: [0.809415340423584, 0.5625] | Discriminator Fake Loss: [0.504780113697052, 0.65625] | CTGAN Loss: 1.578485369682312\n",
      "Epoch: 781 | Discriminator Real Loss: [0.8232710361480713, 0.46875] | Discriminator Fake Loss: [0.8310756087303162, 0.4375] | CTGAN Loss: 1.1770706176757812\n",
      "Epoch: 782 | Discriminator Real Loss: [0.3606964945793152, 0.9375] | Discriminator Fake Loss: [0.9687422513961792, 0.375] | CTGAN Loss: 1.3929857015609741\n",
      "Epoch: 783 | Discriminator Real Loss: [0.5145128965377808, 0.71875] | Discriminator Fake Loss: [0.7592747211456299, 0.46875] | CTGAN Loss: 1.5513780117034912\n",
      "Epoch: 784 | Discriminator Real Loss: [0.5950701236724854, 0.75] | Discriminator Fake Loss: [0.6035681962966919, 0.625] | CTGAN Loss: 1.1536247730255127\n",
      "Epoch: 785 | Discriminator Real Loss: [0.5895804762840271, 0.78125] | Discriminator Fake Loss: [0.7126158475875854, 0.4375] | CTGAN Loss: 1.4168763160705566\n",
      "Epoch: 786 | Discriminator Real Loss: [0.541333794593811, 0.78125] | Discriminator Fake Loss: [0.5033437609672546, 0.625] | CTGAN Loss: 1.8514862060546875\n",
      "Epoch: 787 | Discriminator Real Loss: [0.8274573087692261, 0.5625] | Discriminator Fake Loss: [0.42304694652557373, 0.71875] | CTGAN Loss: 1.4912737607955933\n",
      "Epoch: 788 | Discriminator Real Loss: [0.4660041332244873, 0.84375] | Discriminator Fake Loss: [0.7161422967910767, 0.46875] | CTGAN Loss: 1.1839137077331543\n",
      "Epoch: 789 | Discriminator Real Loss: [0.27422988414764404, 1.0] | Discriminator Fake Loss: [0.6991396546363831, 0.5625] | CTGAN Loss: 1.5140858888626099\n",
      "Epoch: 790 | Discriminator Real Loss: [0.43674200773239136, 0.90625] | Discriminator Fake Loss: [0.5068449974060059, 0.65625] | CTGAN Loss: 1.6597394943237305\n",
      "Epoch: 791 | Discriminator Real Loss: [0.7117942571640015, 0.625] | Discriminator Fake Loss: [0.3604152202606201, 0.9375] | CTGAN Loss: 2.3504533767700195\n",
      "Epoch: 792 | Discriminator Real Loss: [0.7398233413696289, 0.59375] | Discriminator Fake Loss: [0.3419681191444397, 0.8125] | CTGAN Loss: 1.9667925834655762\n",
      "Epoch: 793 | Discriminator Real Loss: [0.4774407744407654, 0.84375] | Discriminator Fake Loss: [0.458924263715744, 0.6875] | CTGAN Loss: 1.6949536800384521\n",
      "Epoch: 794 | Discriminator Real Loss: [0.40742674469947815, 0.875] | Discriminator Fake Loss: [0.6010579466819763, 0.65625] | CTGAN Loss: 1.6401009559631348\n",
      "Epoch: 795 | Discriminator Real Loss: [0.3607189655303955, 0.9375] | Discriminator Fake Loss: [0.7282547354698181, 0.5] | CTGAN Loss: 2.1451921463012695\n",
      "Epoch: 796 | Discriminator Real Loss: [0.500361442565918, 0.84375] | Discriminator Fake Loss: [0.36405399441719055, 0.84375] | CTGAN Loss: 2.3301801681518555\n",
      "Epoch: 797 | Discriminator Real Loss: [0.491420179605484, 0.90625] | Discriminator Fake Loss: [0.3775239586830139, 0.875] | CTGAN Loss: 2.287649631500244\n",
      "Epoch: 798 | Discriminator Real Loss: [0.6319528818130493, 0.6875] | Discriminator Fake Loss: [0.35257837176322937, 0.84375] | CTGAN Loss: 2.4588394165039062\n",
      "Epoch: 799 | Discriminator Real Loss: [0.6793925762176514, 0.71875] | Discriminator Fake Loss: [0.4490261673927307, 0.6875] | CTGAN Loss: 2.012136459350586\n",
      "Epoch: 800 | Discriminator Real Loss: [0.44696664810180664, 0.875] | Discriminator Fake Loss: [0.5317054986953735, 0.75] | CTGAN Loss: 1.7524069547653198\n",
      "Epoch: 801 | Discriminator Real Loss: [0.5662487149238586, 0.75] | Discriminator Fake Loss: [0.6464262008666992, 0.625] | CTGAN Loss: 1.1119966506958008\n",
      "Epoch: 802 | Discriminator Real Loss: [0.5149065256118774, 0.8125] | Discriminator Fake Loss: [0.5400532484054565, 0.65625] | CTGAN Loss: 1.9103357791900635\n",
      "Epoch: 803 | Discriminator Real Loss: [0.48172417283058167, 0.8125] | Discriminator Fake Loss: [0.4088183343410492, 0.6875] | CTGAN Loss: 2.0512094497680664\n",
      "Epoch: 804 | Discriminator Real Loss: [0.7122308015823364, 0.5625] | Discriminator Fake Loss: [0.3909832537174225, 0.78125] | CTGAN Loss: 1.606256127357483\n",
      "Epoch: 805 | Discriminator Real Loss: [0.6648580431938171, 0.71875] | Discriminator Fake Loss: [0.7540861368179321, 0.53125] | CTGAN Loss: 1.5120395421981812\n",
      "Epoch: 806 | Discriminator Real Loss: [0.4658062160015106, 0.78125] | Discriminator Fake Loss: [1.297655463218689, 0.15625] | CTGAN Loss: 1.0581772327423096\n",
      "Epoch: 807 | Discriminator Real Loss: [0.3303857445716858, 0.875] | Discriminator Fake Loss: [0.962861180305481, 0.25] | CTGAN Loss: 1.2774145603179932\n",
      "Epoch: 808 | Discriminator Real Loss: [0.6463419198989868, 0.78125] | Discriminator Fake Loss: [0.527269721031189, 0.71875] | CTGAN Loss: 1.9973686933517456\n",
      "Epoch: 809 | Discriminator Real Loss: [1.2455803155899048, 0.25] | Discriminator Fake Loss: [0.7519496083259583, 0.46875] | CTGAN Loss: 1.2036867141723633\n",
      "Epoch: 810 | Discriminator Real Loss: [0.4248626232147217, 0.84375] | Discriminator Fake Loss: [0.939936637878418, 0.3125] | CTGAN Loss: 0.8275471329689026\n",
      "Epoch: 811 | Discriminator Real Loss: [0.4387381970882416, 0.8125] | Discriminator Fake Loss: [0.891099214553833, 0.28125] | CTGAN Loss: 1.1967748403549194\n",
      "Epoch: 812 | Discriminator Real Loss: [0.5142243504524231, 0.8125] | Discriminator Fake Loss: [0.525111198425293, 0.78125] | CTGAN Loss: 1.4772725105285645\n",
      "Epoch: 813 | Discriminator Real Loss: [0.7464485168457031, 0.59375] | Discriminator Fake Loss: [0.604682207107544, 0.59375] | CTGAN Loss: 1.1370841264724731\n",
      "Epoch: 814 | Discriminator Real Loss: [0.756775975227356, 0.59375] | Discriminator Fake Loss: [0.61357182264328, 0.625] | CTGAN Loss: 1.0682392120361328\n",
      "Epoch: 815 | Discriminator Real Loss: [0.5958956480026245, 0.625] | Discriminator Fake Loss: [0.7372199892997742, 0.5] | CTGAN Loss: 0.8303781747817993\n",
      "Epoch: 816 | Discriminator Real Loss: [0.5399392247200012, 0.75] | Discriminator Fake Loss: [0.7656800746917725, 0.34375] | CTGAN Loss: 1.04819655418396\n",
      "Epoch: 817 | Discriminator Real Loss: [0.49330979585647583, 0.78125] | Discriminator Fake Loss: [0.6472374796867371, 0.5] | CTGAN Loss: 1.1981379985809326\n",
      "Epoch: 818 | Discriminator Real Loss: [0.5562406182289124, 0.78125] | Discriminator Fake Loss: [0.5801495313644409, 0.625] | CTGAN Loss: 1.5168286561965942\n",
      "Epoch: 819 | Discriminator Real Loss: [0.7755221128463745, 0.5625] | Discriminator Fake Loss: [0.6364098191261292, 0.5625] | CTGAN Loss: 0.972075343132019\n",
      "Epoch: 820 | Discriminator Real Loss: [0.6206434965133667, 0.59375] | Discriminator Fake Loss: [0.6963759660720825, 0.53125] | CTGAN Loss: 1.2049474716186523\n",
      "Epoch: 821 | Discriminator Real Loss: [0.6597152948379517, 0.59375] | Discriminator Fake Loss: [0.5681993961334229, 0.71875] | CTGAN Loss: 1.1814271211624146\n",
      "Epoch: 822 | Discriminator Real Loss: [0.595761775970459, 0.625] | Discriminator Fake Loss: [0.5228621959686279, 0.6875] | CTGAN Loss: 1.2102317810058594\n",
      "Epoch: 823 | Discriminator Real Loss: [0.5338115692138672, 0.75] | Discriminator Fake Loss: [0.719235897064209, 0.375] | CTGAN Loss: 1.3305128812789917\n",
      "Epoch: 824 | Discriminator Real Loss: [0.544818103313446, 0.8125] | Discriminator Fake Loss: [0.6342147588729858, 0.53125] | CTGAN Loss: 1.2309229373931885\n",
      "Epoch: 825 | Discriminator Real Loss: [0.6837661862373352, 0.65625] | Discriminator Fake Loss: [0.5696865320205688, 0.625] | CTGAN Loss: 1.2090319395065308\n",
      "Epoch: 826 | Discriminator Real Loss: [0.5264122486114502, 0.78125] | Discriminator Fake Loss: [0.5241606831550598, 0.71875] | CTGAN Loss: 1.1848081350326538\n",
      "Epoch: 827 | Discriminator Real Loss: [0.5875248908996582, 0.71875] | Discriminator Fake Loss: [0.4867767095565796, 0.78125] | CTGAN Loss: 1.343342900276184\n",
      "Epoch: 828 | Discriminator Real Loss: [0.5241758823394775, 0.84375] | Discriminator Fake Loss: [0.555403470993042, 0.71875] | CTGAN Loss: 1.3760943412780762\n",
      "Epoch: 829 | Discriminator Real Loss: [0.6410425305366516, 0.71875] | Discriminator Fake Loss: [0.4377344250679016, 0.84375] | CTGAN Loss: 1.3935372829437256\n",
      "Epoch: 830 | Discriminator Real Loss: [0.623189389705658, 0.6875] | Discriminator Fake Loss: [0.4559171795845032, 0.71875] | CTGAN Loss: 1.2386512756347656\n",
      "Epoch: 831 | Discriminator Real Loss: [0.5052105188369751, 0.84375] | Discriminator Fake Loss: [0.6874246001243591, 0.46875] | CTGAN Loss: 1.238159418106079\n",
      "Epoch: 832 | Discriminator Real Loss: [0.569735050201416, 0.75] | Discriminator Fake Loss: [0.5610291957855225, 0.71875] | CTGAN Loss: 1.548863172531128\n",
      "Epoch: 833 | Discriminator Real Loss: [0.5120168924331665, 0.8125] | Discriminator Fake Loss: [0.621667742729187, 0.53125] | CTGAN Loss: 1.5134999752044678\n",
      "Epoch: 834 | Discriminator Real Loss: [0.5487284660339355, 0.71875] | Discriminator Fake Loss: [0.38973385095596313, 0.84375] | CTGAN Loss: 1.9035736322402954\n",
      "Epoch: 835 | Discriminator Real Loss: [0.5929666757583618, 0.6875] | Discriminator Fake Loss: [0.5047388076782227, 0.78125] | CTGAN Loss: 1.6677191257476807\n",
      "Epoch: 836 | Discriminator Real Loss: [0.6005599498748779, 0.65625] | Discriminator Fake Loss: [0.4276224374771118, 0.75] | CTGAN Loss: 1.587881088256836\n",
      "Epoch: 837 | Discriminator Real Loss: [0.5298116207122803, 0.75] | Discriminator Fake Loss: [0.5118165016174316, 0.75] | CTGAN Loss: 1.4618349075317383\n",
      "Epoch: 838 | Discriminator Real Loss: [0.4476037323474884, 0.84375] | Discriminator Fake Loss: [0.6377847194671631, 0.59375] | CTGAN Loss: 1.4548735618591309\n",
      "Epoch: 839 | Discriminator Real Loss: [0.5920510292053223, 0.65625] | Discriminator Fake Loss: [0.6027940511703491, 0.6875] | CTGAN Loss: 1.274219274520874\n",
      "Epoch: 840 | Discriminator Real Loss: [0.44174712896347046, 0.90625] | Discriminator Fake Loss: [0.7687641382217407, 0.40625] | CTGAN Loss: 1.5999964475631714\n",
      "Epoch: 841 | Discriminator Real Loss: [0.5526010394096375, 0.78125] | Discriminator Fake Loss: [0.6468278169631958, 0.625] | CTGAN Loss: 1.7499244213104248\n",
      "Epoch: 842 | Discriminator Real Loss: [0.7630952596664429, 0.625] | Discriminator Fake Loss: [0.434294193983078, 0.8125] | CTGAN Loss: 1.4290931224822998\n",
      "Epoch: 843 | Discriminator Real Loss: [0.5446507930755615, 0.8125] | Discriminator Fake Loss: [0.6224143505096436, 0.53125] | CTGAN Loss: 1.937551498413086\n",
      "Epoch: 844 | Discriminator Real Loss: [0.5714021921157837, 0.6875] | Discriminator Fake Loss: [0.7754013538360596, 0.53125] | CTGAN Loss: 1.6639485359191895\n",
      "Epoch: 845 | Discriminator Real Loss: [0.4173125624656677, 0.90625] | Discriminator Fake Loss: [0.7703186273574829, 0.40625] | CTGAN Loss: 0.972420871257782\n",
      "Epoch: 846 | Discriminator Real Loss: [0.45950478315353394, 0.8125] | Discriminator Fake Loss: [0.50617915391922, 0.6875] | CTGAN Loss: 1.8439791202545166\n",
      "Epoch: 847 | Discriminator Real Loss: [0.4888157248497009, 0.84375] | Discriminator Fake Loss: [0.38729310035705566, 0.78125] | CTGAN Loss: 1.9784824848175049\n",
      "Epoch: 848 | Discriminator Real Loss: [0.6156588196754456, 0.625] | Discriminator Fake Loss: [0.4193665385246277, 0.84375] | CTGAN Loss: 1.5287830829620361\n",
      "Epoch: 849 | Discriminator Real Loss: [0.6207998394966125, 0.71875] | Discriminator Fake Loss: [0.4883217215538025, 0.71875] | CTGAN Loss: 1.151907205581665\n",
      "Epoch: 850 | Discriminator Real Loss: [0.4707103967666626, 0.8125] | Discriminator Fake Loss: [0.6814509630203247, 0.46875] | CTGAN Loss: 1.4570682048797607\n",
      "Epoch: 851 | Discriminator Real Loss: [0.330068975687027, 0.9375] | Discriminator Fake Loss: [0.914593517780304, 0.46875] | CTGAN Loss: 1.1743841171264648\n",
      "Epoch: 852 | Discriminator Real Loss: [0.35510489344596863, 0.90625] | Discriminator Fake Loss: [0.6296263933181763, 0.5625] | CTGAN Loss: 1.721157431602478\n",
      "Epoch: 853 | Discriminator Real Loss: [0.4941248893737793, 0.875] | Discriminator Fake Loss: [0.3684096336364746, 0.8125] | CTGAN Loss: 2.3731236457824707\n",
      "Epoch: 854 | Discriminator Real Loss: [0.7536564469337463, 0.65625] | Discriminator Fake Loss: [0.3242034912109375, 0.78125] | CTGAN Loss: 2.241915225982666\n",
      "Epoch: 855 | Discriminator Real Loss: [0.6283884048461914, 0.75] | Discriminator Fake Loss: [0.468889057636261, 0.6875] | CTGAN Loss: 1.677668809890747\n",
      "Epoch: 856 | Discriminator Real Loss: [0.4873507618904114, 0.8125] | Discriminator Fake Loss: [0.5055608153343201, 0.71875] | CTGAN Loss: 1.1120190620422363\n",
      "Epoch: 857 | Discriminator Real Loss: [0.3923569917678833, 0.875] | Discriminator Fake Loss: [0.6201857924461365, 0.5625] | CTGAN Loss: 1.81239652633667\n",
      "Epoch: 858 | Discriminator Real Loss: [0.3782220482826233, 0.84375] | Discriminator Fake Loss: [0.451545387506485, 0.71875] | CTGAN Loss: 1.6157195568084717\n",
      "Epoch: 859 | Discriminator Real Loss: [0.3489675223827362, 0.9375] | Discriminator Fake Loss: [0.40878695249557495, 0.84375] | CTGAN Loss: 1.9669994115829468\n",
      "Epoch: 860 | Discriminator Real Loss: [0.5828428268432617, 0.6875] | Discriminator Fake Loss: [0.2755904197692871, 0.84375] | CTGAN Loss: 2.3942477703094482\n",
      "Epoch: 861 | Discriminator Real Loss: [0.5938020944595337, 0.75] | Discriminator Fake Loss: [0.6146706342697144, 0.53125] | CTGAN Loss: 1.2699248790740967\n",
      "Epoch: 862 | Discriminator Real Loss: [0.4141196608543396, 0.875] | Discriminator Fake Loss: [0.5701059103012085, 0.59375] | CTGAN Loss: 1.511345386505127\n",
      "Epoch: 863 | Discriminator Real Loss: [0.48021432757377625, 0.875] | Discriminator Fake Loss: [0.4980207085609436, 0.71875] | CTGAN Loss: 1.6213514804840088\n",
      "Epoch: 864 | Discriminator Real Loss: [0.4462180435657501, 0.90625] | Discriminator Fake Loss: [0.4686644673347473, 0.8125] | CTGAN Loss: 2.414600372314453\n",
      "Epoch: 865 | Discriminator Real Loss: [0.6341711282730103, 0.6875] | Discriminator Fake Loss: [0.3617078363895416, 0.875] | CTGAN Loss: 2.2520041465759277\n",
      "Epoch: 866 | Discriminator Real Loss: [0.6346777677536011, 0.71875] | Discriminator Fake Loss: [0.7147531509399414, 0.53125] | CTGAN Loss: 1.3600082397460938\n",
      "Epoch: 867 | Discriminator Real Loss: [0.4480021595954895, 0.90625] | Discriminator Fake Loss: [0.7574516534805298, 0.53125] | CTGAN Loss: 1.6201143264770508\n",
      "Epoch: 868 | Discriminator Real Loss: [0.497884064912796, 0.875] | Discriminator Fake Loss: [0.5943540334701538, 0.59375] | CTGAN Loss: 2.0612263679504395\n",
      "Epoch: 869 | Discriminator Real Loss: [0.7459317445755005, 0.5625] | Discriminator Fake Loss: [0.819985032081604, 0.4375] | CTGAN Loss: 1.3142139911651611\n",
      "Epoch: 870 | Discriminator Real Loss: [0.571070671081543, 0.78125] | Discriminator Fake Loss: [0.7091978192329407, 0.46875] | CTGAN Loss: 1.1632256507873535\n",
      "Epoch: 871 | Discriminator Real Loss: [0.5193240642547607, 0.75] | Discriminator Fake Loss: [0.7166706323623657, 0.5] | CTGAN Loss: 1.3627426624298096\n",
      "Epoch: 872 | Discriminator Real Loss: [0.4956875443458557, 0.75] | Discriminator Fake Loss: [0.6665245294570923, 0.46875] | CTGAN Loss: 1.405510663986206\n",
      "Epoch: 873 | Discriminator Real Loss: [0.5407687425613403, 0.75] | Discriminator Fake Loss: [0.6027663350105286, 0.5625] | CTGAN Loss: 1.7964091300964355\n",
      "Epoch: 874 | Discriminator Real Loss: [0.824840247631073, 0.46875] | Discriminator Fake Loss: [0.5684210062026978, 0.65625] | CTGAN Loss: 1.3416634798049927\n",
      "Epoch: 875 | Discriminator Real Loss: [0.47721484303474426, 0.875] | Discriminator Fake Loss: [0.5844993591308594, 0.53125] | CTGAN Loss: 1.239027500152588\n",
      "Epoch: 876 | Discriminator Real Loss: [0.3639349341392517, 0.96875] | Discriminator Fake Loss: [0.7499605417251587, 0.5] | CTGAN Loss: 1.6063636541366577\n",
      "Epoch: 877 | Discriminator Real Loss: [0.5034959316253662, 0.71875] | Discriminator Fake Loss: [0.6377637982368469, 0.5] | CTGAN Loss: 1.2513622045516968\n",
      "Epoch: 878 | Discriminator Real Loss: [0.5258041024208069, 0.8125] | Discriminator Fake Loss: [0.763035774230957, 0.46875] | CTGAN Loss: 1.931837558746338\n",
      "Epoch: 879 | Discriminator Real Loss: [0.7970139980316162, 0.59375] | Discriminator Fake Loss: [0.41033127903938293, 0.75] | CTGAN Loss: 1.7379786968231201\n",
      "Epoch: 880 | Discriminator Real Loss: [0.5797885656356812, 0.8125] | Discriminator Fake Loss: [0.5849360227584839, 0.5625] | CTGAN Loss: 1.9723635911941528\n",
      "Epoch: 881 | Discriminator Real Loss: [0.44431573152542114, 0.875] | Discriminator Fake Loss: [0.611923336982727, 0.625] | CTGAN Loss: 1.6614904403686523\n",
      "Epoch: 882 | Discriminator Real Loss: [0.4983310401439667, 0.84375] | Discriminator Fake Loss: [0.5645380020141602, 0.59375] | CTGAN Loss: 2.3895273208618164\n",
      "Epoch: 883 | Discriminator Real Loss: [0.6186860799789429, 0.75] | Discriminator Fake Loss: [0.5587604641914368, 0.65625] | CTGAN Loss: 1.6823090314865112\n",
      "Epoch: 884 | Discriminator Real Loss: [0.4082728624343872, 0.90625] | Discriminator Fake Loss: [0.4531669616699219, 0.625] | CTGAN Loss: 1.6742100715637207\n",
      "Epoch: 885 | Discriminator Real Loss: [0.47749659419059753, 0.84375] | Discriminator Fake Loss: [0.44890114665031433, 0.78125] | CTGAN Loss: 1.850799322128296\n",
      "Epoch: 886 | Discriminator Real Loss: [0.6225501894950867, 0.625] | Discriminator Fake Loss: [0.46174508333206177, 0.71875] | CTGAN Loss: 1.932112693786621\n",
      "Epoch: 887 | Discriminator Real Loss: [0.6529352068901062, 0.65625] | Discriminator Fake Loss: [0.34753066301345825, 0.84375] | CTGAN Loss: 1.876900315284729\n",
      "Epoch: 888 | Discriminator Real Loss: [0.47933483123779297, 0.8125] | Discriminator Fake Loss: [0.5142319202423096, 0.6875] | CTGAN Loss: 1.4664011001586914\n",
      "Epoch: 889 | Discriminator Real Loss: [0.3772081732749939, 0.90625] | Discriminator Fake Loss: [0.705654501914978, 0.53125] | CTGAN Loss: 1.8497980833053589\n",
      "Epoch: 890 | Discriminator Real Loss: [0.47940289974212646, 0.875] | Discriminator Fake Loss: [0.37616005539894104, 0.90625] | CTGAN Loss: 2.1782898902893066\n",
      "Epoch: 891 | Discriminator Real Loss: [0.6596434116363525, 0.71875] | Discriminator Fake Loss: [0.30409806966781616, 0.875] | CTGAN Loss: 2.057004451751709\n",
      "Epoch: 892 | Discriminator Real Loss: [0.6506749391555786, 0.65625] | Discriminator Fake Loss: [0.5254095196723938, 0.65625] | CTGAN Loss: 1.4181879758834839\n",
      "Epoch: 893 | Discriminator Real Loss: [0.46551448106765747, 0.875] | Discriminator Fake Loss: [0.5130591988563538, 0.625] | CTGAN Loss: 1.5242424011230469\n",
      "Epoch: 894 | Discriminator Real Loss: [0.37958216667175293, 1.0] | Discriminator Fake Loss: [0.5568112134933472, 0.71875] | CTGAN Loss: 2.2808821201324463\n",
      "Epoch: 895 | Discriminator Real Loss: [0.8038406372070312, 0.59375] | Discriminator Fake Loss: [0.7427901029586792, 0.46875] | CTGAN Loss: 1.2736055850982666\n",
      "Epoch: 896 | Discriminator Real Loss: [0.47800981998443604, 0.84375] | Discriminator Fake Loss: [0.7501241564750671, 0.4375] | CTGAN Loss: 1.0910017490386963\n",
      "Epoch: 897 | Discriminator Real Loss: [0.6498234272003174, 0.59375] | Discriminator Fake Loss: [0.7464330196380615, 0.46875] | CTGAN Loss: 1.1411933898925781\n",
      "Epoch: 898 | Discriminator Real Loss: [0.5003838539123535, 0.84375] | Discriminator Fake Loss: [0.7510318756103516, 0.5] | CTGAN Loss: 1.5944472551345825\n",
      "Epoch: 899 | Discriminator Real Loss: [1.0303664207458496, 0.53125] | Discriminator Fake Loss: [0.8654712438583374, 0.40625] | CTGAN Loss: 0.7813812494277954\n",
      "Epoch: 900 | Discriminator Real Loss: [0.4824519157409668, 0.84375] | Discriminator Fake Loss: [1.1286628246307373, 0.15625] | CTGAN Loss: 1.4229358434677124\n",
      "Epoch: 901 | Discriminator Real Loss: [0.4195484519004822, 0.90625] | Discriminator Fake Loss: [0.5832098722457886, 0.65625] | CTGAN Loss: 2.4304699897766113\n",
      "Epoch: 902 | Discriminator Real Loss: [1.119236707687378, 0.40625] | Discriminator Fake Loss: [0.7152037620544434, 0.4375] | CTGAN Loss: 0.7495807409286499\n",
      "Epoch: 903 | Discriminator Real Loss: [0.2961755394935608, 1.0] | Discriminator Fake Loss: [0.8828932046890259, 0.34375] | CTGAN Loss: 0.9362514019012451\n",
      "Epoch: 904 | Discriminator Real Loss: [0.35832884907722473, 0.90625] | Discriminator Fake Loss: [0.7715758085250854, 0.5] | CTGAN Loss: 1.2834821939468384\n",
      "Epoch: 905 | Discriminator Real Loss: [0.4289798438549042, 0.90625] | Discriminator Fake Loss: [0.5288888216018677, 0.6875] | CTGAN Loss: 1.5873782634735107\n",
      "Epoch: 906 | Discriminator Real Loss: [0.47369885444641113, 0.90625] | Discriminator Fake Loss: [0.42394697666168213, 0.8125] | CTGAN Loss: 1.914783000946045\n",
      "Epoch: 907 | Discriminator Real Loss: [0.6590859889984131, 0.75] | Discriminator Fake Loss: [0.32630854845046997, 0.84375] | CTGAN Loss: 2.175933599472046\n",
      "Epoch: 908 | Discriminator Real Loss: [0.8883200883865356, 0.34375] | Discriminator Fake Loss: [0.5755451917648315, 0.53125] | CTGAN Loss: 1.2097046375274658\n",
      "Epoch: 909 | Discriminator Real Loss: [0.48346245288848877, 0.875] | Discriminator Fake Loss: [0.5546172857284546, 0.5625] | CTGAN Loss: 1.108794927597046\n",
      "Epoch: 910 | Discriminator Real Loss: [0.4469609260559082, 0.84375] | Discriminator Fake Loss: [0.5769987106323242, 0.53125] | CTGAN Loss: 1.7531230449676514\n",
      "Epoch: 911 | Discriminator Real Loss: [0.44185900688171387, 0.8125] | Discriminator Fake Loss: [0.49437636137008667, 0.65625] | CTGAN Loss: 1.1527972221374512\n",
      "Epoch: 912 | Discriminator Real Loss: [0.39574843645095825, 0.9375] | Discriminator Fake Loss: [0.4120934009552002, 0.90625] | CTGAN Loss: 1.797263503074646\n",
      "Epoch: 913 | Discriminator Real Loss: [0.4488147497177124, 0.9375] | Discriminator Fake Loss: [0.4121982157230377, 0.78125] | CTGAN Loss: 2.1981184482574463\n",
      "Epoch: 914 | Discriminator Real Loss: [0.6228156685829163, 0.6875] | Discriminator Fake Loss: [0.2560293674468994, 0.90625] | CTGAN Loss: 1.8521579504013062\n",
      "Epoch: 915 | Discriminator Real Loss: [0.6476673483848572, 0.71875] | Discriminator Fake Loss: [0.4808918237686157, 0.71875] | CTGAN Loss: 1.3800276517868042\n",
      "Epoch: 916 | Discriminator Real Loss: [0.4054484963417053, 0.9375] | Discriminator Fake Loss: [0.6004908680915833, 0.59375] | CTGAN Loss: 1.192122220993042\n",
      "Epoch: 917 | Discriminator Real Loss: [0.4216250479221344, 0.9375] | Discriminator Fake Loss: [0.4929385185241699, 0.6875] | CTGAN Loss: 1.8160585165023804\n",
      "Epoch: 918 | Discriminator Real Loss: [0.573104202747345, 0.8125] | Discriminator Fake Loss: [0.5116438269615173, 0.6875] | CTGAN Loss: 1.7541685104370117\n",
      "Epoch: 919 | Discriminator Real Loss: [0.7633944153785706, 0.5] | Discriminator Fake Loss: [0.5598788857460022, 0.65625] | CTGAN Loss: 1.44315505027771\n",
      "Epoch: 920 | Discriminator Real Loss: [0.6244736313819885, 0.6875] | Discriminator Fake Loss: [0.6623197197914124, 0.59375] | CTGAN Loss: 1.086493968963623\n",
      "Epoch: 921 | Discriminator Real Loss: [0.4364955723285675, 0.875] | Discriminator Fake Loss: [0.7724789381027222, 0.375] | CTGAN Loss: 1.3560590744018555\n",
      "Epoch: 922 | Discriminator Real Loss: [0.4064503312110901, 0.8125] | Discriminator Fake Loss: [0.706465482711792, 0.625] | CTGAN Loss: 1.568874478340149\n",
      "Epoch: 923 | Discriminator Real Loss: [0.717684268951416, 0.53125] | Discriminator Fake Loss: [0.6537306308746338, 0.65625] | CTGAN Loss: 1.782084584236145\n",
      "Epoch: 924 | Discriminator Real Loss: [0.6330112218856812, 0.65625] | Discriminator Fake Loss: [0.5862604975700378, 0.59375] | CTGAN Loss: 1.4058278799057007\n",
      "Epoch: 925 | Discriminator Real Loss: [0.6424622535705566, 0.6875] | Discriminator Fake Loss: [0.8137024641036987, 0.4375] | CTGAN Loss: 1.4156837463378906\n",
      "Epoch: 926 | Discriminator Real Loss: [0.6591135263442993, 0.6875] | Discriminator Fake Loss: [0.7040747404098511, 0.40625] | CTGAN Loss: 1.3484103679656982\n",
      "Epoch: 927 | Discriminator Real Loss: [0.4674556851387024, 0.84375] | Discriminator Fake Loss: [1.0563616752624512, 0.3125] | CTGAN Loss: 0.9457027912139893\n",
      "Epoch: 928 | Discriminator Real Loss: [0.36151742935180664, 1.0] | Discriminator Fake Loss: [0.8076611161231995, 0.40625] | CTGAN Loss: 1.6354954242706299\n",
      "Epoch: 929 | Discriminator Real Loss: [0.566286563873291, 0.71875] | Discriminator Fake Loss: [0.43342944979667664, 0.75] | CTGAN Loss: 1.9860951900482178\n",
      "Epoch: 930 | Discriminator Real Loss: [1.0903475284576416, 0.40625] | Discriminator Fake Loss: [0.6153001189231873, 0.59375] | CTGAN Loss: 0.8880721926689148\n",
      "Epoch: 931 | Discriminator Real Loss: [0.4373564124107361, 0.90625] | Discriminator Fake Loss: [0.7305630445480347, 0.46875] | CTGAN Loss: 1.0635089874267578\n",
      "Epoch: 932 | Discriminator Real Loss: [0.42360857129096985, 0.96875] | Discriminator Fake Loss: [0.6636446714401245, 0.53125] | CTGAN Loss: 1.2964547872543335\n",
      "Epoch: 933 | Discriminator Real Loss: [0.5215529799461365, 0.84375] | Discriminator Fake Loss: [0.4722793698310852, 0.71875] | CTGAN Loss: 1.5904357433319092\n",
      "Epoch: 934 | Discriminator Real Loss: [0.5405944585800171, 0.8125] | Discriminator Fake Loss: [0.3131451904773712, 0.875] | CTGAN Loss: 1.9095817804336548\n",
      "Epoch: 935 | Discriminator Real Loss: [0.5778467059135437, 0.78125] | Discriminator Fake Loss: [0.476961612701416, 0.6875] | CTGAN Loss: 1.7159333229064941\n",
      "Epoch: 936 | Discriminator Real Loss: [0.7778441905975342, 0.65625] | Discriminator Fake Loss: [0.6124283075332642, 0.5625] | CTGAN Loss: 1.132986068725586\n",
      "Epoch: 937 | Discriminator Real Loss: [0.3687221109867096, 0.9375] | Discriminator Fake Loss: [0.6141826510429382, 0.53125] | CTGAN Loss: 1.1755146980285645\n",
      "Epoch: 938 | Discriminator Real Loss: [0.39241862297058105, 0.96875] | Discriminator Fake Loss: [0.5020588636398315, 0.65625] | CTGAN Loss: 2.552220106124878\n",
      "Epoch: 939 | Discriminator Real Loss: [0.7322095036506653, 0.53125] | Discriminator Fake Loss: [0.39867568016052246, 0.8125] | CTGAN Loss: 1.9993600845336914\n",
      "Epoch: 940 | Discriminator Real Loss: [0.5929150581359863, 0.71875] | Discriminator Fake Loss: [0.41008931398391724, 0.875] | CTGAN Loss: 1.3887546062469482\n",
      "Epoch: 941 | Discriminator Real Loss: [0.5671814680099487, 0.6875] | Discriminator Fake Loss: [0.3774525225162506, 0.90625] | CTGAN Loss: 1.4133639335632324\n",
      "Epoch: 942 | Discriminator Real Loss: [0.48293161392211914, 0.8125] | Discriminator Fake Loss: [0.3767799735069275, 0.875] | CTGAN Loss: 1.8516762256622314\n",
      "Epoch: 943 | Discriminator Real Loss: [0.3613852262496948, 0.96875] | Discriminator Fake Loss: [0.4933614432811737, 0.75] | CTGAN Loss: 1.5588170289993286\n",
      "Epoch: 944 | Discriminator Real Loss: [0.47383740544319153, 0.875] | Discriminator Fake Loss: [0.5924012660980225, 0.65625] | CTGAN Loss: 1.5527479648590088\n",
      "Epoch: 945 | Discriminator Real Loss: [0.5703722238540649, 0.6875] | Discriminator Fake Loss: [0.5264805555343628, 0.71875] | CTGAN Loss: 1.6858928203582764\n",
      "Epoch: 946 | Discriminator Real Loss: [0.5221269130706787, 0.75] | Discriminator Fake Loss: [0.35196447372436523, 0.875] | CTGAN Loss: 1.8739192485809326\n",
      "Epoch: 947 | Discriminator Real Loss: [0.6801207065582275, 0.625] | Discriminator Fake Loss: [0.5505919456481934, 0.65625] | CTGAN Loss: 1.451987862586975\n",
      "Epoch: 948 | Discriminator Real Loss: [0.390636146068573, 0.96875] | Discriminator Fake Loss: [0.856353223323822, 0.40625] | CTGAN Loss: 1.066396951675415\n",
      "Epoch: 949 | Discriminator Real Loss: [0.429176926612854, 0.84375] | Discriminator Fake Loss: [0.8650557994842529, 0.40625] | CTGAN Loss: 1.4904532432556152\n",
      "Epoch: 950 | Discriminator Real Loss: [0.8886631727218628, 0.46875] | Discriminator Fake Loss: [0.5156877636909485, 0.84375] | CTGAN Loss: 1.41340970993042\n",
      "Epoch: 951 | Discriminator Real Loss: [0.5480289459228516, 0.71875] | Discriminator Fake Loss: [0.7587487697601318, 0.5] | CTGAN Loss: 1.122053623199463\n",
      "Epoch: 952 | Discriminator Real Loss: [0.5080177783966064, 0.75] | Discriminator Fake Loss: [0.8626160025596619, 0.46875] | CTGAN Loss: 1.0730606317520142\n",
      "Epoch: 953 | Discriminator Real Loss: [0.44422298669815063, 0.8125] | Discriminator Fake Loss: [0.7050988078117371, 0.46875] | CTGAN Loss: 1.5731536149978638\n",
      "Epoch: 954 | Discriminator Real Loss: [0.8336261510848999, 0.53125] | Discriminator Fake Loss: [0.46519264578819275, 0.78125] | CTGAN Loss: 1.4859594106674194\n",
      "Epoch: 955 | Discriminator Real Loss: [0.595970869064331, 0.78125] | Discriminator Fake Loss: [0.8321962952613831, 0.375] | CTGAN Loss: 1.2905809879302979\n",
      "Epoch: 956 | Discriminator Real Loss: [0.5461114645004272, 0.78125] | Discriminator Fake Loss: [0.5175992250442505, 0.71875] | CTGAN Loss: 1.2868688106536865\n",
      "Epoch: 957 | Discriminator Real Loss: [0.5772634744644165, 0.8125] | Discriminator Fake Loss: [0.7573070526123047, 0.53125] | CTGAN Loss: 1.1026723384857178\n",
      "Epoch: 958 | Discriminator Real Loss: [0.40541785955429077, 0.9375] | Discriminator Fake Loss: [0.7451978921890259, 0.5] | CTGAN Loss: 1.7941899299621582\n",
      "Epoch: 959 | Discriminator Real Loss: [0.46362069249153137, 0.84375] | Discriminator Fake Loss: [0.4526647627353668, 0.8125] | CTGAN Loss: 2.1866049766540527\n",
      "Epoch: 960 | Discriminator Real Loss: [0.7060732841491699, 0.59375] | Discriminator Fake Loss: [0.3069443702697754, 0.90625] | CTGAN Loss: 2.7648327350616455\n",
      "Epoch: 961 | Discriminator Real Loss: [0.6585015058517456, 0.59375] | Discriminator Fake Loss: [0.441702663898468, 0.71875] | CTGAN Loss: 1.9235715866088867\n",
      "Epoch: 962 | Discriminator Real Loss: [0.6165950298309326, 0.6875] | Discriminator Fake Loss: [0.6665723323822021, 0.5] | CTGAN Loss: 1.7203339338302612\n",
      "Epoch: 963 | Discriminator Real Loss: [0.3097951114177704, 0.96875] | Discriminator Fake Loss: [1.0075005292892456, 0.25] | CTGAN Loss: 1.2219656705856323\n",
      "Epoch: 964 | Discriminator Real Loss: [0.3127914071083069, 0.96875] | Discriminator Fake Loss: [0.550555944442749, 0.65625] | CTGAN Loss: 1.9449800252914429\n",
      "Epoch: 965 | Discriminator Real Loss: [0.6088189482688904, 0.71875] | Discriminator Fake Loss: [0.2593928873538971, 0.90625] | CTGAN Loss: 2.4794042110443115\n",
      "Epoch: 966 | Discriminator Real Loss: [0.747946560382843, 0.53125] | Discriminator Fake Loss: [0.3309866189956665, 0.9375] | CTGAN Loss: 1.9738497734069824\n",
      "Epoch: 967 | Discriminator Real Loss: [0.7535164952278137, 0.65625] | Discriminator Fake Loss: [0.6081920862197876, 0.53125] | CTGAN Loss: 1.5443735122680664\n",
      "Epoch: 968 | Discriminator Real Loss: [0.4203079640865326, 0.9375] | Discriminator Fake Loss: [0.5821181535720825, 0.5625] | CTGAN Loss: 1.8825033903121948\n",
      "Epoch: 969 | Discriminator Real Loss: [0.5959799289703369, 0.71875] | Discriminator Fake Loss: [0.4666294455528259, 0.75] | CTGAN Loss: 1.1310791969299316\n",
      "Epoch: 970 | Discriminator Real Loss: [0.3385751247406006, 0.96875] | Discriminator Fake Loss: [0.3631665110588074, 0.84375] | CTGAN Loss: 1.4265332221984863\n",
      "Epoch: 971 | Discriminator Real Loss: [0.3555988669395447, 0.9375] | Discriminator Fake Loss: [0.33427995443344116, 0.90625] | CTGAN Loss: 2.354701042175293\n",
      "Epoch: 972 | Discriminator Real Loss: [0.6095103025436401, 0.75] | Discriminator Fake Loss: [0.18578335642814636, 0.96875] | CTGAN Loss: 2.7171449661254883\n",
      "Epoch: 973 | Discriminator Real Loss: [0.5225224494934082, 0.84375] | Discriminator Fake Loss: [0.32074135541915894, 0.8125] | CTGAN Loss: 2.184476852416992\n",
      "Epoch: 974 | Discriminator Real Loss: [0.32425057888031006, 0.96875] | Discriminator Fake Loss: [0.5123550891876221, 0.5625] | CTGAN Loss: 1.7614487409591675\n",
      "Epoch: 975 | Discriminator Real Loss: [0.4792662262916565, 0.8125] | Discriminator Fake Loss: [0.28484517335891724, 0.90625] | CTGAN Loss: 1.8543651103973389\n",
      "Epoch: 976 | Discriminator Real Loss: [0.37776505947113037, 0.875] | Discriminator Fake Loss: [0.4435913562774658, 0.78125] | CTGAN Loss: 2.710237979888916\n",
      "Epoch: 977 | Discriminator Real Loss: [0.5283802151679993, 0.75] | Discriminator Fake Loss: [0.4853242039680481, 0.71875] | CTGAN Loss: 3.008864641189575\n",
      "Epoch: 978 | Discriminator Real Loss: [0.6198234558105469, 0.78125] | Discriminator Fake Loss: [0.323935866355896, 0.8125] | CTGAN Loss: 2.6650264263153076\n",
      "Epoch: 979 | Discriminator Real Loss: [0.581162691116333, 0.8125] | Discriminator Fake Loss: [0.5561820268630981, 0.625] | CTGAN Loss: 1.4017127752304077\n",
      "Epoch: 980 | Discriminator Real Loss: [0.28024259209632874, 0.96875] | Discriminator Fake Loss: [0.7619075179100037, 0.46875] | CTGAN Loss: 2.0539321899414062\n",
      "Epoch: 981 | Discriminator Real Loss: [0.5355297327041626, 0.75] | Discriminator Fake Loss: [0.43485626578330994, 0.71875] | CTGAN Loss: 2.6837563514709473\n",
      "Epoch: 982 | Discriminator Real Loss: [0.8584721088409424, 0.5] | Discriminator Fake Loss: [0.6749414801597595, 0.59375] | CTGAN Loss: 1.1991941928863525\n",
      "Epoch: 983 | Discriminator Real Loss: [0.413045734167099, 0.84375] | Discriminator Fake Loss: [1.0425739288330078, 0.25] | CTGAN Loss: 1.4229378700256348\n",
      "Epoch: 984 | Discriminator Real Loss: [0.4058982729911804, 0.84375] | Discriminator Fake Loss: [0.5331312417984009, 0.6875] | CTGAN Loss: 1.3304100036621094\n",
      "Epoch: 985 | Discriminator Real Loss: [0.5424384474754333, 0.71875] | Discriminator Fake Loss: [0.6370300650596619, 0.625] | CTGAN Loss: 1.3163527250289917\n",
      "Epoch: 986 | Discriminator Real Loss: [0.5081335306167603, 0.78125] | Discriminator Fake Loss: [0.8449937105178833, 0.4375] | CTGAN Loss: 1.4958951473236084\n",
      "Epoch: 987 | Discriminator Real Loss: [0.5978695154190063, 0.71875] | Discriminator Fake Loss: [0.8537613749504089, 0.40625] | CTGAN Loss: 1.890199899673462\n",
      "Epoch: 988 | Discriminator Real Loss: [0.5181187391281128, 0.78125] | Discriminator Fake Loss: [0.441026508808136, 0.71875] | CTGAN Loss: 1.8267953395843506\n",
      "Epoch: 989 | Discriminator Real Loss: [0.7599228620529175, 0.625] | Discriminator Fake Loss: [0.9730994701385498, 0.25] | CTGAN Loss: 0.7348089814186096\n",
      "Epoch: 990 | Discriminator Real Loss: [0.3068658113479614, 1.0] | Discriminator Fake Loss: [0.8946040868759155, 0.34375] | CTGAN Loss: 0.8341389894485474\n",
      "Epoch: 991 | Discriminator Real Loss: [0.4410073161125183, 0.84375] | Discriminator Fake Loss: [0.5403746962547302, 0.65625] | CTGAN Loss: 1.1561269760131836\n",
      "Epoch: 992 | Discriminator Real Loss: [0.6506319642066956, 0.625] | Discriminator Fake Loss: [0.5535073280334473, 0.5625] | CTGAN Loss: 2.122188091278076\n",
      "Epoch: 993 | Discriminator Real Loss: [0.7427803874015808, 0.59375] | Discriminator Fake Loss: [0.4501441717147827, 0.75] | CTGAN Loss: 1.9086201190948486\n",
      "Epoch: 994 | Discriminator Real Loss: [0.693572998046875, 0.65625] | Discriminator Fake Loss: [0.5574767589569092, 0.75] | CTGAN Loss: 1.607654333114624\n",
      "Epoch: 995 | Discriminator Real Loss: [0.4977222979068756, 0.96875] | Discriminator Fake Loss: [0.4821873903274536, 0.75] | CTGAN Loss: 2.0116944313049316\n",
      "Epoch: 996 | Discriminator Real Loss: [0.4523793160915375, 0.8125] | Discriminator Fake Loss: [0.35704946517944336, 0.90625] | CTGAN Loss: 1.555349588394165\n",
      "Epoch: 997 | Discriminator Real Loss: [0.5365266799926758, 0.78125] | Discriminator Fake Loss: [0.38616999983787537, 0.75] | CTGAN Loss: 1.5532269477844238\n",
      "Epoch: 998 | Discriminator Real Loss: [0.5627135634422302, 0.71875] | Discriminator Fake Loss: [0.522872269153595, 0.65625] | CTGAN Loss: 1.8028748035430908\n",
      "Epoch: 999 | Discriminator Real Loss: [0.42513370513916016, 0.90625] | Discriminator Fake Loss: [0.4989277124404907, 0.6875] | CTGAN Loss: 1.476197361946106\n",
      "Epoch: 1000 | Discriminator Real Loss: [0.4590859115123749, 0.84375] | Discriminator Fake Loss: [0.495003342628479, 0.59375] | CTGAN Loss: 1.7149553298950195\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "846750e4c937fcdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:52:24.842182Z",
     "start_time": "2024-10-11T03:52:24.834768Z"
    }
   },
   "source": [
    "minority_dataset.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99 entries, 1 to 320\n",
      "Data columns (total 47 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GE_SPOP      99 non-null     float64\n",
      " 1   GE_FOXA1     99 non-null     float64\n",
      " 2   GE_CTNNB1    99 non-null     float64\n",
      " 3   GE_CLPTM1L   99 non-null     float64\n",
      " 4   GE_DPYSL2    99 non-null     float64\n",
      " 5   GE_NEIL1     99 non-null     float64\n",
      " 6   GE_PITPNM2   99 non-null     float64\n",
      " 7   GE_ATM       99 non-null     float64\n",
      " 8   GE_EMG1      99 non-null     float64\n",
      " 9   GE_ETV3      99 non-null     float64\n",
      " 10  GE_BRAF      99 non-null     float64\n",
      " 11  GE_NKX3-1    99 non-null     float64\n",
      " 12  GE_ZMYM3     99 non-null     float64\n",
      " 13  GE_SALL1     99 non-null     float64\n",
      " 14  CNA_SPOP     99 non-null     float64\n",
      " 15  CNA_TP53     99 non-null     float64\n",
      " 16  CNA_FOXA1    99 non-null     float64\n",
      " 17  CNA_CTNNB1   99 non-null     float64\n",
      " 18  CNA_MED12    99 non-null     float64\n",
      " 19  CNA_CLPTM1L  99 non-null     float64\n",
      " 20  CNA_DPYSL2   99 non-null     float64\n",
      " 21  CNA_NEIL1    99 non-null     float64\n",
      " 22  CNA_SLC27A4  99 non-null     float64\n",
      " 23  CNA_PITPNM2  99 non-null     float64\n",
      " 24  CNA_PTEN     99 non-null     float64\n",
      " 25  CNA_ATM      99 non-null     float64\n",
      " 26  CNA_EMG1     99 non-null     float64\n",
      " 27  CNA_ETV3     99 non-null     float64\n",
      " 28  CNA_BRAF     99 non-null     float64\n",
      " 29  CNA_ZMYM3    99 non-null     float64\n",
      " 30  CNA_OR4P4    99 non-null     float64\n",
      " 31  CNA_SALL1    99 non-null     float64\n",
      " 32  DM_SPOP      99 non-null     float64\n",
      " 33  DM_FOXA1     99 non-null     float64\n",
      " 34  DM_CTNNB1    99 non-null     float64\n",
      " 35  DM_CLPTM1L   99 non-null     float64\n",
      " 36  DM_DPYSL2    99 non-null     float64\n",
      " 37  DM_NEIL1     99 non-null     float64\n",
      " 38  DM_SLC27A4   99 non-null     float64\n",
      " 39  DM_PITPNM2   99 non-null     float64\n",
      " 40  DM_PTEN      99 non-null     float64\n",
      " 41  DM_EMG1      99 non-null     float64\n",
      " 42  DM_ETV3      99 non-null     float64\n",
      " 43  DM_BRAF      99 non-null     float64\n",
      " 44  DM_NKX3-1    99 non-null     float64\n",
      " 45  DM_SALL1     99 non-null     float64\n",
      " 46  TUMOR_STAGE  99 non-null     int64  \n",
      "dtypes: float64(46), int64(1)\n",
      "memory usage: 37.1 KB\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "aaad11014eb73f5f",
   "metadata": {},
   "source": [
    "**Generating 46 synthetic samples to make sure both the classes have the same 145 classes**"
   ]
  },
  {
   "cell_type": "code",
   "id": "448ceb4989f94115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:52:32.240980Z",
     "start_time": "2024-10-11T03:52:32.091122Z"
    }
   },
   "source": [
    "#Now generating synthetic samples\n",
    "#Majority class 0 has 145 samples while minority class has 99 samples\n",
    "#46 samples have to be generated\n",
    "tf.random.set_seed(42)\n",
    "num_samples = 46\n",
    "noise_vec = tf.random.normal((num_samples, input_dim))\n",
    "noise_label = tf.ones((num_samples, 1))\n",
    "synth_min_samples = gen_model.predict([noise_vec, noise_label])\n",
    "\n",
    "synth_min_df = pd.DataFrame(synth_min_samples, columns = minority_dataset.columns[:-1])\n",
    "synth_min_df['TUMOR_STAGE'] = 1"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "94f3cc51606e31e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:52:35.109729Z",
     "start_time": "2024-10-11T03:52:35.093919Z"
    }
   },
   "source": [
    "synth_min_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "0   0.460690  0.684624   0.739390    0.476540   0.591816  0.552727   \n",
       "1   0.374181  0.464707   0.614054    0.423952   0.550519  0.604560   \n",
       "2   0.469647  0.538567   0.790043    0.199079   0.758320  0.593072   \n",
       "3   0.385461  0.770507   0.922234    0.094836   0.929005  0.861559   \n",
       "4   0.454604  0.524224   0.591271    0.459676   0.118739  0.695094   \n",
       "5   0.461165  0.642609   0.878522    0.167811   0.802036  0.394491   \n",
       "6   0.433343  0.297257   0.284404    0.761152   0.163943  0.788348   \n",
       "7   0.345826  0.582045   0.705722    0.315569   0.905052  0.599764   \n",
       "8   0.378100  0.715668   0.672090    0.413811   0.844195  0.514598   \n",
       "9   0.399085  0.723457   0.693046    0.244554   0.872415  0.793320   \n",
       "10  0.384733  0.555186   0.384184    0.720935   0.265891  0.615938   \n",
       "11  0.474718  0.510816   0.620176    0.638279   0.514535  0.602120   \n",
       "12  0.486316  0.811996   0.850572    0.220172   0.655921  0.505742   \n",
       "13  0.464429  0.475413   0.596658    0.450177   0.601139  0.651673   \n",
       "14  0.486658  0.665136   0.731052    0.319430   0.843302  0.540671   \n",
       "15  0.551978  0.708325   0.899860    0.118783   0.772751  0.598856   \n",
       "16  0.450509  0.538095   0.754888    0.624377   0.608253  0.500454   \n",
       "17  0.439816  0.413594   0.529973    0.566973   0.398306  0.557828   \n",
       "18  0.429833  0.585416   0.817044    0.073095   0.069125  0.949709   \n",
       "19  0.404278  0.471935   0.792443    0.363652   0.443439  0.611009   \n",
       "20  0.514479  0.817503   0.788559    0.179723   0.815939  0.773392   \n",
       "21  0.502154  0.575670   0.852823    0.219622   0.837109  0.463773   \n",
       "22  0.368174  0.768712   0.648713    0.076093   0.023448  0.974976   \n",
       "23  0.495588  0.523913   0.760348    0.520137   0.715823  0.489492   \n",
       "24  0.512109  0.534025   0.823671    0.243696   0.717707  0.434876   \n",
       "25  0.320378  0.854040   0.481355    0.321050   0.340033  0.913157   \n",
       "26  0.456804  0.604337   0.733523    0.612037   0.210121  0.630272   \n",
       "27  0.261896  0.491014   0.345241    0.718332   0.038942  0.893917   \n",
       "28  0.455460  0.525665   0.585626    0.836033   0.132814  0.574711   \n",
       "29  0.430729  0.386621   0.531353    0.650924   0.473990  0.618755   \n",
       "30  0.324970  0.468917   0.703646    0.527406   0.274303  0.630837   \n",
       "31  0.449588  0.929311   0.799153    0.039983   0.711256  0.946578   \n",
       "32  0.429329  0.532619   0.661298    0.292591   0.510487  0.864963   \n",
       "33  0.292998  0.229326   0.445853    0.566980   0.250699  0.557948   \n",
       "34  0.544604  0.790291   0.525544    0.245648   0.922315  0.785322   \n",
       "35  0.401855  0.798189   0.789203    0.282780   0.764235  0.729952   \n",
       "36  0.518835  0.528381   0.715794    0.630619   0.352675  0.536803   \n",
       "37  0.526311  0.679832   0.871356    0.144650   0.830774  0.543398   \n",
       "38  0.349471  0.630625   0.283675    0.827093   0.111521  0.604882   \n",
       "39  0.376833  0.256653   0.626057    0.520128   0.173903  0.582645   \n",
       "40  0.500704  0.464726   0.795012    0.354798   0.606206  0.563215   \n",
       "41  0.436038  0.556576   0.668738    0.479908   0.620172  0.555460   \n",
       "42  0.352950  0.451149   0.663352    0.721664   0.216964  0.756217   \n",
       "43  0.447594  0.362859   0.630836    0.781601   0.359533  0.545462   \n",
       "44  0.408578  0.436122   0.705799    0.533704   0.375522  0.637117   \n",
       "45  0.425975  0.737890   0.722422    0.449444   0.551213  0.556582   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "0     0.408018  0.652332  0.376687  0.625780  ...  0.117593    0.294243   \n",
       "1     0.319770  0.569562  0.490623  0.398615  ...  0.210419    0.459659   \n",
       "2     0.673829  0.549716  0.293546  0.859207  ...  0.406842    0.656882   \n",
       "3     0.790208  0.565499  0.094645  0.989083  ...  0.575331    0.685529   \n",
       "4     0.149629  0.248984  0.426742  0.300742  ...  0.211090    0.290928   \n",
       "5     0.630863  0.722636  0.288655  0.967985  ...  0.409150    0.589928   \n",
       "6     0.268052  0.143261  0.757622  0.021573  ...  0.315639    0.332105   \n",
       "7     0.820633  0.666323  0.334236  0.950865  ...  0.314011    0.224955   \n",
       "8     0.720178  0.759255  0.290482  0.924001  ...  0.380842    0.247909   \n",
       "9     0.847917  0.598592  0.268624  0.670407  ...  0.419067    0.271843   \n",
       "10    0.263767  0.374939  0.777121  0.112752  ...  0.101729    0.165342   \n",
       "11    0.490925  0.497063  0.672470  0.449807  ...  0.166094    0.244125   \n",
       "12    0.582112  0.708511  0.340147  0.979672  ...  0.359502    0.409915   \n",
       "13    0.585186  0.456867  0.523990  0.245819  ...  0.217515    0.323511   \n",
       "14    0.730784  0.636369  0.368985  0.941631  ...  0.283877    0.365723   \n",
       "15    0.739162  0.611330  0.186612  0.975290  ...  0.455390    0.638160   \n",
       "16    0.634141  0.531213  0.580408  0.800250  ...  0.203775    0.336589   \n",
       "17    0.496060  0.301219  0.625409  0.190914  ...  0.237753    0.457121   \n",
       "18    0.154019  0.263365  0.050473  0.873022  ...  0.504289    0.296012   \n",
       "19    0.388951  0.660879  0.500578  0.894712  ...  0.187571    0.482046   \n",
       "20    0.767025  0.763461  0.112573  0.924485  ...  0.285595    0.234935   \n",
       "21    0.663072  0.714738  0.339421  0.959234  ...  0.245422    0.633520   \n",
       "22    0.067142  0.326323  0.042819  0.791251  ...  0.346020    0.075534   \n",
       "23    0.711623  0.595315  0.460416  0.780360  ...  0.205026    0.440503   \n",
       "24    0.641827  0.637944  0.301145  0.961489  ...  0.261736    0.618253   \n",
       "25    0.460920  0.714118  0.200448  0.607803  ...  0.197465    0.062482   \n",
       "26    0.228790  0.637971  0.646004  0.426368  ...  0.080573    0.197383   \n",
       "27    0.070085  0.250561  0.409815  0.071342  ...  0.180820    0.076302   \n",
       "28    0.132781  0.433357  0.800361  0.201521  ...  0.103577    0.207219   \n",
       "29    0.343396  0.416797  0.718821  0.103052  ...  0.084799    0.461738   \n",
       "30    0.344972  0.642971  0.603822  0.662079  ...  0.237967    0.273048   \n",
       "31    0.690859  0.612409  0.045170  0.983898  ...  0.387977    0.089967   \n",
       "32    0.478513  0.284673  0.470549  0.343945  ...  0.453419    0.413412   \n",
       "33    0.183963  0.309535  0.747881  0.113320  ...  0.264341    0.533253   \n",
       "34    0.838773  0.615450  0.253653  0.839329  ...  0.411056    0.180289   \n",
       "35    0.718375  0.627836  0.321047  0.839706  ...  0.438419    0.242190   \n",
       "36    0.331086  0.524560  0.705351  0.489418  ...  0.225542    0.357931   \n",
       "37    0.725082  0.716657  0.282941  0.970976  ...  0.398059    0.512038   \n",
       "38    0.163139  0.479760  0.753214  0.303413  ...  0.085570    0.048609   \n",
       "39    0.272719  0.339126  0.716205  0.106588  ...  0.272350    0.762249   \n",
       "40    0.585807  0.539245  0.527290  0.828001  ...  0.253012    0.628133   \n",
       "41    0.524000  0.595182  0.456026  0.592294  ...  0.154933    0.307864   \n",
       "42    0.238861  0.437247  0.675182  0.151538  ...  0.171299    0.265586   \n",
       "43    0.269030  0.463115  0.733959  0.305396  ...  0.181312    0.345098   \n",
       "44    0.424346  0.481849  0.519685  0.374492  ...  0.154960    0.421375   \n",
       "45    0.453505  0.781458  0.378992  0.927850  ...  0.162774    0.292674   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "0     0.701992  0.090197  0.406515  0.919727  0.284827   0.471317  0.798466   \n",
       "1     0.622761  0.090101  0.398954  0.927291  0.325629   0.464240  0.804773   \n",
       "2     0.513330  0.093520  0.514274  0.934775  0.212035   0.648801  0.576118   \n",
       "3     0.577186  0.004108  0.485670  0.993709  0.179129   0.737160  0.732671   \n",
       "4     0.826425  0.019312  0.353008  0.982219  0.206141   0.557739  0.987786   \n",
       "5     0.590885  0.083300  0.572138  0.873300  0.145212   0.664743  0.580831   \n",
       "6     0.824769  0.035086  0.187163  0.956501  0.198834   0.255435  0.984872   \n",
       "7     0.450216  0.036746  0.336666  0.879695  0.178789   0.480458  0.538689   \n",
       "8     0.460674  0.134630  0.258503  0.721926  0.158051   0.332169  0.632519   \n",
       "9     0.492176  0.038271  0.241433  0.984042  0.291759   0.378932  0.651315   \n",
       "10    0.760935  0.046036  0.151117  0.879784  0.136454   0.137280  0.964510   \n",
       "11    0.741044  0.030627  0.305383  0.803672  0.286591   0.286613  0.955078   \n",
       "12    0.777767  0.052544  0.475966  0.954022  0.193622   0.355596  0.730216   \n",
       "13    0.562376  0.070633  0.372949  0.946959  0.377855   0.467425  0.723200   \n",
       "14    0.565519  0.057049  0.401023  0.879130  0.196963   0.494014  0.684548   \n",
       "15    0.577534  0.012813  0.675722  0.965755  0.241029   0.812950  0.708666   \n",
       "16    0.643602  0.016922  0.432236  0.500812  0.210581   0.295764  0.947081   \n",
       "17    0.486714  0.089577  0.287550  0.775274  0.174203   0.390605  0.812512   \n",
       "18    0.920647  0.002208  0.579916  0.997165  0.225615   0.730385  0.997019   \n",
       "19    0.669435  0.047638  0.479056  0.805300  0.239930   0.454835  0.943057   \n",
       "20    0.652710  0.025374  0.308285  0.994737  0.414483   0.439197  0.747201   \n",
       "21    0.504590  0.056065  0.642823  0.848102  0.222206   0.742164  0.750448   \n",
       "22    0.952591  0.003913  0.372741  0.999593  0.458231   0.585382  0.998744   \n",
       "23    0.560814  0.042700  0.473077  0.775597  0.261100   0.511730  0.830756   \n",
       "24    0.585732  0.039637  0.582765  0.856241  0.256160   0.616294  0.804231   \n",
       "25    0.905067  0.049217  0.094345  0.999406  0.510990   0.335501  0.969568   \n",
       "26    0.844313  0.067831  0.292241  0.954694  0.130246   0.178046  0.970312   \n",
       "27    0.856974  0.022404  0.136291  0.992828  0.318006   0.274595  0.991077   \n",
       "28    0.899147  0.038740  0.354715  0.911050  0.314432   0.097031  0.984410   \n",
       "29    0.695800  0.137617  0.404012  0.842986  0.345585   0.363602  0.831423   \n",
       "30    0.800517  0.065794  0.485362  0.953484  0.202477   0.374264  0.938138   \n",
       "31    0.927887  0.004436  0.171554  0.999049  0.498846   0.246574  0.940827   \n",
       "32    0.794746  0.004108  0.335459  0.991389  0.339778   0.698164  0.943270   \n",
       "33    0.744264  0.078206  0.556554  0.978814  0.268862   0.547797  0.896207   \n",
       "34    0.632693  0.026158  0.171851  0.992579  0.298586   0.331034  0.619220   \n",
       "35    0.552996  0.062117  0.173088  0.980350  0.222677   0.338119  0.602330   \n",
       "36    0.858353  0.018739  0.471181  0.870840  0.223634   0.208815  0.956764   \n",
       "37    0.643515  0.014252  0.508486  0.944887  0.275870   0.680969  0.740378   \n",
       "38    0.931509  0.016860  0.059518  0.726045  0.082299   0.017049  0.995270   \n",
       "39    0.677946  0.125276  0.518934  0.979807  0.283029   0.644982  0.944900   \n",
       "40    0.717094  0.023664  0.603547  0.789261  0.239219   0.579517  0.916373   \n",
       "41    0.525638  0.081974  0.389045  0.786991  0.184071   0.450420  0.736396   \n",
       "42    0.782373  0.039508  0.370302  0.966484  0.434645   0.318670  0.961122   \n",
       "43    0.806491  0.042121  0.378128  0.405074  0.218823   0.181001  0.972223   \n",
       "44    0.725209  0.035509  0.431200  0.880716  0.181959   0.465335  0.956413   \n",
       "45    0.777801  0.073277  0.281195  0.770490  0.183011   0.227127  0.877602   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "5             1  \n",
       "6             1  \n",
       "7             1  \n",
       "8             1  \n",
       "9             1  \n",
       "10            1  \n",
       "11            1  \n",
       "12            1  \n",
       "13            1  \n",
       "14            1  \n",
       "15            1  \n",
       "16            1  \n",
       "17            1  \n",
       "18            1  \n",
       "19            1  \n",
       "20            1  \n",
       "21            1  \n",
       "22            1  \n",
       "23            1  \n",
       "24            1  \n",
       "25            1  \n",
       "26            1  \n",
       "27            1  \n",
       "28            1  \n",
       "29            1  \n",
       "30            1  \n",
       "31            1  \n",
       "32            1  \n",
       "33            1  \n",
       "34            1  \n",
       "35            1  \n",
       "36            1  \n",
       "37            1  \n",
       "38            1  \n",
       "39            1  \n",
       "40            1  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[46 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.460690</td>\n",
       "      <td>0.684624</td>\n",
       "      <td>0.739390</td>\n",
       "      <td>0.476540</td>\n",
       "      <td>0.591816</td>\n",
       "      <td>0.552727</td>\n",
       "      <td>0.408018</td>\n",
       "      <td>0.652332</td>\n",
       "      <td>0.376687</td>\n",
       "      <td>0.625780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117593</td>\n",
       "      <td>0.294243</td>\n",
       "      <td>0.701992</td>\n",
       "      <td>0.090197</td>\n",
       "      <td>0.406515</td>\n",
       "      <td>0.919727</td>\n",
       "      <td>0.284827</td>\n",
       "      <td>0.471317</td>\n",
       "      <td>0.798466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.374181</td>\n",
       "      <td>0.464707</td>\n",
       "      <td>0.614054</td>\n",
       "      <td>0.423952</td>\n",
       "      <td>0.550519</td>\n",
       "      <td>0.604560</td>\n",
       "      <td>0.319770</td>\n",
       "      <td>0.569562</td>\n",
       "      <td>0.490623</td>\n",
       "      <td>0.398615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210419</td>\n",
       "      <td>0.459659</td>\n",
       "      <td>0.622761</td>\n",
       "      <td>0.090101</td>\n",
       "      <td>0.398954</td>\n",
       "      <td>0.927291</td>\n",
       "      <td>0.325629</td>\n",
       "      <td>0.464240</td>\n",
       "      <td>0.804773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469647</td>\n",
       "      <td>0.538567</td>\n",
       "      <td>0.790043</td>\n",
       "      <td>0.199079</td>\n",
       "      <td>0.758320</td>\n",
       "      <td>0.593072</td>\n",
       "      <td>0.673829</td>\n",
       "      <td>0.549716</td>\n",
       "      <td>0.293546</td>\n",
       "      <td>0.859207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406842</td>\n",
       "      <td>0.656882</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.093520</td>\n",
       "      <td>0.514274</td>\n",
       "      <td>0.934775</td>\n",
       "      <td>0.212035</td>\n",
       "      <td>0.648801</td>\n",
       "      <td>0.576118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.385461</td>\n",
       "      <td>0.770507</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.094836</td>\n",
       "      <td>0.929005</td>\n",
       "      <td>0.861559</td>\n",
       "      <td>0.790208</td>\n",
       "      <td>0.565499</td>\n",
       "      <td>0.094645</td>\n",
       "      <td>0.989083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575331</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.577186</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.485670</td>\n",
       "      <td>0.993709</td>\n",
       "      <td>0.179129</td>\n",
       "      <td>0.737160</td>\n",
       "      <td>0.732671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.454604</td>\n",
       "      <td>0.524224</td>\n",
       "      <td>0.591271</td>\n",
       "      <td>0.459676</td>\n",
       "      <td>0.118739</td>\n",
       "      <td>0.695094</td>\n",
       "      <td>0.149629</td>\n",
       "      <td>0.248984</td>\n",
       "      <td>0.426742</td>\n",
       "      <td>0.300742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211090</td>\n",
       "      <td>0.290928</td>\n",
       "      <td>0.826425</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>0.353008</td>\n",
       "      <td>0.982219</td>\n",
       "      <td>0.206141</td>\n",
       "      <td>0.557739</td>\n",
       "      <td>0.987786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.461165</td>\n",
       "      <td>0.642609</td>\n",
       "      <td>0.878522</td>\n",
       "      <td>0.167811</td>\n",
       "      <td>0.802036</td>\n",
       "      <td>0.394491</td>\n",
       "      <td>0.630863</td>\n",
       "      <td>0.722636</td>\n",
       "      <td>0.288655</td>\n",
       "      <td>0.967985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409150</td>\n",
       "      <td>0.589928</td>\n",
       "      <td>0.590885</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.580831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.433343</td>\n",
       "      <td>0.297257</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.761152</td>\n",
       "      <td>0.163943</td>\n",
       "      <td>0.788348</td>\n",
       "      <td>0.268052</td>\n",
       "      <td>0.143261</td>\n",
       "      <td>0.757622</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315639</td>\n",
       "      <td>0.332105</td>\n",
       "      <td>0.824769</td>\n",
       "      <td>0.035086</td>\n",
       "      <td>0.187163</td>\n",
       "      <td>0.956501</td>\n",
       "      <td>0.198834</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.984872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.345826</td>\n",
       "      <td>0.582045</td>\n",
       "      <td>0.705722</td>\n",
       "      <td>0.315569</td>\n",
       "      <td>0.905052</td>\n",
       "      <td>0.599764</td>\n",
       "      <td>0.820633</td>\n",
       "      <td>0.666323</td>\n",
       "      <td>0.334236</td>\n",
       "      <td>0.950865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314011</td>\n",
       "      <td>0.224955</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>0.036746</td>\n",
       "      <td>0.336666</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.178789</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.538689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.378100</td>\n",
       "      <td>0.715668</td>\n",
       "      <td>0.672090</td>\n",
       "      <td>0.413811</td>\n",
       "      <td>0.844195</td>\n",
       "      <td>0.514598</td>\n",
       "      <td>0.720178</td>\n",
       "      <td>0.759255</td>\n",
       "      <td>0.290482</td>\n",
       "      <td>0.924001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380842</td>\n",
       "      <td>0.247909</td>\n",
       "      <td>0.460674</td>\n",
       "      <td>0.134630</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.721926</td>\n",
       "      <td>0.158051</td>\n",
       "      <td>0.332169</td>\n",
       "      <td>0.632519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.399085</td>\n",
       "      <td>0.723457</td>\n",
       "      <td>0.693046</td>\n",
       "      <td>0.244554</td>\n",
       "      <td>0.872415</td>\n",
       "      <td>0.793320</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.598592</td>\n",
       "      <td>0.268624</td>\n",
       "      <td>0.670407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419067</td>\n",
       "      <td>0.271843</td>\n",
       "      <td>0.492176</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.241433</td>\n",
       "      <td>0.984042</td>\n",
       "      <td>0.291759</td>\n",
       "      <td>0.378932</td>\n",
       "      <td>0.651315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.384733</td>\n",
       "      <td>0.555186</td>\n",
       "      <td>0.384184</td>\n",
       "      <td>0.720935</td>\n",
       "      <td>0.265891</td>\n",
       "      <td>0.615938</td>\n",
       "      <td>0.263767</td>\n",
       "      <td>0.374939</td>\n",
       "      <td>0.777121</td>\n",
       "      <td>0.112752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.165342</td>\n",
       "      <td>0.760935</td>\n",
       "      <td>0.046036</td>\n",
       "      <td>0.151117</td>\n",
       "      <td>0.879784</td>\n",
       "      <td>0.136454</td>\n",
       "      <td>0.137280</td>\n",
       "      <td>0.964510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.474718</td>\n",
       "      <td>0.510816</td>\n",
       "      <td>0.620176</td>\n",
       "      <td>0.638279</td>\n",
       "      <td>0.514535</td>\n",
       "      <td>0.602120</td>\n",
       "      <td>0.490925</td>\n",
       "      <td>0.497063</td>\n",
       "      <td>0.672470</td>\n",
       "      <td>0.449807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166094</td>\n",
       "      <td>0.244125</td>\n",
       "      <td>0.741044</td>\n",
       "      <td>0.030627</td>\n",
       "      <td>0.305383</td>\n",
       "      <td>0.803672</td>\n",
       "      <td>0.286591</td>\n",
       "      <td>0.286613</td>\n",
       "      <td>0.955078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.486316</td>\n",
       "      <td>0.811996</td>\n",
       "      <td>0.850572</td>\n",
       "      <td>0.220172</td>\n",
       "      <td>0.655921</td>\n",
       "      <td>0.505742</td>\n",
       "      <td>0.582112</td>\n",
       "      <td>0.708511</td>\n",
       "      <td>0.340147</td>\n",
       "      <td>0.979672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359502</td>\n",
       "      <td>0.409915</td>\n",
       "      <td>0.777767</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.475966</td>\n",
       "      <td>0.954022</td>\n",
       "      <td>0.193622</td>\n",
       "      <td>0.355596</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.464429</td>\n",
       "      <td>0.475413</td>\n",
       "      <td>0.596658</td>\n",
       "      <td>0.450177</td>\n",
       "      <td>0.601139</td>\n",
       "      <td>0.651673</td>\n",
       "      <td>0.585186</td>\n",
       "      <td>0.456867</td>\n",
       "      <td>0.523990</td>\n",
       "      <td>0.245819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217515</td>\n",
       "      <td>0.323511</td>\n",
       "      <td>0.562376</td>\n",
       "      <td>0.070633</td>\n",
       "      <td>0.372949</td>\n",
       "      <td>0.946959</td>\n",
       "      <td>0.377855</td>\n",
       "      <td>0.467425</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.486658</td>\n",
       "      <td>0.665136</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.319430</td>\n",
       "      <td>0.843302</td>\n",
       "      <td>0.540671</td>\n",
       "      <td>0.730784</td>\n",
       "      <td>0.636369</td>\n",
       "      <td>0.368985</td>\n",
       "      <td>0.941631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283877</td>\n",
       "      <td>0.365723</td>\n",
       "      <td>0.565519</td>\n",
       "      <td>0.057049</td>\n",
       "      <td>0.401023</td>\n",
       "      <td>0.879130</td>\n",
       "      <td>0.196963</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.684548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.551978</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.899860</td>\n",
       "      <td>0.118783</td>\n",
       "      <td>0.772751</td>\n",
       "      <td>0.598856</td>\n",
       "      <td>0.739162</td>\n",
       "      <td>0.611330</td>\n",
       "      <td>0.186612</td>\n",
       "      <td>0.975290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455390</td>\n",
       "      <td>0.638160</td>\n",
       "      <td>0.577534</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.675722</td>\n",
       "      <td>0.965755</td>\n",
       "      <td>0.241029</td>\n",
       "      <td>0.812950</td>\n",
       "      <td>0.708666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.450509</td>\n",
       "      <td>0.538095</td>\n",
       "      <td>0.754888</td>\n",
       "      <td>0.624377</td>\n",
       "      <td>0.608253</td>\n",
       "      <td>0.500454</td>\n",
       "      <td>0.634141</td>\n",
       "      <td>0.531213</td>\n",
       "      <td>0.580408</td>\n",
       "      <td>0.800250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203775</td>\n",
       "      <td>0.336589</td>\n",
       "      <td>0.643602</td>\n",
       "      <td>0.016922</td>\n",
       "      <td>0.432236</td>\n",
       "      <td>0.500812</td>\n",
       "      <td>0.210581</td>\n",
       "      <td>0.295764</td>\n",
       "      <td>0.947081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.439816</td>\n",
       "      <td>0.413594</td>\n",
       "      <td>0.529973</td>\n",
       "      <td>0.566973</td>\n",
       "      <td>0.398306</td>\n",
       "      <td>0.557828</td>\n",
       "      <td>0.496060</td>\n",
       "      <td>0.301219</td>\n",
       "      <td>0.625409</td>\n",
       "      <td>0.190914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237753</td>\n",
       "      <td>0.457121</td>\n",
       "      <td>0.486714</td>\n",
       "      <td>0.089577</td>\n",
       "      <td>0.287550</td>\n",
       "      <td>0.775274</td>\n",
       "      <td>0.174203</td>\n",
       "      <td>0.390605</td>\n",
       "      <td>0.812512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.429833</td>\n",
       "      <td>0.585416</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>0.073095</td>\n",
       "      <td>0.069125</td>\n",
       "      <td>0.949709</td>\n",
       "      <td>0.154019</td>\n",
       "      <td>0.263365</td>\n",
       "      <td>0.050473</td>\n",
       "      <td>0.873022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504289</td>\n",
       "      <td>0.296012</td>\n",
       "      <td>0.920647</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>0.997165</td>\n",
       "      <td>0.225615</td>\n",
       "      <td>0.730385</td>\n",
       "      <td>0.997019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.404278</td>\n",
       "      <td>0.471935</td>\n",
       "      <td>0.792443</td>\n",
       "      <td>0.363652</td>\n",
       "      <td>0.443439</td>\n",
       "      <td>0.611009</td>\n",
       "      <td>0.388951</td>\n",
       "      <td>0.660879</td>\n",
       "      <td>0.500578</td>\n",
       "      <td>0.894712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187571</td>\n",
       "      <td>0.482046</td>\n",
       "      <td>0.669435</td>\n",
       "      <td>0.047638</td>\n",
       "      <td>0.479056</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>0.239930</td>\n",
       "      <td>0.454835</td>\n",
       "      <td>0.943057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.514479</td>\n",
       "      <td>0.817503</td>\n",
       "      <td>0.788559</td>\n",
       "      <td>0.179723</td>\n",
       "      <td>0.815939</td>\n",
       "      <td>0.773392</td>\n",
       "      <td>0.767025</td>\n",
       "      <td>0.763461</td>\n",
       "      <td>0.112573</td>\n",
       "      <td>0.924485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285595</td>\n",
       "      <td>0.234935</td>\n",
       "      <td>0.652710</td>\n",
       "      <td>0.025374</td>\n",
       "      <td>0.308285</td>\n",
       "      <td>0.994737</td>\n",
       "      <td>0.414483</td>\n",
       "      <td>0.439197</td>\n",
       "      <td>0.747201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.502154</td>\n",
       "      <td>0.575670</td>\n",
       "      <td>0.852823</td>\n",
       "      <td>0.219622</td>\n",
       "      <td>0.837109</td>\n",
       "      <td>0.463773</td>\n",
       "      <td>0.663072</td>\n",
       "      <td>0.714738</td>\n",
       "      <td>0.339421</td>\n",
       "      <td>0.959234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245422</td>\n",
       "      <td>0.633520</td>\n",
       "      <td>0.504590</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.642823</td>\n",
       "      <td>0.848102</td>\n",
       "      <td>0.222206</td>\n",
       "      <td>0.742164</td>\n",
       "      <td>0.750448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.368174</td>\n",
       "      <td>0.768712</td>\n",
       "      <td>0.648713</td>\n",
       "      <td>0.076093</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.974976</td>\n",
       "      <td>0.067142</td>\n",
       "      <td>0.326323</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>0.791251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346020</td>\n",
       "      <td>0.075534</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.372741</td>\n",
       "      <td>0.999593</td>\n",
       "      <td>0.458231</td>\n",
       "      <td>0.585382</td>\n",
       "      <td>0.998744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.495588</td>\n",
       "      <td>0.523913</td>\n",
       "      <td>0.760348</td>\n",
       "      <td>0.520137</td>\n",
       "      <td>0.715823</td>\n",
       "      <td>0.489492</td>\n",
       "      <td>0.711623</td>\n",
       "      <td>0.595315</td>\n",
       "      <td>0.460416</td>\n",
       "      <td>0.780360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205026</td>\n",
       "      <td>0.440503</td>\n",
       "      <td>0.560814</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.473077</td>\n",
       "      <td>0.775597</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.511730</td>\n",
       "      <td>0.830756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.512109</td>\n",
       "      <td>0.534025</td>\n",
       "      <td>0.823671</td>\n",
       "      <td>0.243696</td>\n",
       "      <td>0.717707</td>\n",
       "      <td>0.434876</td>\n",
       "      <td>0.641827</td>\n",
       "      <td>0.637944</td>\n",
       "      <td>0.301145</td>\n",
       "      <td>0.961489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261736</td>\n",
       "      <td>0.618253</td>\n",
       "      <td>0.585732</td>\n",
       "      <td>0.039637</td>\n",
       "      <td>0.582765</td>\n",
       "      <td>0.856241</td>\n",
       "      <td>0.256160</td>\n",
       "      <td>0.616294</td>\n",
       "      <td>0.804231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.320378</td>\n",
       "      <td>0.854040</td>\n",
       "      <td>0.481355</td>\n",
       "      <td>0.321050</td>\n",
       "      <td>0.340033</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>0.460920</td>\n",
       "      <td>0.714118</td>\n",
       "      <td>0.200448</td>\n",
       "      <td>0.607803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197465</td>\n",
       "      <td>0.062482</td>\n",
       "      <td>0.905067</td>\n",
       "      <td>0.049217</td>\n",
       "      <td>0.094345</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.510990</td>\n",
       "      <td>0.335501</td>\n",
       "      <td>0.969568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.456804</td>\n",
       "      <td>0.604337</td>\n",
       "      <td>0.733523</td>\n",
       "      <td>0.612037</td>\n",
       "      <td>0.210121</td>\n",
       "      <td>0.630272</td>\n",
       "      <td>0.228790</td>\n",
       "      <td>0.637971</td>\n",
       "      <td>0.646004</td>\n",
       "      <td>0.426368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080573</td>\n",
       "      <td>0.197383</td>\n",
       "      <td>0.844313</td>\n",
       "      <td>0.067831</td>\n",
       "      <td>0.292241</td>\n",
       "      <td>0.954694</td>\n",
       "      <td>0.130246</td>\n",
       "      <td>0.178046</td>\n",
       "      <td>0.970312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.261896</td>\n",
       "      <td>0.491014</td>\n",
       "      <td>0.345241</td>\n",
       "      <td>0.718332</td>\n",
       "      <td>0.038942</td>\n",
       "      <td>0.893917</td>\n",
       "      <td>0.070085</td>\n",
       "      <td>0.250561</td>\n",
       "      <td>0.409815</td>\n",
       "      <td>0.071342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180820</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.856974</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>0.136291</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.318006</td>\n",
       "      <td>0.274595</td>\n",
       "      <td>0.991077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.455460</td>\n",
       "      <td>0.525665</td>\n",
       "      <td>0.585626</td>\n",
       "      <td>0.836033</td>\n",
       "      <td>0.132814</td>\n",
       "      <td>0.574711</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>0.433357</td>\n",
       "      <td>0.800361</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103577</td>\n",
       "      <td>0.207219</td>\n",
       "      <td>0.899147</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>0.354715</td>\n",
       "      <td>0.911050</td>\n",
       "      <td>0.314432</td>\n",
       "      <td>0.097031</td>\n",
       "      <td>0.984410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.430729</td>\n",
       "      <td>0.386621</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.650924</td>\n",
       "      <td>0.473990</td>\n",
       "      <td>0.618755</td>\n",
       "      <td>0.343396</td>\n",
       "      <td>0.416797</td>\n",
       "      <td>0.718821</td>\n",
       "      <td>0.103052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084799</td>\n",
       "      <td>0.461738</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>0.137617</td>\n",
       "      <td>0.404012</td>\n",
       "      <td>0.842986</td>\n",
       "      <td>0.345585</td>\n",
       "      <td>0.363602</td>\n",
       "      <td>0.831423</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.324970</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>0.703646</td>\n",
       "      <td>0.527406</td>\n",
       "      <td>0.274303</td>\n",
       "      <td>0.630837</td>\n",
       "      <td>0.344972</td>\n",
       "      <td>0.642971</td>\n",
       "      <td>0.603822</td>\n",
       "      <td>0.662079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237967</td>\n",
       "      <td>0.273048</td>\n",
       "      <td>0.800517</td>\n",
       "      <td>0.065794</td>\n",
       "      <td>0.485362</td>\n",
       "      <td>0.953484</td>\n",
       "      <td>0.202477</td>\n",
       "      <td>0.374264</td>\n",
       "      <td>0.938138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.449588</td>\n",
       "      <td>0.929311</td>\n",
       "      <td>0.799153</td>\n",
       "      <td>0.039983</td>\n",
       "      <td>0.711256</td>\n",
       "      <td>0.946578</td>\n",
       "      <td>0.690859</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.045170</td>\n",
       "      <td>0.983898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387977</td>\n",
       "      <td>0.089967</td>\n",
       "      <td>0.927887</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.171554</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>0.498846</td>\n",
       "      <td>0.246574</td>\n",
       "      <td>0.940827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.429329</td>\n",
       "      <td>0.532619</td>\n",
       "      <td>0.661298</td>\n",
       "      <td>0.292591</td>\n",
       "      <td>0.510487</td>\n",
       "      <td>0.864963</td>\n",
       "      <td>0.478513</td>\n",
       "      <td>0.284673</td>\n",
       "      <td>0.470549</td>\n",
       "      <td>0.343945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453419</td>\n",
       "      <td>0.413412</td>\n",
       "      <td>0.794746</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.335459</td>\n",
       "      <td>0.991389</td>\n",
       "      <td>0.339778</td>\n",
       "      <td>0.698164</td>\n",
       "      <td>0.943270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.292998</td>\n",
       "      <td>0.229326</td>\n",
       "      <td>0.445853</td>\n",
       "      <td>0.566980</td>\n",
       "      <td>0.250699</td>\n",
       "      <td>0.557948</td>\n",
       "      <td>0.183963</td>\n",
       "      <td>0.309535</td>\n",
       "      <td>0.747881</td>\n",
       "      <td>0.113320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264341</td>\n",
       "      <td>0.533253</td>\n",
       "      <td>0.744264</td>\n",
       "      <td>0.078206</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.978814</td>\n",
       "      <td>0.268862</td>\n",
       "      <td>0.547797</td>\n",
       "      <td>0.896207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.544604</td>\n",
       "      <td>0.790291</td>\n",
       "      <td>0.525544</td>\n",
       "      <td>0.245648</td>\n",
       "      <td>0.922315</td>\n",
       "      <td>0.785322</td>\n",
       "      <td>0.838773</td>\n",
       "      <td>0.615450</td>\n",
       "      <td>0.253653</td>\n",
       "      <td>0.839329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411056</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>0.632693</td>\n",
       "      <td>0.026158</td>\n",
       "      <td>0.171851</td>\n",
       "      <td>0.992579</td>\n",
       "      <td>0.298586</td>\n",
       "      <td>0.331034</td>\n",
       "      <td>0.619220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.401855</td>\n",
       "      <td>0.798189</td>\n",
       "      <td>0.789203</td>\n",
       "      <td>0.282780</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.729952</td>\n",
       "      <td>0.718375</td>\n",
       "      <td>0.627836</td>\n",
       "      <td>0.321047</td>\n",
       "      <td>0.839706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438419</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.552996</td>\n",
       "      <td>0.062117</td>\n",
       "      <td>0.173088</td>\n",
       "      <td>0.980350</td>\n",
       "      <td>0.222677</td>\n",
       "      <td>0.338119</td>\n",
       "      <td>0.602330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.518835</td>\n",
       "      <td>0.528381</td>\n",
       "      <td>0.715794</td>\n",
       "      <td>0.630619</td>\n",
       "      <td>0.352675</td>\n",
       "      <td>0.536803</td>\n",
       "      <td>0.331086</td>\n",
       "      <td>0.524560</td>\n",
       "      <td>0.705351</td>\n",
       "      <td>0.489418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225542</td>\n",
       "      <td>0.357931</td>\n",
       "      <td>0.858353</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.471181</td>\n",
       "      <td>0.870840</td>\n",
       "      <td>0.223634</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.956764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.526311</td>\n",
       "      <td>0.679832</td>\n",
       "      <td>0.871356</td>\n",
       "      <td>0.144650</td>\n",
       "      <td>0.830774</td>\n",
       "      <td>0.543398</td>\n",
       "      <td>0.725082</td>\n",
       "      <td>0.716657</td>\n",
       "      <td>0.282941</td>\n",
       "      <td>0.970976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398059</td>\n",
       "      <td>0.512038</td>\n",
       "      <td>0.643515</td>\n",
       "      <td>0.014252</td>\n",
       "      <td>0.508486</td>\n",
       "      <td>0.944887</td>\n",
       "      <td>0.275870</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>0.740378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.349471</td>\n",
       "      <td>0.630625</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>0.827093</td>\n",
       "      <td>0.111521</td>\n",
       "      <td>0.604882</td>\n",
       "      <td>0.163139</td>\n",
       "      <td>0.479760</td>\n",
       "      <td>0.753214</td>\n",
       "      <td>0.303413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085570</td>\n",
       "      <td>0.048609</td>\n",
       "      <td>0.931509</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>0.726045</td>\n",
       "      <td>0.082299</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.995270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.376833</td>\n",
       "      <td>0.256653</td>\n",
       "      <td>0.626057</td>\n",
       "      <td>0.520128</td>\n",
       "      <td>0.173903</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.272719</td>\n",
       "      <td>0.339126</td>\n",
       "      <td>0.716205</td>\n",
       "      <td>0.106588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272350</td>\n",
       "      <td>0.762249</td>\n",
       "      <td>0.677946</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.518934</td>\n",
       "      <td>0.979807</td>\n",
       "      <td>0.283029</td>\n",
       "      <td>0.644982</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.500704</td>\n",
       "      <td>0.464726</td>\n",
       "      <td>0.795012</td>\n",
       "      <td>0.354798</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.563215</td>\n",
       "      <td>0.585807</td>\n",
       "      <td>0.539245</td>\n",
       "      <td>0.527290</td>\n",
       "      <td>0.828001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.628133</td>\n",
       "      <td>0.717094</td>\n",
       "      <td>0.023664</td>\n",
       "      <td>0.603547</td>\n",
       "      <td>0.789261</td>\n",
       "      <td>0.239219</td>\n",
       "      <td>0.579517</td>\n",
       "      <td>0.916373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.436038</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.668738</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.620172</td>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.595182</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>0.592294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154933</td>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.525638</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.389045</td>\n",
       "      <td>0.786991</td>\n",
       "      <td>0.184071</td>\n",
       "      <td>0.450420</td>\n",
       "      <td>0.736396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.352950</td>\n",
       "      <td>0.451149</td>\n",
       "      <td>0.663352</td>\n",
       "      <td>0.721664</td>\n",
       "      <td>0.216964</td>\n",
       "      <td>0.756217</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>0.675182</td>\n",
       "      <td>0.151538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.265586</td>\n",
       "      <td>0.782373</td>\n",
       "      <td>0.039508</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.966484</td>\n",
       "      <td>0.434645</td>\n",
       "      <td>0.318670</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.447594</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>0.630836</td>\n",
       "      <td>0.781601</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.545462</td>\n",
       "      <td>0.269030</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>0.733959</td>\n",
       "      <td>0.305396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181312</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.806491</td>\n",
       "      <td>0.042121</td>\n",
       "      <td>0.378128</td>\n",
       "      <td>0.405074</td>\n",
       "      <td>0.218823</td>\n",
       "      <td>0.181001</td>\n",
       "      <td>0.972223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.408578</td>\n",
       "      <td>0.436122</td>\n",
       "      <td>0.705799</td>\n",
       "      <td>0.533704</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.424346</td>\n",
       "      <td>0.481849</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.374492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154960</td>\n",
       "      <td>0.421375</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.035509</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.465335</td>\n",
       "      <td>0.956413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.425975</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>0.722422</td>\n",
       "      <td>0.449444</td>\n",
       "      <td>0.551213</td>\n",
       "      <td>0.556582</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.378992</td>\n",
       "      <td>0.927850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162774</td>\n",
       "      <td>0.292674</td>\n",
       "      <td>0.777801</td>\n",
       "      <td>0.073277</td>\n",
       "      <td>0.281195</td>\n",
       "      <td>0.770490</td>\n",
       "      <td>0.183011</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "e2cd7f189af8ea3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:52:37.273843Z",
     "start_time": "2024-10-11T03:52:37.268708Z"
    }
   },
   "source": [
    "synth_min_df['TUMOR_STAGE'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "1    46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7fbd0d70ad5d778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T01:45:11.316715Z",
     "start_time": "2024-10-09T01:45:11.282171Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12981/1279714179.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  minority_dataset['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.430408</td>\n",
       "      <td>0.549413</td>\n",
       "      <td>0.691829</td>\n",
       "      <td>0.422083</td>\n",
       "      <td>0.575401</td>\n",
       "      <td>0.621343</td>\n",
       "      <td>0.511619</td>\n",
       "      <td>0.526449</td>\n",
       "      <td>0.470264</td>\n",
       "      <td>0.640008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>0.365601</td>\n",
       "      <td>0.655664</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>0.398783</td>\n",
       "      <td>0.920510</td>\n",
       "      <td>0.245047</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.779241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.125945</td>\n",
       "      <td>0.172995</td>\n",
       "      <td>0.127049</td>\n",
       "      <td>0.170970</td>\n",
       "      <td>0.145871</td>\n",
       "      <td>0.175946</td>\n",
       "      <td>0.178469</td>\n",
       "      <td>0.163476</td>\n",
       "      <td>0.232612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132837</td>\n",
       "      <td>0.184294</td>\n",
       "      <td>0.139925</td>\n",
       "      <td>0.100675</td>\n",
       "      <td>0.152910</td>\n",
       "      <td>0.039621</td>\n",
       "      <td>0.094361</td>\n",
       "      <td>0.184920</td>\n",
       "      <td>0.126294</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.186487</td>\n",
       "      <td>0.145150</td>\n",
       "      <td>0.104812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295086</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.091439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>0.763054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>0.271312</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.355857</td>\n",
       "      <td>0.491945</td>\n",
       "      <td>0.616651</td>\n",
       "      <td>0.348417</td>\n",
       "      <td>0.465805</td>\n",
       "      <td>0.526727</td>\n",
       "      <td>0.409490</td>\n",
       "      <td>0.417746</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.465799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>0.223645</td>\n",
       "      <td>0.560492</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>0.283923</td>\n",
       "      <td>0.901510</td>\n",
       "      <td>0.185615</td>\n",
       "      <td>0.351461</td>\n",
       "      <td>0.719657</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.447820</td>\n",
       "      <td>0.572546</td>\n",
       "      <td>0.705212</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>0.630431</td>\n",
       "      <td>0.517199</td>\n",
       "      <td>0.553874</td>\n",
       "      <td>0.443815</td>\n",
       "      <td>0.718421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223858</td>\n",
       "      <td>0.359262</td>\n",
       "      <td>0.652253</td>\n",
       "      <td>0.060890</td>\n",
       "      <td>0.405276</td>\n",
       "      <td>0.926367</td>\n",
       "      <td>0.236818</td>\n",
       "      <td>0.493626</td>\n",
       "      <td>0.797874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.505466</td>\n",
       "      <td>0.629884</td>\n",
       "      <td>0.824139</td>\n",
       "      <td>0.504164</td>\n",
       "      <td>0.711534</td>\n",
       "      <td>0.708140</td>\n",
       "      <td>0.623376</td>\n",
       "      <td>0.648361</td>\n",
       "      <td>0.553523</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275514</td>\n",
       "      <td>0.495186</td>\n",
       "      <td>0.759650</td>\n",
       "      <td>0.081639</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.949720</td>\n",
       "      <td>0.281463</td>\n",
       "      <td>0.612733</td>\n",
       "      <td>0.858918</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.760484</td>\n",
       "      <td>0.872697</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.704124</td>\n",
       "      <td>0.907982</td>\n",
       "      <td>0.950996</td>\n",
       "      <td>0.829500</td>\n",
       "      <td>0.843633</td>\n",
       "      <td>0.878978</td>\n",
       "      <td>0.967994</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980276</td>\n",
       "      <td>0.915765</td>\n",
       "      <td>0.882551</td>\n",
       "      <td>0.757773</td>\n",
       "      <td>0.988591</td>\n",
       "      <td>0.541422</td>\n",
       "      <td>0.876869</td>\n",
       "      <td>0.983930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         GE_SPOP   GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2   GE_NEIL1  \\\n",
       "count  99.000000  99.000000  99.000000   99.000000  99.000000  99.000000   \n",
       "mean    0.430408   0.549413   0.691829    0.422083   0.575401   0.621343   \n",
       "std     0.123333   0.125945   0.172995    0.127049   0.170970   0.145871   \n",
       "min     0.035294   0.186487   0.145150    0.104812   0.000000   0.295086   \n",
       "25%     0.355857   0.491945   0.616651    0.348417   0.465805   0.526727   \n",
       "50%     0.447820   0.572546   0.705212    0.414669   0.569090   0.630431   \n",
       "75%     0.505466   0.629884   0.824139    0.504164   0.711534   0.708140   \n",
       "max     0.760484   0.872697   0.990674    0.704124   0.907982   0.950996   \n",
       "\n",
       "       GE_PITPNM2     GE_ATM    GE_EMG1    GE_ETV3  ...   DM_NEIL1  \\\n",
       "count   99.000000  99.000000  99.000000  99.000000  ...  99.000000   \n",
       "mean     0.511619   0.526449   0.470264   0.640008  ...   0.242277   \n",
       "std      0.175946   0.178469   0.163476   0.232612  ...   0.132837   \n",
       "min      0.020762   0.091439   0.000000   0.012763  ...   0.034757   \n",
       "25%      0.409490   0.417746   0.366516   0.465799  ...   0.170191   \n",
       "50%      0.517199   0.553874   0.443815   0.718421  ...   0.223858   \n",
       "75%      0.623376   0.648361   0.553523   0.814305  ...   0.275514   \n",
       "max      0.829500   0.843633   0.878978   0.967994  ...   1.000000   \n",
       "\n",
       "       DM_SLC27A4  DM_PITPNM2    DM_PTEN    DM_EMG1    DM_ETV3    DM_BRAF  \\\n",
       "count   99.000000   99.000000  99.000000  99.000000  99.000000  99.000000   \n",
       "mean     0.365601    0.655664   0.078446   0.398783   0.920510   0.245047   \n",
       "std      0.184294    0.139925   0.100675   0.152910   0.039621   0.094361   \n",
       "min      0.000000    0.254443   0.000000   0.072432   0.763054   0.000000   \n",
       "25%      0.223645    0.560492   0.042507   0.283923   0.901510   0.185615   \n",
       "50%      0.359262    0.652253   0.060890   0.405276   0.926367   0.236818   \n",
       "75%      0.495186    0.759650   0.081639   0.508039   0.949720   0.281463   \n",
       "max      0.980276    0.915765   0.882551   0.757773   0.988591   0.541422   \n",
       "\n",
       "       DM_NKX3-1   DM_SALL1  TUMOR_STAGE  \n",
       "count  99.000000  99.000000         99.0  \n",
       "mean    0.476970   0.779241          1.0  \n",
       "std     0.184920   0.126294          0.0  \n",
       "min     0.011983   0.271312          1.0  \n",
       "25%     0.351461   0.719657          1.0  \n",
       "50%     0.493626   0.797874          1.0  \n",
       "75%     0.612733   0.858918          1.0  \n",
       "max     0.876869   0.983930          1.0  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minority_dataset['TUMOR_STAGE'] = df1_gleason['TUMOR_STAGE']\n",
    "#minority_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "id": "fca47c368629ba76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:57:27.047219Z",
     "start_time": "2024-10-11T03:57:26.980409Z"
    }
   },
   "source": [
    "synth_min_df.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         GE_SPOP   GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2   GE_NEIL1  \\\n",
       "count  46.000000  46.000000  46.000000   46.000000  46.000000  46.000000   \n",
       "mean    0.431299   0.578577   0.674491    0.422380   0.514052   0.645570   \n",
       "std     0.066455   0.159079   0.156118    0.223569   0.276744   0.145823   \n",
       "min     0.261896   0.229326   0.283675    0.039983   0.023448   0.394491   \n",
       "25%     0.384915   0.472805   0.601007    0.243911   0.267994   0.553410   \n",
       "50%     0.437927   0.546876   0.704684    0.436698   0.550866   0.603340   \n",
       "75%     0.473450   0.702400   0.789833    0.600773   0.762756   0.749651   \n",
       "max     0.551978   0.929311   0.922234    0.836033   0.929005   0.974976   \n",
       "\n",
       "       GE_PITPNM2     GE_ATM    GE_EMG1    GE_ETV3  ...   DM_NEIL1  \\\n",
       "count   46.000000  46.000000  46.000000  46.000000  ...  46.000000   \n",
       "mean     0.478780   0.531730   0.445018   0.605028  ...   0.267859   \n",
       "std      0.231203   0.161535   0.218888   0.330141  ...   0.122799   \n",
       "min      0.067142   0.143261   0.042819   0.021573  ...   0.080573   \n",
       "25%      0.269953   0.434330   0.291248   0.303909  ...   0.180943   \n",
       "50%      0.484719   0.567531   0.441384   0.666243  ...   0.241695   \n",
       "75%      0.686602   0.641721   0.640855   0.924364  ...   0.375507   \n",
       "max      0.847917   0.781458   0.800361   0.989083  ...   0.575331   \n",
       "\n",
       "       DM_SLC27A4  DM_PITPNM2    DM_PTEN    DM_EMG1    DM_ETV3    DM_BRAF  \\\n",
       "count   46.000000   46.000000  46.000000  46.000000  46.000000  46.000000   \n",
       "mean     0.357698    0.698321   0.048844   0.380042   0.888534   0.257228   \n",
       "std      0.180971    0.143346   0.034490   0.150131   0.125163   0.095810   \n",
       "min      0.048609    0.450216   0.002208   0.059518   0.405074   0.082299   \n",
       "25%      0.242674    0.577273   0.022719   0.288723   0.844265   0.194457   \n",
       "50%      0.327808    0.698896   0.040879   0.383587   0.931033   0.232417   \n",
       "75%      0.461218    0.804997   0.069932   0.483786   0.980215   0.296879   \n",
       "max      0.762249    0.952591   0.137617   0.675722   0.999593   0.510990   \n",
       "\n",
       "       DM_NKX3-1   DM_SALL1  TUMOR_STAGE  \n",
       "count  46.000000  46.000000         46.0  \n",
       "mean    0.431953   0.839772          1.0  \n",
       "std     0.189725   0.140541          0.0  \n",
       "min     0.017049   0.538689          1.0  \n",
       "25%     0.301490   0.733602          1.0  \n",
       "50%     0.444808   0.886904          1.0  \n",
       "75%     0.574072   0.960033          1.0  \n",
       "max     0.812950   0.998744          1.0  \n",
       "\n",
       "[8 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.431299</td>\n",
       "      <td>0.578577</td>\n",
       "      <td>0.674491</td>\n",
       "      <td>0.422380</td>\n",
       "      <td>0.514052</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.478780</td>\n",
       "      <td>0.531730</td>\n",
       "      <td>0.445018</td>\n",
       "      <td>0.605028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.357698</td>\n",
       "      <td>0.698321</td>\n",
       "      <td>0.048844</td>\n",
       "      <td>0.380042</td>\n",
       "      <td>0.888534</td>\n",
       "      <td>0.257228</td>\n",
       "      <td>0.431953</td>\n",
       "      <td>0.839772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.066455</td>\n",
       "      <td>0.159079</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.223569</td>\n",
       "      <td>0.276744</td>\n",
       "      <td>0.145823</td>\n",
       "      <td>0.231203</td>\n",
       "      <td>0.161535</td>\n",
       "      <td>0.218888</td>\n",
       "      <td>0.330141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122799</td>\n",
       "      <td>0.180971</td>\n",
       "      <td>0.143346</td>\n",
       "      <td>0.034490</td>\n",
       "      <td>0.150131</td>\n",
       "      <td>0.125163</td>\n",
       "      <td>0.095810</td>\n",
       "      <td>0.189725</td>\n",
       "      <td>0.140541</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.261896</td>\n",
       "      <td>0.229326</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>0.039983</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.394491</td>\n",
       "      <td>0.067142</td>\n",
       "      <td>0.143261</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080573</td>\n",
       "      <td>0.048609</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>0.405074</td>\n",
       "      <td>0.082299</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.538689</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384915</td>\n",
       "      <td>0.472805</td>\n",
       "      <td>0.601007</td>\n",
       "      <td>0.243911</td>\n",
       "      <td>0.267994</td>\n",
       "      <td>0.553410</td>\n",
       "      <td>0.269953</td>\n",
       "      <td>0.434330</td>\n",
       "      <td>0.291248</td>\n",
       "      <td>0.303909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180943</td>\n",
       "      <td>0.242674</td>\n",
       "      <td>0.577273</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>0.288723</td>\n",
       "      <td>0.844265</td>\n",
       "      <td>0.194457</td>\n",
       "      <td>0.301490</td>\n",
       "      <td>0.733602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.437927</td>\n",
       "      <td>0.546876</td>\n",
       "      <td>0.704684</td>\n",
       "      <td>0.436698</td>\n",
       "      <td>0.550866</td>\n",
       "      <td>0.603340</td>\n",
       "      <td>0.484719</td>\n",
       "      <td>0.567531</td>\n",
       "      <td>0.441384</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241695</td>\n",
       "      <td>0.327808</td>\n",
       "      <td>0.698896</td>\n",
       "      <td>0.040879</td>\n",
       "      <td>0.383587</td>\n",
       "      <td>0.931033</td>\n",
       "      <td>0.232417</td>\n",
       "      <td>0.444808</td>\n",
       "      <td>0.886904</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.473450</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.789833</td>\n",
       "      <td>0.600773</td>\n",
       "      <td>0.762756</td>\n",
       "      <td>0.749651</td>\n",
       "      <td>0.686602</td>\n",
       "      <td>0.641721</td>\n",
       "      <td>0.640855</td>\n",
       "      <td>0.924364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375507</td>\n",
       "      <td>0.461218</td>\n",
       "      <td>0.804997</td>\n",
       "      <td>0.069932</td>\n",
       "      <td>0.483786</td>\n",
       "      <td>0.980215</td>\n",
       "      <td>0.296879</td>\n",
       "      <td>0.574072</td>\n",
       "      <td>0.960033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.551978</td>\n",
       "      <td>0.929311</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.836033</td>\n",
       "      <td>0.929005</td>\n",
       "      <td>0.974976</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.800361</td>\n",
       "      <td>0.989083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575331</td>\n",
       "      <td>0.762249</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>0.137617</td>\n",
       "      <td>0.675722</td>\n",
       "      <td>0.999593</td>\n",
       "      <td>0.510990</td>\n",
       "      <td>0.812950</td>\n",
       "      <td>0.998744</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "a7be7ed33dd51704",
   "metadata": {},
   "source": [
    "**Using PCA to decompose the dataset into 2 principal components**"
   ]
  },
  {
   "cell_type": "code",
   "id": "492c0b11748884a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:57:42.067913Z",
     "start_time": "2024-10-11T03:57:41.855234Z"
    }
   },
   "source": [
    "#Checking if synthetic samples generated are within the permissible values of the original dataset using Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "synth_values = synth_min_df.values\n",
    "original_min_values = minority_dataset.values\n",
    "#Vertically stacking the synthetic samples with the minority class samples\n",
    "stacked_values = np.vstack([original_min_values, synth_values])\n",
    "pca_values = pca.fit_transform(stacked_values)\n",
    "\n",
    "#Plotting the values on a scatterplot\n",
    "original_pca_values = pca_values[: len(original_min_values)]\n",
    "synth_pca_values = pca_values[len(original_min_values): ]\n",
    "\n",
    "plt.scatter(x = original_pca_values[:, 0], y = original_pca_values[:, 1], alpha = 1, c = 'g', label = 'Original_Dataset')\n",
    "plt.scatter(x = synth_pca_values[:, 0], y = synth_pca_values[:, 1], alpha = 0.5, c = 'b', label = 'Synthetic_Dataset')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "951f8b9d170cabfc",
   "metadata": {},
   "source": [
    "**Concatenating the minority class samples with the synthetic samples to get 145 samples**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5579ea949c82685b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:58:36.393390Z",
     "start_time": "2024-10-11T03:58:36.389780Z"
    }
   },
   "source": [
    "combined_min_df = pd.concat([minority_dataset, synth_min_df], axis = 0)"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "891f22a3f9e09103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:58:37.680883Z",
     "start_time": "2024-10-11T03:58:37.669546Z"
    }
   },
   "source": [
    "combined_min_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L  GE_DPYSL2  GE_NEIL1  \\\n",
       "1   0.373498  0.790415   0.648564    0.367439   0.000000  0.638505   \n",
       "4   0.363634  0.624428   0.892508    0.501788   0.472260  0.465397   \n",
       "9   0.418700  0.702095   0.745375    0.430890   0.487548  0.669876   \n",
       "11  0.470982  0.654804   0.799236    0.414669   0.421094  0.578125   \n",
       "15  0.535495  0.574125   0.990674    0.227795   0.686890  0.486809   \n",
       "..       ...       ...        ...         ...        ...       ...   \n",
       "41  0.436038  0.556576   0.668738    0.479908   0.620172  0.555460   \n",
       "42  0.352950  0.451149   0.663352    0.721664   0.216964  0.756217   \n",
       "43  0.447594  0.362859   0.630836    0.781601   0.359533  0.545462   \n",
       "44  0.408578  0.436122   0.705799    0.533704   0.375522  0.637117   \n",
       "45  0.425975  0.737890   0.722422    0.449444   0.551213  0.556582   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "1     0.156215  0.673907  0.384063  0.807230  ...  0.034757    0.000000   \n",
       "4     0.619784  0.562305  0.450817  0.729161  ...  0.182983    0.119817   \n",
       "9     0.441037  0.506928  0.140151  0.867808  ...  0.181650    0.299419   \n",
       "11    0.466010  0.715588  0.355391  0.813532  ...  0.172506    0.200640   \n",
       "15    0.735071  0.832965  0.387468  0.947819  ...  0.330505    0.531055   \n",
       "..         ...       ...       ...       ...  ...       ...         ...   \n",
       "41    0.524000  0.595182  0.456026  0.592294  ...  0.154933    0.307864   \n",
       "42    0.238861  0.437247  0.675182  0.151538  ...  0.171299    0.265586   \n",
       "43    0.269030  0.463115  0.733959  0.305396  ...  0.181312    0.345098   \n",
       "44    0.424346  0.481849  0.519685  0.374492  ...  0.154960    0.421375   \n",
       "45    0.453505  0.781458  0.378992  0.927850  ...  0.162774    0.292674   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "1     0.791218  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329   \n",
       "4     0.907230  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265   \n",
       "9     0.605663  0.045999  0.422079  0.885389  0.238896   0.548180  0.828444   \n",
       "11    0.782428  0.035299  0.285695  0.933832  0.175760   0.263804  0.776006   \n",
       "15    0.546693  0.048915  0.458723  0.868237  0.251513   0.654329  0.667877   \n",
       "..         ...       ...       ...       ...       ...        ...       ...   \n",
       "41    0.525638  0.081974  0.389045  0.786991  0.184071   0.450420  0.736396   \n",
       "42    0.782373  0.039508  0.370302  0.966484  0.434645   0.318670  0.961122   \n",
       "43    0.806491  0.042121  0.378128  0.405074  0.218823   0.181001  0.972223   \n",
       "44    0.725209  0.035509  0.431200  0.880716  0.181959   0.465335  0.956413   \n",
       "45    0.777801  0.073277  0.281195  0.770490  0.183011   0.227127  0.877602   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "1             1  \n",
       "4             1  \n",
       "9             1  \n",
       "11            1  \n",
       "15            1  \n",
       "..          ...  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[145 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.702095</td>\n",
       "      <td>0.745375</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.669876</td>\n",
       "      <td>0.441037</td>\n",
       "      <td>0.506928</td>\n",
       "      <td>0.140151</td>\n",
       "      <td>0.867808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181650</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.605663</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.422079</td>\n",
       "      <td>0.885389</td>\n",
       "      <td>0.238896</td>\n",
       "      <td>0.548180</td>\n",
       "      <td>0.828444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.470982</td>\n",
       "      <td>0.654804</td>\n",
       "      <td>0.799236</td>\n",
       "      <td>0.414669</td>\n",
       "      <td>0.421094</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.466010</td>\n",
       "      <td>0.715588</td>\n",
       "      <td>0.355391</td>\n",
       "      <td>0.813532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.200640</td>\n",
       "      <td>0.782428</td>\n",
       "      <td>0.035299</td>\n",
       "      <td>0.285695</td>\n",
       "      <td>0.933832</td>\n",
       "      <td>0.175760</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.776006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.535495</td>\n",
       "      <td>0.574125</td>\n",
       "      <td>0.990674</td>\n",
       "      <td>0.227795</td>\n",
       "      <td>0.686890</td>\n",
       "      <td>0.486809</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.387468</td>\n",
       "      <td>0.947819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330505</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.458723</td>\n",
       "      <td>0.868237</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.654329</td>\n",
       "      <td>0.667877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.436038</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.668738</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.620172</td>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.595182</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>0.592294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154933</td>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.525638</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.389045</td>\n",
       "      <td>0.786991</td>\n",
       "      <td>0.184071</td>\n",
       "      <td>0.450420</td>\n",
       "      <td>0.736396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.352950</td>\n",
       "      <td>0.451149</td>\n",
       "      <td>0.663352</td>\n",
       "      <td>0.721664</td>\n",
       "      <td>0.216964</td>\n",
       "      <td>0.756217</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>0.675182</td>\n",
       "      <td>0.151538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.265586</td>\n",
       "      <td>0.782373</td>\n",
       "      <td>0.039508</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.966484</td>\n",
       "      <td>0.434645</td>\n",
       "      <td>0.318670</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.447594</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>0.630836</td>\n",
       "      <td>0.781601</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.545462</td>\n",
       "      <td>0.269030</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>0.733959</td>\n",
       "      <td>0.305396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181312</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.806491</td>\n",
       "      <td>0.042121</td>\n",
       "      <td>0.378128</td>\n",
       "      <td>0.405074</td>\n",
       "      <td>0.218823</td>\n",
       "      <td>0.181001</td>\n",
       "      <td>0.972223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.408578</td>\n",
       "      <td>0.436122</td>\n",
       "      <td>0.705799</td>\n",
       "      <td>0.533704</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.424346</td>\n",
       "      <td>0.481849</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.374492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154960</td>\n",
       "      <td>0.421375</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.035509</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.465335</td>\n",
       "      <td>0.956413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.425975</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>0.722422</td>\n",
       "      <td>0.449444</td>\n",
       "      <td>0.551213</td>\n",
       "      <td>0.556582</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.378992</td>\n",
       "      <td>0.927850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162774</td>\n",
       "      <td>0.292674</td>\n",
       "      <td>0.777801</td>\n",
       "      <td>0.073277</td>\n",
       "      <td>0.281195</td>\n",
       "      <td>0.770490</td>\n",
       "      <td>0.183011</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "145be9902dab4442",
   "metadata": {},
   "source": [
    "**Finally combining the synthetic samples to the gleason dataset**"
   ]
  },
  {
   "cell_type": "code",
   "id": "15ed97d9d3ea44d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:59:02.698251Z",
     "start_time": "2024-10-11T03:59:02.692435Z"
    }
   },
   "source": [
    "processed_gleason_df = pd.concat([df1_gleason, synth_min_df], axis = 0)\n",
    "processed_gleason_df['TUMOR_STAGE'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "1    145\n",
       "0    145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "85ea13d4f2e6260c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:59:07.775420Z",
     "start_time": "2024-10-11T03:59:07.758750Z"
    }
   },
   "source": [
    "processed_gleason_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L            GE_DPYSL2  GE_NEIL1  \\\n",
       "1   0.373498  0.790415   0.648564    0.367439                  0.0  0.638505   \n",
       "3   0.433556  0.637116   0.777338    0.559615  0.47676041700945415  0.623626   \n",
       "4   0.363634  0.624428   0.892508    0.501788   0.4722603494002571  0.465397   \n",
       "5   0.540453  0.630588   0.671140    0.275536   0.6624865041294341  0.476065   \n",
       "6   0.506989  0.590670   0.727922    0.255442    0.733651313733152  0.691024   \n",
       "..       ...       ...        ...         ...                  ...       ...   \n",
       "41  0.436038  0.556576   0.668738    0.479908             0.620172  0.555460   \n",
       "42  0.352950  0.451149   0.663352    0.721664             0.216964  0.756217   \n",
       "43  0.447594  0.362859   0.630836    0.781601             0.359533  0.545462   \n",
       "44  0.408578  0.436122   0.705799    0.533704             0.375522  0.637117   \n",
       "45  0.425975  0.737890   0.722422    0.449444             0.551213  0.556582   \n",
       "\n",
       "    GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "1     0.156215  0.673907  0.384063  0.807230  ...  0.034757    0.000000   \n",
       "3     0.415425  0.539748  0.466929  0.599673  ...  0.111825    0.085465   \n",
       "4     0.619784  0.562305  0.450817  0.729161  ...  0.182983    0.119817   \n",
       "5     0.577890  0.824079  0.297729  0.899286  ...  0.228759    0.230212   \n",
       "6     0.398213  0.644989  0.383090  0.713497  ...  0.129223    0.502728   \n",
       "..         ...       ...       ...       ...  ...       ...         ...   \n",
       "41    0.524000  0.595182  0.456026  0.592294  ...  0.154933    0.307864   \n",
       "42    0.238861  0.437247  0.675182  0.151538  ...  0.171299    0.265586   \n",
       "43    0.269030  0.463115  0.733959  0.305396  ...  0.181312    0.345098   \n",
       "44    0.424346  0.481849  0.519685  0.374492  ...  0.154960    0.421375   \n",
       "45    0.453505  0.781458  0.378992  0.927850  ...  0.162774    0.292674   \n",
       "\n",
       "    DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "1     0.791218  0.045264  0.090997  0.942279  0.257308   0.093755  0.762329   \n",
       "3     0.569465  0.047191  0.101831  0.930270  0.221800   0.257343  1.000000   \n",
       "4     0.907230  0.082071  0.155321  0.901625  0.171257   0.201274  0.695265   \n",
       "5     0.831449  0.040868  0.266358  0.943859  0.178990   0.505571  0.831493   \n",
       "6     0.836422  0.036482  0.328812  0.954992  0.185582   0.391752  0.892649   \n",
       "..         ...       ...       ...       ...       ...        ...       ...   \n",
       "41    0.525638  0.081974  0.389045  0.786991  0.184071   0.450420  0.736396   \n",
       "42    0.782373  0.039508  0.370302  0.966484  0.434645   0.318670  0.961122   \n",
       "43    0.806491  0.042121  0.378128  0.405074  0.218823   0.181001  0.972223   \n",
       "44    0.725209  0.035509  0.431200  0.880716  0.181959   0.465335  0.956413   \n",
       "45    0.777801  0.073277  0.281195  0.770490  0.183011   0.227127  0.877602   \n",
       "\n",
       "    TUMOR_STAGE  \n",
       "1             1  \n",
       "3             0  \n",
       "4             1  \n",
       "5             0  \n",
       "6             0  \n",
       "..          ...  \n",
       "41            1  \n",
       "42            1  \n",
       "43            1  \n",
       "44            1  \n",
       "45            1  \n",
       "\n",
       "[290 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373498</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.648564</td>\n",
       "      <td>0.367439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638505</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0.673907</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.807230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791218</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.257308</td>\n",
       "      <td>0.093755</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.637116</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.559615</td>\n",
       "      <td>0.47676041700945415</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.415425</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>0.599673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111825</td>\n",
       "      <td>0.085465</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.047191</td>\n",
       "      <td>0.101831</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>0.257343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363634</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.892508</td>\n",
       "      <td>0.501788</td>\n",
       "      <td>0.4722603494002571</td>\n",
       "      <td>0.465397</td>\n",
       "      <td>0.619784</td>\n",
       "      <td>0.562305</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>0.729161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182983</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.901625</td>\n",
       "      <td>0.171257</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.695265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540453</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.275536</td>\n",
       "      <td>0.6624865041294341</td>\n",
       "      <td>0.476065</td>\n",
       "      <td>0.577890</td>\n",
       "      <td>0.824079</td>\n",
       "      <td>0.297729</td>\n",
       "      <td>0.899286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228759</td>\n",
       "      <td>0.230212</td>\n",
       "      <td>0.831449</td>\n",
       "      <td>0.040868</td>\n",
       "      <td>0.266358</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.831493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506989</td>\n",
       "      <td>0.590670</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.255442</td>\n",
       "      <td>0.733651313733152</td>\n",
       "      <td>0.691024</td>\n",
       "      <td>0.398213</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.383090</td>\n",
       "      <td>0.713497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129223</td>\n",
       "      <td>0.502728</td>\n",
       "      <td>0.836422</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>0.328812</td>\n",
       "      <td>0.954992</td>\n",
       "      <td>0.185582</td>\n",
       "      <td>0.391752</td>\n",
       "      <td>0.892649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.436038</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.668738</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.620172</td>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.595182</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>0.592294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154933</td>\n",
       "      <td>0.307864</td>\n",
       "      <td>0.525638</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.389045</td>\n",
       "      <td>0.786991</td>\n",
       "      <td>0.184071</td>\n",
       "      <td>0.450420</td>\n",
       "      <td>0.736396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.352950</td>\n",
       "      <td>0.451149</td>\n",
       "      <td>0.663352</td>\n",
       "      <td>0.721664</td>\n",
       "      <td>0.216964</td>\n",
       "      <td>0.756217</td>\n",
       "      <td>0.238861</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>0.675182</td>\n",
       "      <td>0.151538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.265586</td>\n",
       "      <td>0.782373</td>\n",
       "      <td>0.039508</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.966484</td>\n",
       "      <td>0.434645</td>\n",
       "      <td>0.318670</td>\n",
       "      <td>0.961122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.447594</td>\n",
       "      <td>0.362859</td>\n",
       "      <td>0.630836</td>\n",
       "      <td>0.781601</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.545462</td>\n",
       "      <td>0.269030</td>\n",
       "      <td>0.463115</td>\n",
       "      <td>0.733959</td>\n",
       "      <td>0.305396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181312</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.806491</td>\n",
       "      <td>0.042121</td>\n",
       "      <td>0.378128</td>\n",
       "      <td>0.405074</td>\n",
       "      <td>0.218823</td>\n",
       "      <td>0.181001</td>\n",
       "      <td>0.972223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.408578</td>\n",
       "      <td>0.436122</td>\n",
       "      <td>0.705799</td>\n",
       "      <td>0.533704</td>\n",
       "      <td>0.375522</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.424346</td>\n",
       "      <td>0.481849</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.374492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154960</td>\n",
       "      <td>0.421375</td>\n",
       "      <td>0.725209</td>\n",
       "      <td>0.035509</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.465335</td>\n",
       "      <td>0.956413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.425975</td>\n",
       "      <td>0.737890</td>\n",
       "      <td>0.722422</td>\n",
       "      <td>0.449444</td>\n",
       "      <td>0.551213</td>\n",
       "      <td>0.556582</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.378992</td>\n",
       "      <td>0.927850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162774</td>\n",
       "      <td>0.292674</td>\n",
       "      <td>0.777801</td>\n",
       "      <td>0.073277</td>\n",
       "      <td>0.281195</td>\n",
       "      <td>0.770490</td>\n",
       "      <td>0.183011</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "abadd30d68899210",
   "metadata": {},
   "source": [
    "**Shuffling the dataset to stop a neural network from capturing patterns associated with the order of the samples**"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3176da14769317e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:59:47.469021Z",
     "start_time": "2024-10-11T03:59:47.458823Z"
    }
   },
   "source": [
    "processed_gleason_df = processed_gleason_df.sample(frac = 1).reset_index(drop = True)\n",
    "processed_gleason_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      GE_SPOP  GE_FOXA1  GE_CTNNB1  GE_CLPTM1L           GE_DPYSL2  GE_NEIL1  \\\n",
       "0    0.485713  0.602396   0.875180    0.493005  0.5356774057026716  0.579175   \n",
       "1    0.384733  0.555186   0.384184    0.720935            0.265891  0.615938   \n",
       "2    0.501783  0.617020   0.792797    0.390719  0.7326797107663086  0.499783   \n",
       "3    0.610479  0.478407   0.786356    0.235442  0.7963768589855218  0.592039   \n",
       "4    0.470017  0.377998   0.824718    0.393915  0.8224050291903859  0.585371   \n",
       "..        ...       ...        ...         ...                 ...       ...   \n",
       "285  0.346003  0.100142   0.781156    0.439434  0.9503121996878838  0.555374   \n",
       "286  0.427905  0.624984   0.860459    0.453867  0.6887381290727799  0.457943   \n",
       "287  0.461165  0.642609   0.878522    0.167811            0.802036  0.394491   \n",
       "288  0.350445  0.535638   0.851672    0.412014  0.5734532769864495  0.593781   \n",
       "289  0.466585  0.671103   0.809819    0.331889  0.6362481406367202  0.625956   \n",
       "\n",
       "     GE_PITPNM2    GE_ATM   GE_EMG1   GE_ETV3  ...  DM_NEIL1  DM_SLC27A4  \\\n",
       "0      0.332847  0.725424  0.320880  0.639029  ...  0.180638    0.278463   \n",
       "1      0.263767  0.374939  0.777121  0.112752  ...  0.101729    0.165342   \n",
       "2      0.711649  0.664329  0.408121  0.878717  ...  0.389931    0.507315   \n",
       "3      0.715180  0.838920  0.173883  0.934750  ...  0.253729    0.535583   \n",
       "4      0.481900  0.611074  0.412000  0.624647  ...  0.240000    0.554207   \n",
       "..          ...       ...       ...       ...  ...       ...         ...   \n",
       "285    0.784326  1.000000  0.315625  0.905615  ...  0.368381    0.750055   \n",
       "286    0.530200  0.688023  0.447066  0.878165  ...  0.213601    0.189264   \n",
       "287    0.630863  0.722636  0.288655  0.967985  ...  0.409150    0.589928   \n",
       "288    0.586341  0.667384  0.419121  0.853813  ...  0.135575    0.320320   \n",
       "289    0.586835  0.742236  0.305013  0.926243  ...  0.186024    0.236396   \n",
       "\n",
       "     DM_PITPNM2   DM_PTEN   DM_EMG1   DM_ETV3   DM_BRAF  DM_NKX3-1  DM_SALL1  \\\n",
       "0      0.658140  0.043097  0.390242  0.911493  0.248904   0.433275  0.805977   \n",
       "1      0.760935  0.046036  0.151117  0.879784  0.136454   0.137280  0.964510   \n",
       "2      0.490987  0.041370  0.462390  0.947289  0.182826   0.587181  0.624428   \n",
       "3      0.334942  0.022867  0.606468  0.897280  0.192746   0.619840  0.745078   \n",
       "4      0.381248  0.076909  0.533006  0.935659  0.319159   0.876869  0.580277   \n",
       "..          ...       ...       ...       ...       ...        ...       ...   \n",
       "285    0.318052  0.023063  0.633076  0.903943  0.442787   0.809851  0.569212   \n",
       "286    0.558862  0.077598  0.320214  0.905402  0.398802   0.521767  0.811772   \n",
       "287    0.590885  0.083300  0.572138  0.873300  0.145212   0.664743  0.580831   \n",
       "288    0.716790  0.016695  0.320311  0.908059  0.170628   0.316081  0.886800   \n",
       "289    0.555513  0.013775  0.266985  0.874898  0.147778   0.338625  0.751966   \n",
       "\n",
       "     TUMOR_STAGE  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "..           ...  \n",
       "285            0  \n",
       "286            0  \n",
       "287            1  \n",
       "288            0  \n",
       "289            0  \n",
       "\n",
       "[290 rows x 47 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GE_SPOP</th>\n",
       "      <th>GE_FOXA1</th>\n",
       "      <th>GE_CTNNB1</th>\n",
       "      <th>GE_CLPTM1L</th>\n",
       "      <th>GE_DPYSL2</th>\n",
       "      <th>GE_NEIL1</th>\n",
       "      <th>GE_PITPNM2</th>\n",
       "      <th>GE_ATM</th>\n",
       "      <th>GE_EMG1</th>\n",
       "      <th>GE_ETV3</th>\n",
       "      <th>...</th>\n",
       "      <th>DM_NEIL1</th>\n",
       "      <th>DM_SLC27A4</th>\n",
       "      <th>DM_PITPNM2</th>\n",
       "      <th>DM_PTEN</th>\n",
       "      <th>DM_EMG1</th>\n",
       "      <th>DM_ETV3</th>\n",
       "      <th>DM_BRAF</th>\n",
       "      <th>DM_NKX3-1</th>\n",
       "      <th>DM_SALL1</th>\n",
       "      <th>TUMOR_STAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485713</td>\n",
       "      <td>0.602396</td>\n",
       "      <td>0.875180</td>\n",
       "      <td>0.493005</td>\n",
       "      <td>0.5356774057026716</td>\n",
       "      <td>0.579175</td>\n",
       "      <td>0.332847</td>\n",
       "      <td>0.725424</td>\n",
       "      <td>0.320880</td>\n",
       "      <td>0.639029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180638</td>\n",
       "      <td>0.278463</td>\n",
       "      <td>0.658140</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>0.390242</td>\n",
       "      <td>0.911493</td>\n",
       "      <td>0.248904</td>\n",
       "      <td>0.433275</td>\n",
       "      <td>0.805977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.384733</td>\n",
       "      <td>0.555186</td>\n",
       "      <td>0.384184</td>\n",
       "      <td>0.720935</td>\n",
       "      <td>0.265891</td>\n",
       "      <td>0.615938</td>\n",
       "      <td>0.263767</td>\n",
       "      <td>0.374939</td>\n",
       "      <td>0.777121</td>\n",
       "      <td>0.112752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.165342</td>\n",
       "      <td>0.760935</td>\n",
       "      <td>0.046036</td>\n",
       "      <td>0.151117</td>\n",
       "      <td>0.879784</td>\n",
       "      <td>0.136454</td>\n",
       "      <td>0.137280</td>\n",
       "      <td>0.964510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501783</td>\n",
       "      <td>0.617020</td>\n",
       "      <td>0.792797</td>\n",
       "      <td>0.390719</td>\n",
       "      <td>0.7326797107663086</td>\n",
       "      <td>0.499783</td>\n",
       "      <td>0.711649</td>\n",
       "      <td>0.664329</td>\n",
       "      <td>0.408121</td>\n",
       "      <td>0.878717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389931</td>\n",
       "      <td>0.507315</td>\n",
       "      <td>0.490987</td>\n",
       "      <td>0.041370</td>\n",
       "      <td>0.462390</td>\n",
       "      <td>0.947289</td>\n",
       "      <td>0.182826</td>\n",
       "      <td>0.587181</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.610479</td>\n",
       "      <td>0.478407</td>\n",
       "      <td>0.786356</td>\n",
       "      <td>0.235442</td>\n",
       "      <td>0.7963768589855218</td>\n",
       "      <td>0.592039</td>\n",
       "      <td>0.715180</td>\n",
       "      <td>0.838920</td>\n",
       "      <td>0.173883</td>\n",
       "      <td>0.934750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253729</td>\n",
       "      <td>0.535583</td>\n",
       "      <td>0.334942</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.606468</td>\n",
       "      <td>0.897280</td>\n",
       "      <td>0.192746</td>\n",
       "      <td>0.619840</td>\n",
       "      <td>0.745078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.470017</td>\n",
       "      <td>0.377998</td>\n",
       "      <td>0.824718</td>\n",
       "      <td>0.393915</td>\n",
       "      <td>0.8224050291903859</td>\n",
       "      <td>0.585371</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.611074</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.624647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.554207</td>\n",
       "      <td>0.381248</td>\n",
       "      <td>0.076909</td>\n",
       "      <td>0.533006</td>\n",
       "      <td>0.935659</td>\n",
       "      <td>0.319159</td>\n",
       "      <td>0.876869</td>\n",
       "      <td>0.580277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.346003</td>\n",
       "      <td>0.100142</td>\n",
       "      <td>0.781156</td>\n",
       "      <td>0.439434</td>\n",
       "      <td>0.9503121996878838</td>\n",
       "      <td>0.555374</td>\n",
       "      <td>0.784326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315625</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368381</td>\n",
       "      <td>0.750055</td>\n",
       "      <td>0.318052</td>\n",
       "      <td>0.023063</td>\n",
       "      <td>0.633076</td>\n",
       "      <td>0.903943</td>\n",
       "      <td>0.442787</td>\n",
       "      <td>0.809851</td>\n",
       "      <td>0.569212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.427905</td>\n",
       "      <td>0.624984</td>\n",
       "      <td>0.860459</td>\n",
       "      <td>0.453867</td>\n",
       "      <td>0.6887381290727799</td>\n",
       "      <td>0.457943</td>\n",
       "      <td>0.530200</td>\n",
       "      <td>0.688023</td>\n",
       "      <td>0.447066</td>\n",
       "      <td>0.878165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213601</td>\n",
       "      <td>0.189264</td>\n",
       "      <td>0.558862</td>\n",
       "      <td>0.077598</td>\n",
       "      <td>0.320214</td>\n",
       "      <td>0.905402</td>\n",
       "      <td>0.398802</td>\n",
       "      <td>0.521767</td>\n",
       "      <td>0.811772</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.461165</td>\n",
       "      <td>0.642609</td>\n",
       "      <td>0.878522</td>\n",
       "      <td>0.167811</td>\n",
       "      <td>0.802036</td>\n",
       "      <td>0.394491</td>\n",
       "      <td>0.630863</td>\n",
       "      <td>0.722636</td>\n",
       "      <td>0.288655</td>\n",
       "      <td>0.967985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409150</td>\n",
       "      <td>0.589928</td>\n",
       "      <td>0.590885</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.580831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.350445</td>\n",
       "      <td>0.535638</td>\n",
       "      <td>0.851672</td>\n",
       "      <td>0.412014</td>\n",
       "      <td>0.5734532769864495</td>\n",
       "      <td>0.593781</td>\n",
       "      <td>0.586341</td>\n",
       "      <td>0.667384</td>\n",
       "      <td>0.419121</td>\n",
       "      <td>0.853813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135575</td>\n",
       "      <td>0.320320</td>\n",
       "      <td>0.716790</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.320311</td>\n",
       "      <td>0.908059</td>\n",
       "      <td>0.170628</td>\n",
       "      <td>0.316081</td>\n",
       "      <td>0.886800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.466585</td>\n",
       "      <td>0.671103</td>\n",
       "      <td>0.809819</td>\n",
       "      <td>0.331889</td>\n",
       "      <td>0.6362481406367202</td>\n",
       "      <td>0.625956</td>\n",
       "      <td>0.586835</td>\n",
       "      <td>0.742236</td>\n",
       "      <td>0.305013</td>\n",
       "      <td>0.926243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0.236396</td>\n",
       "      <td>0.555513</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.266985</td>\n",
       "      <td>0.874898</td>\n",
       "      <td>0.147778</td>\n",
       "      <td>0.338625</td>\n",
       "      <td>0.751966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 47 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "43533eeae8e3ec91",
   "metadata": {},
   "source": [
    "**Balanced gleason dataset**"
   ]
  },
  {
   "cell_type": "code",
   "id": "998b063919993534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:59:51.149146Z",
     "start_time": "2024-10-11T03:59:51.141398Z"
    }
   },
   "source": [
    "processed_gleason_df['TUMOR_STAGE'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TUMOR_STAGE\n",
       "1    145\n",
       "0    145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "7ea6a58dc3a08867",
   "metadata": {},
   "source": [
    "**Saving the final dataframe as a .CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "id": "23f9a32037772e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T03:59:55.488048Z",
     "start_time": "2024-10-11T03:59:55.463042Z"
    }
   },
   "source": [
    "processed_gleason_df.to_csv('CTGAN_Gleason_Score_Dataset.csv', index = False)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6ac3230d24f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
